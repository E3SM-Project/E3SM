4/28/2022 13:46:30 PDT

* Okay, as far as I can tell everything is running fine - the zlnd_v2 case,
  compiled without debug, now runs really fast, like 50 minutes wall clock
  for two hours.  I just don't know if I believe that, and I'm worried
  somewhere in my coding I've short-circuited some calculation we used to
  be doing, but I'm going to build with debug and see if it takes four
  hours again or not.

* Otherwise, I'm ready to merge with Alan when he's ready, hopefully today
  or tomorrow, although he's had some issues running on compy.

* In the meantime, I'm going to review the log files and see if I can see
  anything else that doesn't seem to work right, or something else I don't
  understand.  I also should probably dump the coupled avects somehow and
  check them over manually - maybe find an i index (or a range) where I
  can check the math (averaging, maxmonmean) as we go along.

4/28/2022

* So, I learned an important thing: The "eli" counters over the
  num_inst_lnd and num_inst_iac in the prep_foo_mod functions are *not*
  looping over the number of processors in the component in question -
  instead, num_inst_xxx refers to the number of *coupler* instances of
  those avects.

  So it's not looping over teh component decomposition, but the *coupler*
  decomposition.  And, for our runs at least (and probably for most
  others), nun_inst_xxx are all 1 - we don't decompose in the coupler, we
  keep the whole arrays together.

  My guess is only fully coupled, very complicated model runs will
  decompose in the coupler - or maybe the ocean model, which seems to have
  some exotic ways to run, has multiple instances of the avects in the
  coupler.  

  Anyway, so the weird mod behavior in these loops, e.g.
  
	do eli=1,num_inst_lnd 
	   ezi=mod((eli-1), num_inst_iac) +1

  are still weird and I don't understand them, but becuase they are all
  related to coupler decompositions that we don't user, I don't have to
  understand.  Apparently it's some kind of way of associating instances of
  the decompositions when you have a different total number of instances
  for each coupled component, so the mapping from instance 1 and 3 of
  component a (with num_inst_a=4) will go into instance 1 of component b
  (with num_inst=b=2).  In other words, the two instances of component b
  each cover the merged domain of two instances of component a.

* Whatever, just don't think about it too hard, as none of that mod stuff
  mattters if all the num_insts are 1.

4/27/2022

* Wrt timing, it looks like the first sample of the year (or of the run) is
  actually timestamped 1800 seconds into the year - not midnight of the
  year, but 0030 of the year.  If you look at the alarms for the regular
  components, they all start at year 1, 1800 seconds, including iac.

  Therefore, this implies January 1, 00:00 is the last sample of the
  previous year.

* I've double checked this with print statements in zlnd_v2 - the first
  sample time in the cime_run() while loop has a time of day (tod, in
  seconds) of tod=1800.  The iacrun_alarm and lndrun_alarm both are set to true
  for this first sample.

  At the end of the year, the iacrun_avg_alarm is true only for 20160101
  tod=0, then the iacrun_alarm is true for the next sample, tod=1800.

* Okay, so that means a year is basically from tod=1800 on January 1 year 1
  to tod=0000 January 1 for year 2.  When you tell it to run for 2015, it
  starts one time step in.  Theis might make it a little easier to
  set meaningful restart files; the restart for 20160101 will contain all
  the info up to midnight on 20151231, but it has the same time stamp as
  the year you want to restart on.

* Anyway, this is important because I was wondering how the iac monthly
  averaging would work with restart files, since I current do the monthly
  (and yearly) average on 0000 of the first day of the following month.  I
  was assuming that tod=0 was the first sample of the month, so I did the
  average and maximum at the top of the while loop, which meant it might
  not be computed at the end of a yearly run if we stopped processing at
  2xxx1231.233000.  So we would have to do the averaging and final
  maxmonmean stuff before storing the restart data - but *that* might cause
  issues with the restart at 0000 of January 1, unless we turn off the
  iacrun_avg_alarm for the first part on restart.

* But if the run (or restart)  actually starts at tod=1800, then we will
  have wrapped up the averaging and maxing on 12/31 0000Z, and that will be
  ready for use by iac when it runs on tod=1800 on the first restart day.

! Anyway, the point of all this exposition is that I should keep the alarms
  the way they are - iac runs on tod=1800 of the first year, while iac_avg
  runs on tod=0000 of the first day of the month...but I should run iac_avg
  *after* everything (or, at least, after the lnd component), rather than
  before, because the iacrun_avg_alarm is offset by -1800 seconds from the
  tod iacrun_alarm.  So it wraps up the year a time step before iac runs,
  so we need to do it after all computations are done.

@ So, the point is: run iac_accum_avg() in the lnd post section, not at the
  top of the cime_run() loop.

4/21/2022

* Discovered the reason why zatm ran so slow - the dtime for cam was at 4
  seconds, because it was dividing one day by 17520, rather than one year.

  We found this for zlnd, but apparently the fix has to happen in each
  component.  For clm we modified .../clm/bldCMBuildNameList.pm, which was
  a perl module the namelist script was built upon.  For cam, I had to
  modify .../cam/cime_config/buildnml (a python script) directly.  We need
  to grab NCPL_BASE_PERIOD to determine that our base period is one year,
  and then modify around line 167 to use a full year as the numerator when
  calculting dtime.

! So, every component that we use in our build will probably have to be
  modified to use a full year to calculate dtime.

* With dtime at 1800 like it should, it took five hours wallclock to run
  one year of zatm.  That tracks - zlnd takes five hours to run two years,
  so adding another component should make it take twice as long.

@ I should probably run zatm for two years, just to make sure I haven't
  munged up the land back coupling (l->z) with these mods.

4/6/2022

* Okay, ran a grep and it appears that yes, we never actually fill
  iac2atm_var%co2emiss, so I gotta figure out how to extract that from the
  gcam_run_mod() call.

  Pro tip: when all else fails, look at the *names* of the arrays being
  passed to a function.  When it has 36 input arrays named like this:

  gcamoco2sfcaug(:,:)
  gcamoco2airlojun(:,:)
  gcamoco2airhioct(:,:)

  ...then obviously those are the monthly values for surface, lower level
  air, and higher level air fluxes from CO2.  So I just need to collapse
  these down into individual arrays of inputs:

  iac2atm_var%co2emissfc(mon,i,j) 
  iac2atm_var%co2emisairlo(mon,i,j) 
  iac2atm_var%co2emisairhi(mon,i,j) 

  Or, I guess I could just have 36 arrays directly, but I feel like we'll
@ need the integer month index on the back end for how eam will use it.
  Review how cam deals with monthly input fossil fuel values from a file,
  as per the notes from 4/5/2022.

4/5/2022

* Getting our ducks in a row for teh rebase:

* Alan is making progress on fractions.  My understanding is taht the GCAM
  downscaling and upscaling functions will deal with any land-area
  normalization that has to happen, so for l->z we don't need to weight
  anything, and thus we don't need landfractions in our mapping.  The
  gridcell conversions (in both directions) of the GCAM variables already
  assume a full grid cell, and any further normalization happens in the
  gcam stuff.

* Co2 - I'm trying to get the co2 variables coupled.  I *think* Kate never
  got to the part where she took the emissions-on-a-gridcell (downscaled)
  Co2 output and stuffed it into the actual lnd2atm_var data structure that
@ gets coupled.  So I need to set up and allocate those variables, and then
  figure out how to put them correctly into lnd2atm_var.  

  But, the time interpolation of these monthly values should *not* be a
  problem - Alan remembered, and I did too after some prodding, that the
  reason we are sending monthly Co2 values is that's the format the cam
  (eam) code gets them from when reading a data file.  So eam has already
  implemented whatever interpolation across these monthly values they
  want.  So we can just send across our monthly values, fill in the data
  structures with our values instead of the co2emissions file they
  currently are using, and it should work.

* Trying to get maximum of monthly averages going - the real trick is to
  set up a monthly alarm to calculate the averages of our accum data, and
  then a yearly alarm to grab a maximum of those.

  The yearly alarm is just the regular gcam alarm - we'll just add in an
  average for December of the previous year first, then do the max, and
  then get on with running gcam.  We won't want to do this for the first
  year, though...

3/4/2022

* Heh, so here's something funny.  Kate's original ZATM compset string was:

  2000_CAM5_CLM45%SPBC_CICE%PRES_DOCN%DOM_SROF_SGLC_SWAV_GCAM

  I think she was trying to find a real CAM/CLM coupling case to attach
  GCAM to.  However, SPBC doesn't run the carbon nitrogen model (and other
  bgc options), so we need to use CLM45%%CNPRDCTCBC like ZLND does.  (I
  think any CN* or BGC* option to clm would probably work, but any SP*
  option definitely will not.  See
  ~/compy/E3SM_current/components/clm/bld/CLMBuildNamelist.pm).

! But, I wasn't sure what CICE was doing there - we had never talked about
  coupling with the ice model.  So I also changed things to stub out ice
  and ocean - ...SICE_SOCN....

  But, obviously, that doesn't work - you can't just stub out the ocean
  model when coupling with atmosphere, because all those water grid cells
  then are just using their initialized data values.  In practical terms,
  this means that the surface temp (along with all the other variables) is
  set to zero (degK), which ends up failing in some calculation where we
  are dividing by surf_t.

  So, of course we need at least DOCN set when using the atm/lnd coupling.
  Apparently, CICE *also* prescribes ice values - DICE does not work, but I
  don't think CICE actually runs a model but inputs data values?  Maybe
  calculated ice values?

  Anyway, I could find no existing compsets with atm/lnd coupling where we
  didn't either have an active ice model or CICE set, so which is probably
  why Kate included it in the first place.

* The 2000_CAM5_CLM45%CNPRDCTCBC_CICE%PRES_DOCN%DOM_SROF_SGLC_SWAV_GCAM
  compset with 480 procs for lnd and atm runs without bombing, but it times
  out after only one day of processing, with 4 hours wall clock.  That
  seems really really slow, but maybe that's just the way these things
  work.  That's why restarting is so important, and maybe I need thousands
  of procs to run with as well.

  I've turned off DEBUG and rebuilt and resubmitted, to see if that will
  speed things up.

* Also, Kate encountered run issues with DEBUG=FALSE.  I briefly looked at
  her log message, and it failed while trying to do E3SM mapping (which
  isn't supported), which is crazy because that is governed by a namelist
  variable which should always be false.  So my guess was a memory error,
  but I've fixed some issues in the merged code since then, and Alan had
  made some changes in his code (which is merged into my branch), so I'm
  kind of hoping the thing just runs now.  Kate was unable to get ZATM to
  run off her branch and I'm cranking out a full day (about 50 timesteps)
  before time out, so maybe that will even happen.

* If things really are this slow, then we'll have to figure something out
  to test lnd->iac coupling, which will only happen at the end of a model
  year.  In principle, it shouldn't change between ZLND and ZATM, and we
  can run a full year of ZLND in our four hour block, so there's that.  But
  we definitely want to make sure it still works correctly under ZATM as
  well. 

  Given how ZLND runs relatively quickly, I'm skeptical that adding ZATM
  coupling (and maybe the ocean/ice calculations) could make it run
  something like 700 times slower (as we get two years of ZLND in 4.5 wall
  clock hours).  But I don't really know how slow cam is on compy, so I
  definitely need to ask somebody about that, and what the usual number of
  procs are used for lnd/atm runs.

3/1/2022

* I never included Kate's last update, which splains where we are and where
  we should be going when she left.

----
  From:	Bond-Lamberty, Benjamin
  Sent:	Wednesday, January 12, 2022 9:40 AM
  To:	Alan Di Vittorio; Shippert, Timothy R
  Subject:	Current status notes

  Hi guys, as promised here are Kate's notes from last week on the current
  status of the GCAM coupling work. Thoughts/corrections are welcome if you
  see something surprising. 

  Thanks,
  Ben

Current status
* We have three compsets that we've been testing: Z, ZLND, ZATM

o Z: GCAM + all iac code (GLM, mksurfdat, etc). This compset will complete 
  without errors.

o ZLND: GCAM + all iac code + ELM. This compset will complete without
  errors.  However, Alan & Tim are still verifying that the land is
  being passed correctly from GCAM to ELM (they think that the
  mapping file isn't doing fractions correctly right
  now). Additionally, I don't think we've tested whether the ELM data
  used for scalars (NPP, HR) are being passed correctly. 

o ZATM: GCAM + ELM + EAM. This compset currently times out in the atm run. 
  I have not tried giving it a large enough wall clock or enough
  nodes to see if it  would complete successfully. It does
  successfully run the initialization code +  GCAM. I doubt that CO2
  emissions are being passed correctly from GCAM to  EAM even though
  we have no errors (Kate will try to check this more before  Sunday) 

* Everything has been tested in debug mode. For reasons I don't
  understand, running with DEBUG set to false generates an error for
  compsets that work in DEBUG mode. 

o For ZLND, the error is: "ERROR: (seq_map_init_rcfile)  ERROR: esmf
  SMM not supported" 

* We have tested running multiple years and running from annual
  restarts. I have not tried using a monthly restart, but don't
  expect issues. 

* We have several branches. 

o The main branches are Tim's:
? E3SM: shippert/gcam/active-gcam-debug
? GIAC: shippert/merge

o Alan & Kate will create new branches from those and then Tim integrates those 
  changes later. I think all of Alan's changes have been incorporated
  into Tim's branches. Kate still has changes on these branches: 

? E3SM: kvcalvin/gcam/atm-coupling
? GIAC: kvcalvin/cleanup
o GCAM is using the `e3sm-integration` branch on GitHub (jgcri/gcam-core repo). 

  It branched from GCAMv5.3 and then added a bunch of coupling
  code. I've also cherry-picked some post v5.3 bugfixes (e.g., those
  related to tidyr). I debated upgrading to v5.4 since it includes
  DACCS and lets GCAM run on multiple processors on a single
  node. But, GCAMv5.4 has a lot more solver failures and that seemed
  too risky for E3SM. 

Remaining work
* Need to thoroughly test the coupling

o We've got things mechanically working, but need to check that
  everything that is being passed through the coupler is being passed
  correctly 

* Need to get all input files in the right spot. Right now, our run
  scripts copy a bunch of files to the case directory to work. 

o Namelist defaults: The namelist isn't being generated with the
  right defaults (Tim was looking into this). 

o GCAM inputs: The model was slow when it was using GCAM input files
  from the inputdata directory. To speed up testing, we've just been
  copying those files to the case directory, but they should not be
  there. 

o Something else: I think there was some other issue related to
  having C/C++ copy files to the right directory on its own, but I
  need to look into this to remember what was going wrong. 

* Need to figure out (and fix) whatever is leading to the creation of
  a $RUNDIR folder in the scratch directory. 

* Need to add tests, clean up code, and open a PR

o Once the coupling is working correctly, we need to open the PR. We
  have only been testing on compy. Rob J says the infrastructure team
  can help port to other E3SM supported machines when we are ready

o We need to add a set of smoke tests at a minimum to ensure no one
  breaks the code. I don't think those need to be scientifically
  valid compsets, but they do need to work out of the box.

o There is probably some code clean up to be done. Not sure how
  important that is given E3SM coding/style standards but probably
  worth a look 

* Need to document, including how to use and how to extend the
  coupling to other variables

o We may want to check with Rob on whether there would be value in
  having documentation on how to add a new component - in general, I
  would think so, but I think they are switching couplers at some
  point in the future. 

* Need to develop scientifically valid compsets

o Resolution: We wanted a coarser resolution version of the land &
  atm for testing, but you have to make mapping files between
  components. We've currently set up the land/atm to be at ~2 x 2.5
  and the iac at 0.9 x 1.25. However, the land/atm resolution is not
  scientifically supported in E3SM. It will need to be changed to a
  ne grid (standard low res is ne30, but they test sometimes at
  ne4). We could change the resolution of the iac grid but we'd need
  to regenerate some mapping files there between GCAM regions and the
  grid.  

o GCAM configurations: 

? I've just been using a GCAM reference case for testing. We will
  need to set up configurations for the specific v2 scenarios planned
  (low radiative forcing scenarios with different types of CDR).  

? For reasons I don't completely understand, the GCAM reference
  scenario has solving problems in 2020 --- this could be related to
  debug mode (testing that now).  

? GCAM uses binary restarts, so changing the scenario (or the machine
  or the compiler) means we need to regenerate the restarts for the
  GCAM historical period (this can be done by simply running GCAM
  through that period with the exact compiler options & configuration
  planned). I do not know whether that needs to be done when you turn
  off debug mode (testing that now). 

* Need to develop some standard diagnostics and analysis scripts. We
  have stuff from both E3SM and GCAM, but haven't standardized or
  automated stuff related to the coupling. We could probably start
  from my GEC paper figure scripts or other iESM scripts.  

----

2/11/2022

* Okay, I don't know if anything below makes sense anymore, but *this* is
  how fractions are used:

1 As weights when interpolating from one grid to the next - so if we are
  averaging the land contribution of some variable to a bigger grid, we
  clearly want to weight by the land fractions in the smaller grid.

2 As weights when calculating the contribution of each component to a grid
  cell for a given quantity.

  As an example of the latter, let's say the atmosphere is trying to
  determine a quantity over a cell where the components are 1/3 land,
  ocean, and ice.  Obviously, you want to scale that quantity for each
  component in that cell by 1/3 before adding them together.

* The thing is, you can apply these fractions in a lot of different places
  in the code.  Apparently, the atm component chooses to do it not when the
  quantity is calculated in the atm code, but in the prep_atm_merge
  function, as soon as it grabs each components avects from the coupler.
  That's why you see all those "* xfrac" scalings in the prep_atm_merge
  function. 

* But other components don't do this kind of scaling, presumably because
  either they don't need to weight by fraction or because they do it
  internally in the component model code.

* Now, as far as iac is concerned, we actually have a number of fractions
  to consider.  When iac is coupling with atm, we have already scaled the
  co2 flux to be the contribution of the full grid cell, not just from the
  land part (I dunno, do we consider boats?  Or we have just scaled it up
  to full grid cell.  Whatever.)  So zfrac(a) = 1.0, for z2a.

  We also have the landfrac, which should match that of the land component,
  but we've read it in separately.  So that's zfrac(l) = lfrac(z)
  (hopefully). 

  But Alan has pointed out that we also need to further subgrid the cell by
  vegetation fraction - some pfts are vegetation land types, while some are
  urban, so we need a zfrac(veg), which we will use as a weight when
  coupling z2l.

  So, when coupling with atm, we have frac=1.0.  When coupling with lnd, we
  sometimes need to just weight by landfrac (to interpolate onto the bigger
  grid), but sometimes we need to weight by vegfrac/landfrac (if vegfrac is
  the fraction of the whole grid cell - or, we could store the fraction of
  the *land* part of the cell that is vegetation,
  i.e. vegfrac'=vegfrac/landfrac).

* To get this right, then, we really need to figure out how we want to
  weight each field we are coupling in all the cases we are using them.
  But I think that, unlike atm, we'll scale by the appropriate fractions
  internally in the lnd and iac components, and not in the p   qy
rep_merge
  functions.  We also need to couple fractions_z(zair:zfrac:zvfrac), with
  fractions_a(...:zair) and fractions_l(...:zfrac:zvfrac)

2/1/2022

* So, I've got a couple things we are working through.

* Fractions_z, mapped to the land grid - does that make sense?  The lnd and
  iac landfrac is ostensibly the same, but if we interpolate one onto the
  other that could make changes.  Alan pointed out the mapping conserves
  area, and that's what's important, but it still seems like
  fractions_l(lfrac) and fractions_l(zfrac) should be identical, but
  there's a lot of reason to think they might vary somewhat, and does that
  matter. 

* The other issue is coupling z->a, sending CO2 emissions across.  Right
  now we were going to have a number of different heights, so we couple N
  different emiss_co2[h] variables, similar to how we couple 17 different
  variables of npp for the 17 different pfts in z->l.

  Alan points out, though, that gcam calculates a single emission value on
  its grid, without it being scaled to landfrac.  In other words, the iac
  generates a value on the grid, not a value from the fraction of land on
  each grid.  So we need a separate fractions_z(z->a) that gives fraction
  values of 1.0 for use when coupling z->a.

? So, my question is - isn't that the same thing as fractions_z(afrac)?
  The atmos fractions are always 1.0, so if we interpolate that onto the z
  grid, which I think we do, we should get 1.0 everywhere.  So if we use
  fractions_z(afrac) to build our z->a maps rather than fractions_z(zfrac),
  wouldn't that work automatically?

? Or, do I not know what I'm talking about, wrt how fractions are
  calcualted.  My understanding is that we build fractions_c(cfrac) on
  component c's grid, and then run it through the mappings to calculate
  fractions_c(yfrac) for every coupled component y's fractions on the c
  grid.  Then the mapping uses the fractions from the source component on
  both grids to do the interpolation. If that's true, then
  fractions_z(afrac) makes sense, and is what we want to use while coupling
  to atmosphere - 1.0s everywhere on the z grid.

  The alternative is to build a whole new variable to hold the z2a
  fractions (which are 1.0): something like fractions_z2a, and then make
  sure to use that whenever we map z->a.

  My worry is that we might not be able to tell it to use
  fractions_z(afrac) while coupling z->a - the calling sequence to the
  functions might force you to look up fractions_z(zfrac) and
  fractions_a(zfrac) only when doing z->a coupling.  If that's the case,
  then I'm not sure a new variable like fractions_z2a will even work,
  unless we send the fraction (and not the index to use) into
  seq_map_map(). 

! A third possibility is to actually scale co2 emissions that we get by
  gcam by 1/fractions_z(zfrac), so that the scaling offsets the fraction
  used my MCT to interpolate.  I.e. we trick MCT into doing the right
  math.  This is obviously obfuscatory and not ideal, but it might involve
  a lot less coding and refactoring MCT functions than the other way.

1/24/2022

* I've found the problem - the field list of fields to couple is getting
  truncated.   The z2x%rlist(:) is a colon-separated character string
  listing all the fields to send to the coupler, and there's a similar one
  built for x2l.  We are coupling way more fields than other compsets, so
  we (eventually) end up with a truncated list, and that's why prev for
  pft=14 and both current and pref for pfts 15 and 16 aren't getting
  coupled. 

  But it's trickier than this, because the z2x%rlist DOES allocate enough
  space (4096 characters, the "CXX" size), and the seq_add_flds() function
  actually does a check to make sure we don't write past the allocated
  space, and it will throw an error if we do.  So these rlists are being
  built and stored correctly in the coupling variables.

  BUT! Deep in the MCT code, in seq_map_map(), we *copy* over these lists
  to a working variable before calling mct_aVect_init() to set up all the
  coupled variables.  (Maybe we destroy the fields list while using is,
  popping off values or something?  Or, maybe no reason.)  Anyway, the
  variable lrList in seq_map_map() was only allocated "CX" (512)
  characters.  As it turns out, that gets you to Sz_pct_pft14, but NOT to
  Sz_pct_pft_prev14 or the pft=15,16 variables.  So we end up truncating
  our correct list of fields, and only couple through current_pft14.

  The solution, of course, is to allocate lrList with size CXX (4096), so
  we can copy over the full list and couple all the fields we ask.

  When you do that, the normalization and other errors go away, and the
  code (lnd/iac coupled) runs through two full model years.

1/21/2022

* Okay, I've learned a couple things.  Here is the output for pft=14 for
  all 13 grid cells the masterproc (apparently) gets from the decomp, after
  importing from the coupler

           g pft      lat       lon      pft_prev   pft_curr        diff
TRS8:      1  14  -90.00000    0.00000    0.00000    0.00000    0.0000000000
TRS8:      2  14  -86.21053  280.00000    0.00000    0.00000    0.0000000000
TRS8:      3  14  -80.52632  200.00000    0.00000    0.00000    0.0000000000
TRS8:      4  14  -74.84211  152.50000    0.00000    0.00000    0.0000000000
TRS8:      5  14  -54.00000  295.00000    0.00000    0.00000    0.0000000000
TRS8:      6  14  -18.00000   35.00000    0.00000   51.32734   51.3273393133
TRS8:      7  14   -0.94737  300.00000    0.00000    0.10030    0.1002995088
TRS8:      8  14   16.10526  347.50000    0.00000   33.08695   33.0869547983
TRS8:      9  14   29.36842   40.00000    0.00000    0.00000    0.0000000000
TRS8:     10  14   38.84211  250.00000    0.00000    0.83565    0.8356476962
TRS8:     11  14   48.31579  115.00000    0.00000    0.00000    0.0000000000
TRS8:     12  14   55.89474  265.00000    0.00000    0.00000    0.0000000000
TRS8:     13  14   63.47368  202.50000    0.00000    0.00000    0.0000000000

  You will notice that pft_prev=0 for all cells.  Only four cells have
  anything for pft_curr, and those are the ones I've found a problem with,
  but what this means is that:

! The problem is in the coupling of pft=14 (p=15 on iac side) for the
  pft_prev field, not in the mapping or anything else.  Somehow, the pft=14
  pft_prev field is not getting coupled correctly, while (probably/maybe/we
  don't have any evidence otherwise) the pft_curr field IS getting coupled
  correctly. 

* Here are the index_z2x_Sz_pct_pft(p), and _pref values on the iac side:

                             p         z2x_curr   z2x_prev
 TRS: indexz2x(p):           1           1           2
 TRS: indexz2x(p):           2           4           5
 TRS: indexz2x(p):           3           7           8
 TRS: indexz2x(p):           4          10          11
 TRS: indexz2x(p):           5          13          14
 TRS: indexz2x(p):           6          16          17
 TRS: indexz2x(p):           7          18          19
 TRS: indexz2x(p):           8          20          21
 TRS: indexz2x(p):           9          22          23
 TRS: indexz2x(p):          10          24          25
 TRS: indexz2x(p):          11          26          27
 TRS: indexz2x(p):          12          28          29
 TRS: indexz2x(p):          13          30          31
 TRS: indexz2x(p):          14          32          33
 TRS: indexz2x(p):          15          34          35
 TRS: indexz2x(p):          16          36          37
 TRS: indexz2x(p):          17          38          39

  ...so these look normal.  I'll investigate the lnd side in the next run,
  to make sure the l2x(p) indeces are getting set correctly over there.
  Then, I guess I need to track down where in the code we tell MCT which
  fields to couple and pull over.

  My conjecture is that somehow the pct_pft_prev stuff stops working at
  pft=13, or at least before pft=14.  So some index isn't being set right,
  we aren't looping over the right things, or there is a typo somewhere
  that's keeping either the lnd from importing or iac from exporting the
  prev field, but not the curr field.

* Okay, I just checked the lnd log again - both prev and current are all
  zeros for pft=15 and pft=16.  So, either they are not getting coupled
  correctly too, or those pfts really are zero for these grid cells -
  either way, it's consistent with fields starting at pct_pft_pref(pft=14)
  and higher indeces not getting imported/exported correctly.

  I.e. it's not just pft=14, it could be that and everything with a higher
  index, which means there's a looping index or something that's not going
  far enough.

* In iac, the z2x_index_pct_pft_prev(p=15) = 35, so that's the limit we are
  looking at over there.

* At some point we tell MCT how many fields we expect the coupler to give
  and take from us - my current guess is we didn't update that right for
  either adding pref/harvest to iac, or adding iac to lnd.

1/20/2022

* Okay, I'm dumb - we zero-offset our pfts in the lnd model, but one-offset
  them in the iac model, so we need to look at p=15 in iac to understand
  whatever is going on with pft=14 in lnd.

  And when we do that, we get this for the iac grid points around g=6,8
  (lnd):

        g     p   i   j     lon       lat        prev   current
TRS:  21628  15  28  76   33.7500  -19.3194   50.6301   50.6301
TRS:  21629  15  29  76   35.0000  -19.3194   51.5226   51.5226
TRS:  21630  15  30  76   36.2500  -19.3194   32.5006   32.5005
TRS:  21916  15  28  77   33.7500  -18.3770   48.8705   48.8704
TRS:  21917  15  29  77   35.0000  -18.3770   46.3922   46.3922
TRS:  21918  15  30  77   36.2500  -18.3770   41.9186   41.9186
TRS:  22204  15  28  78   33.7500  -17.4346   61.1200   61.1200
TRS:  22205  15  29  78   35.0000  -17.4346   64.5736   64.5735
TRS:  22206  15  30  78   36.2500  -17.4346   41.2494   41.2494
TRS:  22492  15  28  79   33.7500  -16.4921   70.1149   70.1150
TRS:  22493  15  29  79   35.0000  -16.4921   50.1504   50.1504
TRS:  22494  15  30  79   36.2500  -16.4921   30.1438   30.1438

TRS:  32245  15 277 112  345.0000   14.6073   59.1920   59.1920
TRS:  32246  15 278 112  346.2500   14.6073   76.3799   76.3799
TRS:  32247  15 279 112  347.5000   14.6073   64.4084   64.4084
TRS:  32248  15 280 112  348.7500   14.6073   70.9835   70.9835
TRS:  32533  15 277 113  345.0000   15.5497   51.0057   51.0057
TRS:  32534  15 278 113  346.2500   15.5497   44.7880   44.7880
TRS:  32535  15 279 113  347.5000   15.5497   35.9804   35.9804
TRS:  32536  15 280 113  348.7500   15.5497   58.1320   58.1320
TRS:  32821  15 277 114  345.0000   16.4921   42.3875   42.3875
TRS:  32822  15 278 114  346.2500   16.4921   16.2948   16.2948
TRS:  32823  15 279 114  347.5000   16.4921   21.0899   21.0899
TRS:  32824  15 280 114  348.7500   16.4921   45.1869   45.1869
TRS:  33109  15 277 115  345.0000   17.4346   10.2364   10.2364
TRS:  33110  15 278 115  346.2500   17.4346    7.9039    7.9039
TRS:  33111  15 279 115  347.5000   17.4346    5.4487    5.4487
TRS:  33112  15 280 115  348.7500   17.4346    5.8889    5.8889

* So, the first thing we notice is that these values for g=6, which we
  analyze below, actually do compare with the value of 51.33 we get for the
  pct_current calculation.  Similarly, without doing the average, I can see
  where g=8 (lat=16.11,lon=347.5) might lead to a value of 33.09 or so - we
  have some higher and some lower, although there is more of a gradiant.

* Okay, so I'll probably have to actually calculate an average at some
  point, but for now it looks like the main problem may NOT be the mapping
  file, but why the prev is getting mapped to zero while the current is
  not.  The prev and current values are either identical or within 1e-5 or
  so on the iac side, so it looks like we are somehow not mapping prev in
  some (or all?) cases in the same way as we are mapping current.

* Could we be overwriting the prev values somehow, post-coupling?  My test
  1/19/2022 shows that it's wrong from the first time we see the x2l fields
  in lnd, but maybe something is happening upstream of that?  Something
  like we couple our correct fields, and then some downstream coupling
  fields overwrites it - i.e. an ABR or something?

  Or, could we somehow not be telling the coupler which mapping to use for
  prev?  Maybe there's another place in the driver or cpl component we have
  to send our coupling indeces and we didn't correctly add them in that
  spot when adding in the _prev fields?  (I originally just coupled the
  pct_current fields, but Alan, I think, updated to send prev (and harvest)
  fields because we need to interpolate across the year.)

1/19/2022

* Just some test runs to nail down when the interpolation fails - I wanted
  to check to see if maybe a memory error was corrupting the x2l fields
  post-coupling.  But I don't think so:

 TRS8:            1           1   51.3273393132519       0.000000000000000E+000

  This is printing x2l(<pft_current>,<g=6>) and x2l(<pft_pref>,<g=6>) for
  each loop over g, to see if it changes during this assignment loop from
  the coupler fields x2l to internal lnd fields.  As you can see, the
  values are the same from the jump, so my only conclusion is that the
  mapping is wrong, and thus our interpolated values coming out of the
  coupler are wrong.

* Next up is examining the mapping files and seeing if I can convince
  myself they are either correct or incorrect.

1/17/2022

* Okay, I don't understand something.  Looking at the g=6 case below, on
  the lnd side we have prev=0.0, val=51.3, around lat=-18, lon=35.4

  Around those lat/lons on the iac, we have these values:

 TRS: g=       21628 p=          14 i=          28 j=          76
 TRS: lon=   33.7500000000000      lat=  -19.3193717277487     
 TRS: pft_prev=   8.43277380296120      pft_current=   8.43277166762248     
 TRS: g=       21629 p=          14 i=          29 j=          76
 TRS: lon=   35.0000000000000      lat=  -19.3193717277487     
 TRS: pft_prev=   7.18035845724506      pft_current=   7.18035669368101     
 TRS: g=       21630 p=          14 i=          30 j=          76
 TRS: lon=   36.2500000000000      lat=  -19.3193717277487     
 TRS: pft_prev=   7.98092779772900      pft_current=   7.98091431201467     
 TRS: g=       21916 p=          14 i=          28 j=          77
 TRS: lon=   33.7500000000000      lat=  -18.3769633507853     
 TRS: pft_prev=   11.5671136481443      pft_current=   11.5671110424742     
 TRS: g=       21917 p=          14 i=          29 j=          77
 TRS: lon=   35.0000000000000      lat=  -18.3769633507853     
 TRS: pft_prev=   7.27121726373166      pft_current=   7.27121706487410     
 TRS: g=       21918 p=          14 i=          30 j=          77
 TRS: lon=   36.2500000000000      lat=  -18.3769633507853     
 TRS: pft_prev=   14.2817357391326      pft_current=   14.2817319949631     
 TRS: g=       22204 p=          14 i=          28 j=          78
 TRS: lon=   33.7500000000000      lat=  -17.4345549738220     
 TRS: pft_prev=   5.48228584734313      pft_current=   5.48228586988706     
 TRS: g=       22205 p=          14 i=          29 j=          78
 TRS: lon=   35.0000000000000      lat=  -17.4345549738220     
 TRS: pft_prev=   3.03892747447383      pft_current=   3.03892515919722     
 TRS: g=       22206 p=          14 i=          30 j=          78
 TRS: lon=   36.2500000000000      lat=  -17.4345549738220     
 TRS: pft_prev=   14.2585355637895      pft_current=   14.2585308215280     
 TRS: g=       22492 p=          14 i=          28 j=          79
 TRS: lon=   33.7500000000000      lat=  -16.4921465968586     
 TRS: pft_prev=  0.000000000000000E+000 pft_current=  0.000000000000000E+000
 TRS: g=       22493 p=          14 i=          29 j=          79
 TRS: lon=   35.0000000000000      lat=  -16.4921465968586     
 TRS: pft_prev=   7.67635828282414      pft_current=   7.67635737261794     
 TRS: g=       22494 p=          14 i=          30 j=          79
 TRS: lon=   36.2500000000000      lat=  -16.4921465968586     
 TRS: pft_prev=   19.3584850211624      pft_current=   19.3584796391147     

  These are all the points in the iac grid within 2 degrees of both the
  target lon and lat (35 degN, -18 degE).  Notice that (a) the prev and
  current values are all very similar, if not identical, in each point, (b)
  there are no prev=0, current>0 pairs, which is what we see on the lnd
  side, and (c) none of the percentages approach the pft_current lnd val of 
  51.33.  

  So, I don't see how we can interpolate or average some subset of the
  iac points (above) to get the lnd values (below).

* If you look at the values in lnd_import_export.F90, right after grabbing
  them from the coupler, you see prev=0, currrent=51.33, just like in my
  print statement.  So the problem really is in the mapping/coupling, and
  we aren't overwriting values with a memory error in the lnd code or
  something.

* I'm going to do a run where we try to find a prev value of 0.0 and and
  current value of > 0.0 on iac.  That seems like a prerequisite to get
  values like that in lnd post-coupling.  If we have none of those, then
  I'll be very confused - how can we get 0 and non-zero interpolated values
  starting with non-zero values, using the same mapping for both variables?

? Okay, while that's building and running, let's speculate.  Obviously, the
  mapping could be wrong somehow - the wrong grids in the mapping file?  Or
  the iac grid could be specified incorrectly internally, so MCT maps the
  wrong grid points.  Or, our coupling variables get messed up on the iac
  side some time after the import/export function.

  Here's a thing - it looks like the lnd import_export loops over 0,numpft
  while extracting the pft-based variables, while in iac import_export, we
  loop the p index from 1,npft.  So, need to check this - is numpft=16 in
  lnd, so our pft index is 0-offset and p is 1-offset?  Or, do we have a
  mismatch between the number of pfts we are sending/recieving on both
  sides of the coupling?

1/13/2022

* Issue: veg_pp%wtcol(p), calculated in iac2lndMod.F90 c. L192, do not sum
  to 100%.  This is calculated from the coupled pft values we send over
  from iac, so something is wrong about the coupling.  The actual failure
  happens in subgridWeightsMod.F90, in the calculations above L683.

* Here are the key points where we have pft_prev=0 and pft_current>0 in the
  calculation of veg_pp%wtcol(p) (basically taking our g,pft decomposed
  percentages coupled from iac and building  patch-indexed weights):

 TRS g=           6 p=         100 pft=          14 wt1=   1.00000000000000     
 TRS lat=  -18.0000000000001      lon=   35.0000000000000     
 TRS prev_val=  0.000000000000000E+000 val=   51.3273393132519     
 TRS interp=  0.000000000000000E+000

 TRS g=           7 p=         117 pft=          14 wt1=   1.00000000000000     
 TRS lat= -0.947368421052715      lon=   300.000000000000     
 TRS prev_val=  0.000000000000000E+000 val=  0.100299508791858     
 TRS interp=  0.000000000000000E+000

 TRS g=           8 p=         134 pft=          14 wt1=   1.00000000000000     
 TRS lat=   16.1052631578947      lon=   347.500000000000     
 TRS prev_val=  0.000000000000000E+000 val=   33.0869547982631     
 TRS interp=  0.000000000000000E+000

 TRS g=          10 p=         168 pft=          14 wt1=   1.00000000000000     
 TRS lat=   38.8421052631578      lon=   250.000000000000     
 TRS prev_val=  0.000000000000000E+000 val=  0.835647696229087     
 TRS interp=  0.000000000000000E+000

  So, this is for the very first sample, so wt1 = 1 (meaning completely
  weighted towards the prev values).  It just looks wrong to have the
  prev_val = 0.0 and the current_val greater than 1, sometimes by a lot.
  So this is my first guess as to why we end up with weights summing to <
  1.

* As far as I can tell, the iac2lnd mapping is being set up correctly, or
  at least in a similar way as with other components.  There might still be
  an issue there, some irregularity in iac that makes following the (e.g.)
  glc2lnd mapping code structure incorrect, but that seems fairly unlikely.

  So, my current guess is one or more of these problems:

1 The mapping file is incorrect, so we end up not interpolating between the
  iac and lnd grid correctly.

2 The prev and current values are not being correctly calculated and/or
  read in on the iac side, which leads to problems post-coupling inside the
  lnd code.

3 Memory errors modifying values inside of lnd?  Or some kind of weird
  initialization that corrupts the prev values?

4 The mappings between components we all follow use the flux mapping
  variables, e.g. Fa2l_foo, whereas between lnd and iac we have things set
  up as state variables (Sz2l_foo).  It's possible, I guess, that this
  causes a problem building the mappings?  Or there is some slightly
  different procedure for building a state based mapping?

* I'm grasping at straws with 4. above - there are state variables being
  coupled elsewhere, and the ocean even has SFo2a_foo mappings which,
  presumably means both state and flux variables.  Anyway, I just remember
  seeing a thing where we were using the flux mappings as some kind of
  baseline, and Alan added in the state mappings for iac in the same way,
  and I need to verify that that makes sense.

1/7/2022

* Well, I'm dumb.  Of course we crack the iac2lnd_smapname file to build
  the mapper_Sz2l, etc - see calls to seq_map_init_rcfile() in
  prep_lnd_mod.F90 and similarly for other components.  But the 'aream'
  stuff below is largely correct - we need aream for each component we are
  going to couple, and then we also build a mapping for each coupling, and
  some of that information comes from the same place.  So we end up
  cracking some of our map files twice - once to read in aream for our
  component, and once to build the mapping.

* So we already have implemented the mapping stuff for z2l in
  prep_lnd_mod.F90.  We may need to do the same thing for z2a, or maybe
  it's already in there.  But the main thing is to update the
  component_init_aream() code.

  Do we need to do the samegrid_lz stuff we see elsewhere?  We'd have to
  pass in samegrid_lz as an argument, which means updating where we call
  component_init_aream()...in cime_comp_mod.F90.

! Okay, more stuff - the mapper_Ab2c is always defined in the 'c'
  component, and we do have complementary maps (mapper_Fa2l and
  mapper_Fl2a).  So we will need both mapper_Fz2l (defined and assigned in 
  prep_lnd_mod.F90) and mapper_Fl2z (in prep_iac_mod.F90).

1/6/2022

* Okay, I think I've got this aream thing figured out.  The trick is that
  we have a default grid, the one we setup the case with - the aream
  loading then basically works against that default grid.  So we only need
  to crack open one fmap (or smap) file to get the area_a/b per component.
  So, e.g., when we are setting up lnd to do the mapping between lnd and
  air, we only need to crack the fmap_atm2lnd to get the lnd mapping,
  because atm is on the default grid.  (I think).  

* Anyway, it looks like "area_a" and "area_b" are the mapping areas
  between components a and b, with the file variable "a2b_fmapname"
  indicating which is a and which is b.

  So, this is how it works:

* First, in component_init_cc(), we set the aream attribute vector to the
  area array of the default grid.  We do this for each component, so at
  this point the areas are all the same and match the default grid.

* Then, in component_init_aream(), we read in the areams from the mapping
  files for each component that does not match the default grid.  So,
  pre-iac, we have five sections:

1 if atm and ocean are coupling, then if they are not on the same grid,
  crack and read aream from the ocn2atm_fmap file, with area_a the ocean
  and area_b the atmosphere.  I'm not exactly sure why we need both, but
  this is the only case where we read both area_a and area_b, so my
  conjecture is that's dilatory - the atm is on the default grid anyway, so
  area_a will match the aream already in there.  But I don't know.

  if atm and ocean are on the same grid, we skip reading a mapping file and
  just directly call seq_map_map() to generate mapper_Fa2o.

2 if ice and ocean are coupling, we don't crack any files and just do a
  straight mapping.  Maybe ice and ocean have to be on the same grid?

  Also, mapper_SFo2i - the same mapping for state and flux calculations?
  Would we ever have different mappings?

3 If rof and ocean couple and are on different grids, we crack open *two*
  mapping files, one for liq and one for ice.  But it appears it reads both
  of these into the rof mapping aream, with only the "string" value
  different in seq_map_readdata().  Okay, I don't really get this either,
  but it follows my general suggestion that ocean aream has already been
  set (either by the default grid aream, or by the atm2ocn_fmap stuff in
  section 1).

  If rof and ocn are on the same grid, do nothing.  No mapping required?

4 If lnd and atm is present and on different grids, then crack
  atm2lnd_fmapname and read in the lnd aream, meaning the atm aream is
  either the default grid or from atm2ocean mapping.

  If atm and lnd on same grid, seq_map_map(mapper_Sa2l,...), using the
  areams already set (probably by default grid).

5 if lnd and glc and not the same grid, crack and read in aream for glc,
  using lnd aream already set (default or as per previous couplings).  If
  same grid, call seq_map_map() on mapper_Sl2g.

* So, while I still have questions, I think we follow the glc model - if
  lnd and iac, if samegrid generate mapper_Si2l, otherwise crack
  iac2lnd_smapfile and read area_a into the aream attribute vector.

  This will provide the right iac aream to allow us to later generate the
  mapper_Fi2a to couple iac with atm model.  We use iac and lnd to decide
  if we need to crack a file, because we always couple with lnd as well if
  we are coupling with atm.

  I'll need to dig in to the mapper_S/Fx2x stuff elsewhere - I think the
  full set of these mappings are generated in the prep_foo_mod.F90 files,
  during initialization (and maybe at run time if the grid and therefore
  mappings change, although I don't know if that happens - I thought lnd
  model had dynamic grid changes (i.e. sea-level changes), but I might just
  be making that up).

  Look at ~/mapper.out to find where mapper_Xy2z are used and (hopefully)
  defined and generated.

1/4/2022

* So, the mapping files are set in env_run.xml in the case directory
  (i.e. ~/zlnd_v2), and we get entries like this:

    <entry id="IAC2LND_SMAPNAME" value="cpl/gridmaps/0.9x1.25/map_0.9x1.25_TO_1.9x2.5_aave.210910.nc">
      <type>char</type>
      <desc>iac2lnd state mapping file</desc>
    </entry>
    <entry id="IAC2LND_SMAPTYPE" value="X">
      <type>char</type>
      <valid_values>X,Y</valid_values>
      <desc>iac2lnd state mapping file decomp type</desc>
    </entry>
  
  This seems to be governed up by
  .../cime/src/drivers/mct/cime_config/config_component.xml, or maybe is
  set by default or something somewhere else in the build process.

* Anyway, these mapping files show (e.g.) area_a for the a grid and area_b
  for the b grid, which are fields that are passed into MCT to calculate
  the fractions and mappings between the domain boundaries.

* Leaving aside how that file name is passed in, what we need is a mapping
  between l->z, since they are on different grids (0.9x1.25 vs. 1.9x2.5, in
  our test case).

  So, I need to revisit all this stuff - figure out how we read area_a/b
  into the attribute vectors needed to do the mapping, then build in the
  mapping infrastructure (mct function calls) do actually build this
  mapping correctly.

12/15/2021

* Okay, with one more change to deallocate topo at the end of the iac run,
  so we can allocate it again next time, and getting the domain stuff
  working, I have now successfully run two full timesteps.

  So I'm going to try and see how things get coupled back from lnd to iac,
  and whether that is happening right.  I suspect it is not - the accum
  calls for averaging the lnd stuff don't bomb, but I'm not convinced they
  work correctly.

* Also, Alan has said the z->l coupling didn't work, because the order of
  operations in cime_run() wasn't right.  After looking at what I've
  currently got in there, I think I may see the issue - we do, in order,
  iac setup, lnd setup, iac run, lnd run, iac post, lnd post.  I think we
  have to wrap up iac setup/run/post before we call lnd setup in order to
  get the output of iac for this timestep to get through the coupler to the
  lnd model.

  The problem is the iac is weirdly different than the other components -
  I think they are fine coupling with the previous timestep, so lnd takes
  the atm post output from last timestep and doesn't worry about it.  But
  we can't really do that with iac, because that's a whole year previous -
  so we have to wrap up everything iac before anything else runs.

* Unless, of course, I don't understand what setup-send/post-recv do - I
  believe setup-send grabs the stuff it needs *from* the copler, and
  post-recv sends the output of this time step *to* the coupler.  (The
  "send" and "recv", therefore, is from the perspective of the coupler - so
  you send stuff to the component first, then you run, then you receive the
  stuff from the component afterwards.)

! Huh - it looks like we have calls to prep_lnd_calc_z2x_lx() in *both* the
  iac post-recv and the lnd setup-send functions!  I don't know where it
  more appropriate - do we couple every land time step, or only at the end
  of the iac time step?

* Okay, I think it's obvious we couple all ways on the iac time scale, if
  possible, so l->z should happen in iac setup-send, while z->l, z->a
  should happen on iac post-recv.  Hopefully that works and the values from
  z->x coupling stick around.  If not, then maybe we couple z->x in the x
  component setup-send, instead.

  Also, perhaps l->z coupling needs to happen on the lnd time scale, if the
  accum/averaging happens in the coupler.

! So, I need to figure out exactly what the coupler does - does stuff
  coupled once a year stick around that whole year, and does the coupler do
  the accum/average?

* Okay, it looks like lnd recv-post calls prep_iac_accum() on its time
  scale, which makes sense.  So then iac setup should call the averaging
  function - see how rof and glc accum works wrt to land.  I know the glc
  averaging has its own alarm setup to do the averaging, but we can simply
  do it in iac setup-send.

* Right, cime_run_glc_setup_send() checks the glc averaging alarm, and if
  it's sent, then it calls prep_glc_accum_avg().  So, by comparison, we
  should always call prep_iac_accum_avg() in cime_run_iac_setup_send(), and
  call prep_iac_accum() in cime_run_lnd_recv_post() (like we currently do).

12/14/2021

* Figured out the memory issue with domain_read_map() in the mksurfdat
  stuff.  

  TL;DR: domain_clean frees up domain%ns, so have to store ne=ns before the
  clean and allocate with ne rather than ns; otherwise you end up
  allocating an undefined number of elements.

  Apparently, every time you call that function, it reinitialized
  the domain structure, which involves calling domain_clean() to deallocate
  everything before you reallocate the space.  The problem comes when you
  call something like:  

      call domain_init(domain, domain%ns)

  The domain_clean() wipes clean the domain structure...including the %ns
  variable!  Internally, domain_init(domain,ns) uses ns like a regular
  variable, but it's actually a pointer back to domain%ns, which means the
  internal ns variable gets reset after the clean.  If you then use ns as
  the size of the allocate that follows the clean, you end up allocating an
  arbitrary number of elements.

  Obviously, then, in domain_init() you should use ne=ns and allocate with
  ne. 

* I've fixed the allocate(foo(ne),...) stuff, but I still wonder why we
  have to crack and read the grid file using this domain stuff every time?
  We actually *are* carrying the domain structure from timestep to
  timestep, and only deallocate/reallocate when calling read_domain_map()
  again during the next timestep - so there is no memory savings, all we do
  is take the time and churn to read the file again for no real reason.

  UNLESS that file changes, like we write out a new grid file between
  timesteps or something, then there doesn't seem to be a reason to call
  domain_read_map again - we should just switch on "%set" and only call the
  domain_read if it is not yet set.

11/30/2021

* Huh.  When I ran with lnd ntasks=64, we crash on a different column
  (c=30) but same time step 366.  My col=56 prints from this run look like
  this:

======
 Beginning timestep   : 2015-01-08_15:00:00
 TRS Debug2         366          56 -3.825552378243753E-010
  4.134697279933682E-004  7.551932844945582E-005  6.884084725096922E-005
  7.310531940145805E-005  1.487689291751433E-002          10
 TRS Debug2         366          56 -1.434158082969919E-013
  1.721422719162096E-007  3.751353951305753E-008  4.713990565421693E-007
  7.310531940145805E-005  1.487689291751433E-002           0
 CH4 Conservation Error in CH4Mod during diffusion, nstep, c, errch4 (mol /m^2.t
 imestep)         366          30   2147483648.00000     
======

  So these values are of the right magnitude, but different than the
  ntasks=36 runs - which, I'm not sure should be happening.  The
  decomposition shouldn't affect each column calculation, right?

  Also, of course, we don't have the spurious 7e21 value blowing everything
  up.  But note that the errch4 value is *exactly* the same in the old c=56
  as it is in the currentn c=30 runs.

? Do the c=56 values look the same for previous nsteps?

* Okay, this *now* looks like a memory thing, since changing the memory map
  across multiple nodes changes the answers I'm getting.

11/29/2021

* Digging into the first error we find with a lnd run - stops at nstep 366
  with c = 56.  The errch4(c) value jumps to 2147483648.00000, which not
  merely above the 1e-8 tolerance but comically so, which suggests a math
  or memory error.

  The element of errch4(c) that blows up is ch4_surf_diff(c) at this time
  step, which goes from order ~1e-9 in the previous time step, to
  7.359163363910448E+021. 

  (see
  /compyfs/d3a230/e3sm_scratch/zlnd_v2/run/lnd.log.253888.211129-104902).

* So now I'm printing near line 3436 to see how ch4_surf_diff(c) is
  calculated for this nstep.

  ...it looks like conc_ch4_rel(c,1) is the term that blows up, from 1e-6
  to 1e26 in successive calls near line 3436.  Note that we get two
  printing of nstep 366 and c=56.  The first call everything looks normal,
  it's for the second call that conc_ch4_rel(c,1) has blown up.  This can't
  be in the fc loop, because conc_ch4_rel(c,1) hasn't changed inside this
  loop, so we must be nested inside another loop...

  Here's the relevant printout from the log:

=======
 Beginning timestep   : 2015-01-08_15:00:00
 TRS Debug2         366          56 -1.675304498840987E-009
  6.455042782800097E-003  9.288544559760978E-005  4.599325768290392E-005
  6.969888586105318E-005  3.387602486391894E-003          10
 TRS Debug2         366          56  7.359163363910448E+021
  1.375684987377554E-004  1.069890771714985E+026  8.083328515988145E-007
  6.969888586105318E-005  3.387602486391894E-003           0
 CH4 Conservation Error in CH4Mod during diffusion, nstep, c, errch4 (mol /m^2.t
 imestep)         366          56   2147483648.00000     
 Latdeg,Londeg=  -33.1578947368422        17.5000000000000     
=======

  ...and here's the code for the printout:

-------
       if (c .eq. 56) then 
          write(iulog,*), 'TRS Debug2', nstep, c,&
               ch4_surf_diff(c), dm1_zm1(c,1), &
               conc_ch4_rel(c,1), conc_ch4_rel_old(c,1), &
               c_atm(g,s), k_h_cc(c,0,s), jwt(c)
       endif
-------


11/18/2021

* To get to run lnd with multiprocessors, I simply did this:

  ./xmlchange --id "NTASKS_LND" --val 24

  You have to do this (which it will tell you):

  ./case.setup --reset
  ./case.build --clean-all
  ./case.build

* I had to fix my code to run in multiproc mode, though - I had been
  skipping the arrays of NaNs by checking the first value of the array
  (.ie. isnan(foo(1) ).  But since these were column arrays and we decomp
  over columns, for all but the first proc "1" is not a valid array index.
  So I had to change all of these to "foo(begc)".  

* So it now runs with 24 procs, and seems to crash in the same place as
  before, PhosphorusStateUpdate3Mod.F90, line 162.  We now get eight cores
  dumped, which show the bomb in the same place.  Of course, running
  asynchronously, some procs will hit the problem before others, so that's
  why we don't get 24 cores.  But it does make the logs complicated,
  because some will show the error and some will simply show where they
  were when the abort came through.

  You can use gdb to find the problem on each core:

  $ gdb /compyfs/d3a230/e3sm_scratch/zlnd_v2/bld/e3sm.exe core.417493
  
  ... then do a 'where' to climb back up the calling three to where the error
  occured, because the first few levels will be abort and support
  functions.

  (gdb) where
  #0  0x00002ac460b09207 in raise () from /lib64/libc.so.6
  #1  0x00002ac460b0a8f8 in abort () from /lib64/libc.so.6
  #2  0x000000000571ba41 in for__signal_handler ()
  #3  <signal handler called>
  #4  0x0000000002d65fb6 in phosphorusstateupdate3mod::phosphorusstateupdate3 (
    bounds=<error reading variable: Cannot access memory at address 0x0>, 
    num_soilc=<error reading variable: Cannot access memory at address 0x0>, 
    filter_soilc=<error reading variable: Cannot resolve DW_OP_push_object_address for a missing object>, 
    num_soilp=<error reading variable: Cannot access memory at address 0x0>, 
    filter_soilp=<error reading variable: Cannot resolve DW_OP_push_object_address for a missing object>, 
    cnstate_vars=<error reading variable: Cannot access memory at address 0x0>, 
    phosphorusflux_vars=<error reading variable: Cannot access memory at address 0x0>, 
    phosphorusstate_vars=<error reading variable: Cannot access memory at address 0x0>)
    at
    /qfs/people/d3a230/E3SM_current/components/clm/src/biogeochem/PhosphorusStateUpdate3Mod.F90:162

* You can switch to new core files with:

  (gdb) target core core.417509

* When running in serial mode you can examine variables and stuff, but for
  some reason these parallel core dumps show this "cannot access memory"
  errors. 

* Huh, interesting - one of the cores bombs in a different place (core.417509):

  #4  0x0000000001291f42 in balancecheckmod::colwaterbalancecheck (bounds=<error reading variable: Cannot access memory at address 0x0>, 
    num_do_smb_c=<error reading variable: Cannot access memory at address 0x0>, 
    filter_do_smb_c=<error reading variable: Cannot resolve DW_OP_push_object_address for a missing object>, 
    atm2lnd_vars=<error reading variable: Cannot access memory at address 0x0>, 
    glc2lnd_vars=<error reading variable: Cannot access memory at address 0x0>, 
    solarabs_vars=<error reading variable: Cannot access memory at address 0x0>, 
    waterflux_vars=<error reading variable: Cannot access memory at address 0x0>, 
    waterstate_vars=<error reading variable: Cannot access memory at address 0x0>, 
    energyflux_vars=<error reading variable: Cannot access memory at address 0x0>, 
    canopystate_vars=<error reading variable: Cannot access memory at address 0x0>)
    at /qfs/people/d3a230/E3SM_current/components/clm/src/biogeophys/BalanceCheckMod.F90:657

  This is from the surface energy balance check, during the log write for
  "sabg", which is weird.

* The log also shows some exiting at BalanceCheckMod.F90 line 695, which is
  another explicit call to endrun after tthe soil balance error fails.
  Apparently calls to endrun don't dump a core, so you still have to look
  at the log.

* Okay, there are a number of places where the code stopped - only the
  phosphorus and balancecheckmod line 657 dump a core, but I'll examine all
  the other places.

  e3sm.exe           0000000000902414  accumulmod_mp_pri         221  accumulMod.F90
  *e3sm.exe           0000000002D65FB6  phosphorusstateup         162  PhosphorusStateUpdate3Mod.F90
  e3sm.exe           00000000038A20A2  urbanradiationmod         705  UrbanRadiationMod.F90
  *e3sm.exe           0000000001291F42  balancecheckmod_m         657  BalanceCheckMod.F90
  e3sm.exe           000000000129AAEC  balancecheckmod_m         694  BalanceCheckMod.F90
  e3sm.exe           0000000002834E46  firemod_mp_lnfm_i        1812  FireMod.F90

* accumulMod.F90 line 221: a write statement, so not really an error.

* UrbanRadiationMod.F90 line 704 is an endrun call after failing urban net
  longwave radiation balance error.

* BalanceCheckMod.F90 line 694: endrun after failing soil balance error
  check. 

* FireMod.F90 line 1812: regular call to shr_strdata_advance(), so probably
  not really an error.

* Okay, so either we are stopping in the middle when teh abort comes
  through, or we have a number of endruns() happening after failing various
  checks.  

* So what this all means, I think, is that we actually have a number of
  issues at this level - we are failing a number of checks (simultaneously
  and/or asynchronously), plus we get some math errors that lead to core
  dumpy crashes.

! Just for fun, I'm going to run in 32 tasks and see what happens.

11/9/2021

* Okay!  The iac_in file in the run directory was *copied* over from Kate,
  and thus with Alan's mods we need to copy over *his* iac_in from his run
  directory:

  rundir=$scratchdir/run
  adir=/compyfs/divi553/e3sm_scratch/zlnd_ldt2/run
  cp $adir/iac_in $rundir

  I've incorporated this into the latest build instructions.

* So giac.template is never actually called, and iac_in is not generated by
  the E3SM build process but built *by hand*, right now.  And Alan added a
  bunch of stuff to the iac_in file, but did not update giac.template to
  reflect that, but it doesn't matter because giac.template isn't called
  anyway.  


11/9/2021

* Time for another set of instructions on how to clone and build.  Note
  that the input files we copy over to the run directory will probably
  change the next time we have to clone anything.

  # Set these to reflect what directories to clone and build into, and what
  # branches and submodule branches to change to.
  clone=E3SM_current
  build=zatm_v2
  branch=shippert/gcam/active-gcam-debug
  giac_branch=kvcalvin/working
  gcam_branch=e3sm-integration

  mkdir $clone
  git clone git@github.com:E3SM-Project/E3SM.git $clone
  cd $clone
  git checkout $branch
  git submodule update --init --recursive

  cd components/gcam/src
  git checkout $giac_branch
  cd iac/gcam
  git checkout $gcam_branch

  cd ~/$clone/cime/scripts

  ./create_newcase --project e3sm --case ~/$build --compset ZATM --res f19_g16

  cd ~/$build

  ./xmlchange --id "RUN_STARTDATE" --val "2015-01-01"
  ./xmlchange --id STOP_N --val 1
  ./xmlchange --id DEBUG --val TRUE
  ./xmlchange --id NTASKS --val 1
  ./xmlchange -id "NCPL_BASE_PERIOD" -val "year"
  ./xmlchange -id "ATM_NCPL" -val 17520
  ./xmlchange -id "IAC_NCPL" -val 1
  ./xmlchange -id "STOP_OPTION" -val "nyears"
  ./xmlchange --id "JOB_WALLCLOCK_TIME" --val 4:00:00
  ./xmlchange --id "NTASKS_LND" --val 480
  ./xmlchange --id "NTASKS_ATM" --val 480

  cp ~/zlnd_v2/user_nl_gcam .

  ./case.setup --reset
  ./case.setup 

  # Modify this to match the new build directory name
  scratchdir=/compyfs/d3a230/e3sm_scratch/$build
  rundir=$scratchdir/run
  kdir=/compyfs/d3x132/e3sm_scratch/zatm

  cp -r $kdir/input $scratchdir
  cp $kdir/run/log_conf.xml $rundir
  cp $kdir/run/iac_in $rundir
  cp $kdir/run/gcamo_base.csv $rundir
  cp $kdir/run/glm.fut.conf $rundir
  cp $kdir/run/iESM_Dyn_CropPast.nc $rundir
  cp $kdir/run/mksurf_landuse_iESM_720x360.nc $rundir
  cp $kdir/run/restart.* $rundir
  cp $kdir/run/surfdata_360x720* $rundir
  cp $kdir/run/surfdata_iESM_dyn.nc $rundir

  adir=/compyfs/divi553/e3sm_scratch/zlnd_ldt2/run

  cp $adir/iESM_Init_CropPast.nc $rundir
  cp $adir/surfdata_360x720_mcrop_init.nc $rundir
  cp $adir/landuse.timeseries_0.9x1.25_HIST_simyr2015_c201021.nc $rundir
  cp $adir/surfdata_360x720_potveg.nc $rundir
  cp $adir/iac_in $rundir
  cp $rundir/iESM_Init_CropPast.nc $rundir/iESM_Dyn_CropPast.nc
  cp $rundir/surfdata_360x720_mcrop_init.nc  $rundir/surfdata_360x720_mcrop_dyn.nc
  cp $rundir/landuse.timeseries_0.9x1.25_HIST_simyr2015_c201021.nc  $rundir/surfdata_iESM_dyn.nc

  ./case.build --clean-all && ./case.build && ./case.submit

11/8/2021

* Alan's updated instructions for setting up a run with the right
  initialization files (from email from today).  I've modified it a bit so
  we can see what is happening:

--------
  rundir=/compyfs/d3a230/e3sm_scratch/zlnd_v2/run
  adir=/compyfs/divi553/e3sm_scratch/zlnd_ldt2/run

  cp $adir/iESM_Init_CropPast.nc $rundir
  cp $adir/surfdata_360x720_mcrop_init.nc $rundir
  cp $adir/landuse.timeseries_0.9x1.25_HIST_simyr2015_c201021.nc $rundir
  cp $adir/surfdata_360x720_potveg.nc $rundir
  cp $rundir/iESM_Init_CropPast.nc $rundir/iESM_Dyn_CropPast.nc
  cp $rundir/surfdata_360x720_mcrop_init.nc  $rundir/surfdata_360x720_mcrop_dyn.nc
  cp $rundir/landuse.timeseries_0.9x1.25_HIST_simyr2015_c201021.nc  $rundir/surfdata_iESM_dyn.nc
--------

  Basically, we want to have actual initialization files, because the
  dynamic files grow with each run, so if we debug we keep adding to them.
  They still work, but it's better to start with a clean init each time.

? I don't know if we do this every time we build...

11/5/2021

* Alan's branches:

  E3SM: aldivi/gcam/active-gcam/iac2land_transfer
  GIAC: aldivi/mksurfdat_update_7oct

  Kate's branch:

  E3SM: kvcalvin/gcam/active-gcam

  My current branch:

  E3SM: shippert/gcam/active-gcam-debug


* I *think* you can pull a branch that you are not on without changing
 
 locally, by this:

  git pull origin 

XXXXXXXXXX
XX Then you can git merge with that branch to do a full merge.
XXXXXXXXXX

  Nope - it appears that maybe you automatically merge, or maybe there's a
  git option that does this.  Anyway, there's nothing to it but to make the
  pull and hope for the best.

* Yeah, the pull automatically merged and committed.  So if you want more
  control of this, you need to do it in two steps:

  git checkout aldivi/gcam/active-gcam/iac2land_transfer
  git pull
  git checkout shippert/active-gcam-debug
  git merge aldivi/gcam/active-gcam/iac2land_transfer

* If the new branch isn't in there yet, then you may have to pull master
  first and then do the above (or maybe just go back to your branch and
  merge). 

10/29/2021

* So I think we are at the point where we are bombing for non-bug reasons -
  my current theory is that we are not supplying a finidat initialization
  file, so that leads to the negative flux from the methane model, and that
  leads to us failing the carbon balance check (we aren't conserving carbon
  mass from timestep to timestep).  

* Here is what I'm seeing: 

 Beginning timestep   : 2015-01-01_00:00:00
 --WARNING-- skipping CN balance check for first timestep
 --WARNING-- skipping CN balance check for first timestep
    Completed timestep: 2015-01-01_00:00:00
 Beginning timestep   : 2015-01-01_00:30:00
 --WARNING-- skipping CN balance check for first timestep
 Note: sink > source in ch4_tran, sources are changing  quickly relative to diff
 usion timestep, and/or diffusion is rapid.
 Latdeg,Londeg=   19.8947368421052        205.000000000000     
 This typically occurs when there is a larger than normal  diffusive flux.
 If this occurs frequently, consider reducing land model (or  methane model) tim
 estep, or reducing the max. sink per timestep in the methane model.
 Negative conc. in ch4tran. c,j,deficit (mol):        2907           2
   12.3179796521575     
 --WARNING-- skipping CN balance check for first timestep
    Completed timestep: 2015-01-01_00:30:00
 Beginning timestep   : 2015-01-01_01:00:00
    Completed timestep: 2015-01-01_01:00:00
 Beginning timestep   : 2015-01-01_01:30:00
 column cbalance error =  -2.980232238769531E-007        2907
 Latdeg,Londeg         =    19.8947368421052        205.000000000000     
 input                 =   0.000000000000000E+000
 output                =   5.744097029181580E-007
 er                    =    386336967.241103        386336967.241101     
 fire                  =   0.000000000000000E+000
 hrv_to_atm            =   0.000000000000000E+000
 hrv_to_prod10         =   0.000000000000000E+000
 hrv_to_prod100        =   0.000000000000000E+000
 leach                 =   0.000000000000000E+000
 begcb                 =   -3756.59607498606     
 endcb                 =   -386340723.837178       -386334545.774726     
 delta store           =   -386336967.241103     
 ENDRUN:
 ERROR in EcosystemBalanceCheckMod.F90 at line 289                              
 

10/27/2021

* Most (or all?) of the NaNs I'm seeing come from the vegetation variables,
  and from calculating a summary to write out to a history file -
  otherwise, these variables/structure elements don't seem to be used for
  anything in the code.

  My conjecture is these are hooks for further development - they get
  initialized to NaN and then never considered again (at least in this old
  version of clm).  This would imply that there are a bunch of history
  files being written with NaNs in the summary fields, but apparently
  nobody looks at those, either.

* Latest error - still in vegetation calculation, but this time we end up
  with a negative lw flux in veg_ef%eflx_lwrad_out, which causes the
  conversion back to temperature in lnd2atm_minimal() (c. line 115 of
  lnd2atmMod.F90, at the end of the function) to crash, since it takes the
  fourth root (or sqrt(sqrt(flx)), technically).  

  I'm doing some detailed write statements to find out what is going on -
  see ~/zlnd_v2/lnd.log.eflx_lwrad_out.analysis.  The tags are this:

    TRS4 - just before the p2g() call for eflux_lwrad_out (other variables
    	   use this, so I needed a tag to find the pertinent output).
	   (lnd2atmMod.F90) 

    TRS3 - (subgridAveMod.F90, p2g_1d())
           variables used in the p2g conversion, for gridcell (g) = 2907,
           where the negative flux happens.  The columns are:

	   p, c, l, g, parr(p), scale_c2l(c), scale_l2g(l), garr(g), sumwt(g)

	   parr(p) is the veg_nf p-indexed input variable (in this case
	   veg_nf%eflux_lwrad_out), and garr(g) is the output g-indexed
	   variable we are building. (Above, garr(g) is the current,
	   pre-calculation value, before adding in this patch's
	   contribution.) 

	   garr(g) is set to zero if sumwt(g) is zero, so it resets at
	   the start of each patch loop.

    TRS5 - the value of garr(g) after the above calculation.

  We iterate over several patches (p) to build up the g-based average, so
  as the sumwt(g) approaches 1.0 we approach the final value, and at the
  end that's what's used in the flux->T calculation that's crashing.

* It looks like we do an initialization, then the first time step, then
  tries another time step.  But!  The problem seems to be in the CN balance
  check, that they skip for the first timestep.  We end up with a negative
  flux in the methane model (and maybe elsewhere, but this is where it
  crashes).  Here's the note in the log:

     Beginning timestep   : 2015-01-01_00:30:00
     --WARNING-- skipping CN balance check for first timestep
     Note: sink > source in ch4_tran, sources are changing  quickly relative to diff
     usion timestep, and/or diffusion is rapid.
     Latdeg,Londeg=   19.8947368421052        205.000000000000     
     This typically occurs when there is a larger than normal  diffusive flux.
     If this occurs frequently, consider reducing land model (or  methane model) tim
     estep, or reducing the max. sink per timestep in the methane model.
     Negative conc. in ch4tran. c,j,deficit (mol):        2907           2

* To me, this suggests that we aren't initializing things very well, so
  that second timestep leads to a too-rapid diffusion and we get a negative
  flux.  This *might* imply that we need better initialization, or it might
  just be normal and the (flux)^-4 calculation with a negative flux just
  leads to an NaN somewhere that nobody cares about, at least in this
  initial step.

* Okay, so we'll just trap out the flux^-4 calc for flux < 0, and continue on.

10/22/2021

* veg_nf%livestemn_to_litter(p)
  veg_pf%livestemp_to_litter(p)

  Both set to NaN.

* Other NaNs in dyngrid stuff, and veg_cs% functions - grep for 'TRS'
  comments.

* lnd2atmMod.F90, line 109, lnd2atm_vars%eflx_lwrad_out_grc(g) is large and
  negative for g = 2907 (out of 1:5663), value = -146196.658095096.

  The calculation takes the 4th root, so this crashes.

  See the output of :
  /compyfs/d3a230/e3sm_scratch/zlnd_v2/run/lnd.log.239063.211022-130553

  This comes after a p2g() call, so check veg_ef%eflx_lwrad_out(), which is
  (possibly) an input to that transformation function.

10/5/2021

* Heh, lots of notes I got to track down somewhere else.

* Kate's current build is in /compyfs/d3x132/zlnd_v2, there's a readme with
  some configuration information.

* Building Kate's branch:

  mkdir E3SM_current
  git clone git@github.com:E3SM-Project/E3SM.git E3SM_current
  cd E3SM_current
  git checkout kvcalvin/gcam/active-gcam
  git submodule update --init --recursive

  # Submodule checkout:
  # giac branch: kvcalvin/working
  # gcam branch: e3sm-integration

  cd components/gcam/src
  git checkout kvcalvin/working
  cd iac/gcam
  git checkout e3sm-integration

  cd ~/E3SM_current/cime

  ./create_newcase --project e3sm --case ~/zlnd_v2 --compset ZLND --res f19_g16

  cd ~/zlnd_v2

  ./xmlchange --id "RUN_STARTDATE" --val "2015-01-01"
  ./xmlchange --id STOP_N --val 1
  ./xmlchange --id DEBUG --val TRUE
  ./xmlchange --id NTASKS --val 1
  ./xmlchange -id "NCPL_BASE_PERIOD" -val "year"
  ./xmlchange -id "IAC_NCPL" -val 1
  ./xmlchange -id "STOP_OPTION" -val "nyears"
  ./xmlchange --id "JOB_WALLCLOCK_TIME" --val 4:00:00

  cp /compyfs/d3x132/iac_E3SM_working_Z/user_nl_gcam .

  ./case.setup --reset
  ./case.setup 

  # Modify this to match the new build directory name
  scratchdir=/compyfs/d3a230/e3sm_scratch/zlnd_v2
  rundir=$scratchdir/run
  cp -r /compyfs/d3x132/e3sm_scratch/zlnd_v2/input $scratchdir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/log_conf.xml $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/iac_in $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/gcamo_base.csv $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/glm.fut.conf $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/iESM_Dyn_CropPast.nc $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/mksurf_landuse_iESM_720x360.nc $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/restart.* $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/surfdata_360x720* $rundir
  cp /compyfs/d3x132/e3sm_scratch/zlnd_v2/run/surfdata_iESM_dyn.nc $rundir

  ./case.build --clean-all
  ./case.build 
  ./case.submit

3/16/2021

* Git merge strategy syntax, for git pull etc.

  git merge -s recursive -Xtheirs # short options
  git merge --strategy recursive --strategy-option theirs # long options

* Trying to update the e3sm-integration branch for iac/gcam:

  # save definitions.h and wrapper.cpp print changes, and revert
  # so the pull won't complain
  git stash 

  # Try to pull the latest branch, and hope their are no merge issues. 
  git pull origin e3sm-integration

* Sadly, that didn't work, for whatever reason - lots of merge issues.
  First, reset --hard to get back to my unpulled, unmerged, unfubar'd code,
  then try a vanila git pull:

  git reset --hard
  git pull

* Finally, try to use the merge strategy above - hopefully, blast
  everything I have that conflicts and grab just the latest stuff in the
  e3sm-integration repo. 

  git reset --hard
  git pull -s recursive -Xtheirs

3/11/2021

* Okay, so the easiest thing is to just copy over from the E3SM_test
  directory:

  cd ~/E3SM_gcam_core_submodule/components/gcam/src
  cp ~/E3SM_test/components/gcam/src/iac/bld/giac.template iac/bld/giac.template 
  cp ~/E3SM_test/components/gcam/src/iac/bld/iac_glm_namelist_items.txt iac/bld/iac_glm_namelist_items.txt
  cp ~/E3SM_test/components/gcam/src/iac/coupling/gcam2glm_mod.F90 iac/coupling/gcam2glm_mod.F90 
  cp ~/E3SM_test/components/gcam/src/iac/coupling/updateannuallanduse_v2.c iac/coupling/updateannuallanduse_v2.c 
  cp ~/E3SM_test/components/gcam/src/iac/glm/glm.conf iac/glm/glm.conf 
  cp ~/E3SM_test/components/gcam/src/iac/glm/glm.fut.conf iac/glm/glm.fut.conf 
  cp ~/E3SM_test/components/gcam/src/iac/glm/glm_future.c iac/glm/glm_future.c 

* The gcam_comp_mod, gcam_var_mod, iac_data_mod, iac_init_mod are my branch
  changes, so we are simply trying to merge in the giac-update changes that
  matter - the glm stuff that Alan added that I didn't properly bring over
  to my compy branch but that I haven't otherwise touched.

* Kate's email from 2/25/21 on errors:

=====
@Xcall gcam_init_mod

  I can get this to successfully complete with the changes I made + the
  hardcoding of strings as they go into GCAM. I think if we can null
  terminate the strings in fortran this problem will be solved without any
  workarounds. 

@ call gcam2glm_init_mod

  This fails on line 172 of gcam2glm_mod.F90. It looks like it can’t open a
  netCDF file, but I can’t tell which file or why it won’t open. 

  The solution here is to move everything to namelists rather than the
  silly cdata structure - so use gcam_var_mod read and set the variables
  there - mods to gcam2glm

@ call glm_init_mod

  This fails on line 88 of glm_comp_mod.F90. It says “Attempt to use
  pointer GLMI when it is not associated with a target” 

@ call iac_domain_mct

  This fails on this line “call mct_gGrid_importIAttr(dom_z,"mask"
  ,idata,lsize)” because it cannot find the attribute “mask” 

@ call iac_export

  I get a bunch of errors here all essentially related to indices being out
  of range. It does not like that LAT starts at -90 (it wants it to start
  at 1) or that LON starts at 0 (it wants it to start at 1). I tried
  offsetting both, but then it gets out of range on the high end (sees
  index 193 when only defined until 192). Similarly, the statement
  iac_ctl%lat(g) will fail because g reaches 193 and should stop at
  192. And, index_z2x_Sz_pct_pft(p) starts at 0 and it wants it to start at
  1. 
 
3/10/2021

* Here's the output of 'git diff working' when checking out the giac-update
  branch of gcam-core in the ~/E3SM_test/components/gcam/src directory:

=========
  [d3a230@compy01 coupling]$ egrep '\+\+\+' *.diff
  +++ b/.gitmodules
  +++ b/iac/bld/giac.template
  +++ b/iac/bld/iac_glm_namelist_items.txt
  +++ b/iac/coupling/gcam2glm_mod.F90
  +++ b/iac/coupling/gcam_comp_mod.F90
  +++ b/iac/coupling/gcam_var_mod.F90
  +++ b/iac/coupling/iac_data_mod.F90
  +++ b/iac/coupling/iac_init_mod.F90
  +++ b/iac/coupling/updateannuallanduse_v2.c
  +++ b/iac/gcam
  +++ b/iac/glm/glm.conf
  +++ b/iac/glm/glm.fut.conf
  +++ /dev/null
  +++ /dev/null
  +++ b/iac/glm/glm_future.c
=========

* Argh.  This is so hard to keep track of.  But I *think* this means we
  need to copy the gcam2glm_mod.F90, updateannuallanduse_v2.c,
  glm_future.c, and the glm comp files from E3SM_test (giac-update branch)
  to E3SM_gcam_core_submodule (working branch), then commit and push.

* Also, check on the diffs of these others - maybe need the giac.template
  and iac_glm_namelist_items.txt, too, especially if they don't exist in
  gcam_core. 

2/10/2021

* Here are the xmlchanges you want to do after newcase for zero level
  debugging: 

    ./xmlchange --id STOP_N --val 1
    ./xmlchange --id DEBUG --val TRUE
    ./xmlchange --id NTASKS --val 1

* So, here are my build instructions for my branch.  Also, check out
  ~/NOTES.build, for an ever so slightly more generic version, to make it
  easier to do a bunch of tests:

  mkdir E3SM_test
  git clone git@github.com:E3SM-Project/E3SM.git E3SM_test
  cd E3SM_test
  git checkout shippert/gcam/active-gcam-submod
  git submodule update --init --recursive

  # Modify definitions.h here if necessary
  cd cime/scripts
  ./create_newcase --project e3sm --case ~/iac_E3SM_test_Z --compset Z --res f19_g16

  cd ~/iac_E3SM_test_Z/
  ./xmlchange --id STOP_N --val 1
  ./xmlchange --id DEBUG --val TRUE
  ./xmlchange --id NTASKS --val 1
  ./case.setup
  ./case.build 

  ./case.submit

2/3/2021

* Kate's pnnl id is d3x132

  /compyfs/d3x132/gcam-e3sm/cpl/data

  for config files.

12/1/2020

* Man, it's been a frustrating couple of weeks - I can't get the namelist
  defaults to work, and the compy system is now balking at mpi_bcast.  I'm
  trying to restrict the number of tasks via:

  ./xmlchange --id NTASKS --val 4

  ...and hopefully that will help setup the right mpi_bcast setup 

* ./pelayout says how things are configured.

* Might need to do stuff like 

  ./xmlchange --id NTASKS_IAC --val 1

  ...etc.

* The reason I'm restricting the ntasks is I'm getting this message:

  MPIR_Bcast_binomial(265)..............: message sizes do not match across processes in the collective routine: Received 98 but expected 4096)

* The former pe layout had 40 tasks for each component, which is excessive,
  and that adds up to 360 tasks so I don't know where the 98 is coming
  from.  And I certainly don't know why it expects 4096 - perhaps the IAC
  bcast I've got setup doesn't have something initialized like it should
  be. 

! Ah, wait! It's the *message size*, not the number of procs - we are
  telling it we are sending 4096 bits but only sending 98.  I've got
  something initialized wrong.

11/10/2020

* Email from Alan way back when:

============

  The land grid is defined in the mksurfout_init namelist file.
  This file is an existing dynamic land surface file that is used as a
  template for updating with the annually calculated land cover. 

  I just generated land surface files for 0.9x1.25 to create the new
  mksurfout_init file (on Compy), as I found a note in my giac branch that
  this file needs to be updated. So far I have just put this file on compy,
  but I will eventually put it on NERSC and Blues as well:
  /compyfs/inputdata/iac/giac/glm2iac/landuse.timeseries_0.9x1.25_template_simyr2015-2100_c201021.nc
  I am designating a future file in case the years matter (it looks there
  used to be different files for the historical and future). I also created
  the historical files. This future file has been renamed as a template
  from the ssp5rcp85 scenario that was used to create it. 

  I am not sure how the grid used to be read in, as the new file includes
  only the cell centers, and not the edges (the old files included
  both). The dimensions are uniform (0.9424084 in lat and 1.25 in lon), but
  at the poles there are half cells in the lat dimension. The relevant
  variables that you are looking for (if I recall correctly) are: AREA -
  grid cell area in km^2 LANDFRAC_PFT - fraction of grid cell that is land
  PFTDATA_MASK - land mask LATIXY - center lat of grid cell LONGXY - center
  lon of grid cell 

  It looks like any finite-element grid (e.g., 0.9x1.25, 0.5x0.5) has these
  variables indexed by lat and lon, but the spectral grids (e.g., ne30np4)
  have these indexed by land grid cell. iESM currently supports the lat-lon
  grid indexing, but not the spectral grid indexing.
============ 

11/3/2020

* .../E3SM_gcam_core_submodule/components/gcam/bld/namelist_files

8/21/2020

* Aha, found the documentation for the version of Totalview (2017) on   Compy:

  https://docs.roguewave.com/totalview/2017.3/pdfs/TotalView_for_HPC_User_Guide.pdf

* Basically, in .case.run.sh:

  module load totalview   totalview srun -a <...>

  The trick is the -a in the right place, not -args before or not some   other syntax, which you find in the documentation for newer (or at least   different) versions of totalview.  To tell the truth, this documentation   doesn't even get it right (it wants -args srun), but this -a after worked   for something like the SGI machines, so I tried it, and it finally seems   to start things up correctly.

8/19/2020 13:29:33 PDT

* So, Kate mentioned in the phonecall that the standard we need for gcam is   -std=c++14, which explains a couple of the errors I've been seeing.

  But I can confirm that if the compile line goes -std=c++14 -std=gnu99, we   have problems, whereas if it goes -std=gnu99 -std=c++14 we are fine   (although we still have warnings about deprecated calls, etc.)

* In configure_compilers.xml, we see CFLAGS set to -std=gnu99 and CXXFLAGS   set to -std=c++14, but I can see in the compile line that c++ flags come   first - apparently when you build the compile line you use  $(CXXFLAGS)   $(CFLAGS) in that order somewhere.

* The issue here is that updateannuallanduse.c in gcam is straight C, and   may need the gnu99 standard.  So I may not be able to overwrite the   CFLAGS -std call to use c++14 without breaking that piece of code.  We'll   see.

* Simply modifying config_compilers.xml didn't work - I may have to   case.setup to get the build MAkefiles right - there's a Macros.make file   in the build directory that was probably made by case.setup.

  Of course, the Tools/Makefile has CXXFLAGS+=$(CFLAGS), so when building a   c++ we tack the CFLAGS on the end.  Instead, we should make:

  CXXFLAGS := $(CFLAGS) $(CXXFLAGS)

  ...to prepend.

! Huh.  Well, when modifing config_compilers.xml, I had to actually   *remove* Macros.make before calling case.setup again in order to get the   right CXXFLAGS.  Whatever, it should work now.

8/19/2020

* From Kate:

  Here are the modules and libraries that work with GCAM on compy.

  Modules:     module purge      module load intel/19.0.3 mvapich2/2.3.1 netcdf/4.6.3 pnetcdf/1.9.0 mkl/2019u3

  Libraries:

  boost: /qfs/people/crai556/boost_intel_19.0.3   xerces: /qfs/people/crai556/xerces_intel_19.0.3

* Excellent - all those compiler mods are already set up for compy in   config_machines.xml.  I just need to modify buildlib to load boost and   xerces correctly.

  Also, .../cime/scripts/Tools/Makefile to point XDIR to the libs for xerces

8/18/2020

* Once again, for compy:

  mkdir E3SM_gcam_core_submodule
  git clone git@github.com:E3SM-Project/E3SM.git E3SM_gcam_core_submodule
  cd E3SM_gcam_core_submodule/
  git checkout shippert/gcam/active-gcam-submod
  git submodule update --init --recursive

  cd cime/scripts
  ./create_newcase --project e3sm --case ~/iac_ZLND_test --compset Z --res f19_g16

  cd ~/iac_ZLND_test/
  ./case.setup
  ./case.build 

* If you do the submodule update *before* checking out the branch, you get   a bunch of git errors, probably because master has a ton of new   submodules that muck up everything.  I think they've moved CIME and other   stuff in to a submod.  Whatever, just do this in order.

* Note, you need --project e3sm in create_newcase - that is new.

* Once again, you have to modify:

    .../components/gcam/src/iac/gcam/cvs/objects/util/base/include/definitions.h

  so that HAVE_JAVA and USE_HECTOR are 0.

8/10/2020

* salloc -N 4 -t 30:00 -p acme-small -A condo

  This now seems to be giving me some problems allocating - fairly long   delays.  I don't know if that's something new, or if I got lucky in the   past, or what.  Is there an interactive queue on anvil?  Go ahead and try   -p interactive and see...

  Well, no interactive, but it might be in an maintenance mode.  Let me   just try to run regularly without a salloc call and see.

* Um, I really have no notes about where to change acme to acme-small?   Sheesh.  Apparently I modify env_batch.xml to replace acme with   acme-small.

* And, I'm right, the maintenance mode means my job is submitted, but not   accepted - that's why the salloc didn't work just now.

* Worried about the X11 lag - the DMF is much worse, for whatever reason,   but I still see a minute delay to pop up an xterm.

6/10/2020

* By default, E3SM builds with -DNDEBUG command line option to the   compilers, whcih turns off assert() functions.  But when you specify the   DEBUG=TRUE E3SM build option (in the .xml files), it turns off -DNDEBUG,   which is why building a debug version of es3m.exe suddenly found all   these problems with assert()s in gcam.

  Kate says that there are a ton of assert() functions throughout e3sm that   are obsolete - the problems I found were all because of obsolete variable   names, either because they'e been superceded or renamed something else.   They got around this legacy problem by simply always compiling with   -DNDEBUG - otherwise, they are going to have to clean up all their   assert() calls.

  The DEBUG=TRUE E3SM build option basically turns on -g -O0 - and there is   no reason why this should be incompatible with -DNDEBUG, which is   actually a C/C++ standard used by assert.h to void out assert() calls if   set. 

  So, it is my claim that having E3SM turn off NDEBUG if DEBUG is set is   incorrect - they can coexist peacefully (i.e. -g -O0 -DNDEBUG is   perfectly valid and functional).  I kludged the Tools/Makefile to always   set -DNDEBUG in CPPDEFS no matter what, but realistically it should be   allowed in it's own E3SM build options.  I tried setting it using   xmlchange, but that won't me set an environment variable that's not   already defined, so we'd have to modify the schema or something.

* Anyway, right now my kludged solution has let me build a debugging   version of e3sm.exe with asserts all turned off.  So, yay for me.

6/9/2020

* So, this is weird - I suddenly cannot compile gcam anymore.

* Acutal error:

  /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/util/base/source/calibrate_resource_visitor.cpp(152): error: class "SubRenewableResource" has no member "gdpSupplyElasticity"

  calibrate_resource_visitor.cpp is the only place a straight   "gdpSupplyElasticity" is used without some kind of prefix; it's in an   assert that it is 0.

* Conjecture: this came about because I ran xmlchange to change to DEBUG and   also to change NTASKS - I think the latter forced a ./case.build   --clean-all and maybe another thing, and that may have forced a   recheckout of the repo, with the faulty calibrate_resource_visitor.cpp file.

* I don't have any notes below on dealing with this, but when originally   building gcam I had to hide a number of standalone files and some others   for use with gcam builds in different contexts.  So my conjecture is that   I originally hid this file, and the checkout brought it back.  But I   don't know...

* The other possibility is that moving to ac.shippert has messed soemthing   up in buildlib and it's compiling wrong.  Or something.

* Anyway, my current solution is to hide this offending source file.  If   that doesn't work, maybe remove the offending assert?

  I checked out a whole new build into ~/iac_ZLND_debug, but the native   build failed in the same way, even without any xmlchange mods.

* So, removing the assert worked, as did removing the file altogether...at   least, they both lead to this failure in   /land_allocator/source/land_leaf.cpp, line 207:

    assert( mCarbonContentCalc.get() );

  ...which gets me:

    /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/land_allocator/source/land_leaf.cpp(207):     error: expression must have class type 

  Yeah, I don't know.  So I commented out *that* assert, as well...

* Now this:

  /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/containers/source/scenario.cpp(190): error: identifier "world" is undefined   /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/containers/source/scenario.cpp(200): error: identifier "modeltime" is undefined   gmake: *** [scenario.o] Error 2   /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/sectors/source/sector.cpp(389): error: identifier "subsec" is undefined   /blues/gpfs/home/ac.shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/sectors/source/sector.cpp(404): error: identifier "subsec" is undefined


6/3/2020

* config_compilers.xml for compiler options.  Probably somewhere in   config_machines.xml for machine specific options.

5/22/2020

* Some links to help runnign and debugging:

1 https://acme-climate.atlassian.net/wiki/spaces/ED/pages/98992379/Anvil+-+ACME+s+dedicated+nodes+hosted+on+Blues 2 https://www.lcrc.anl.gov/for-users/using-lcrc/running-jobs/running-jobs-on-blues/

* To make an interactive session, use salloc (from 2, above).  

* The idea seems to be to use ./preview_run to help, and to modify some of   the .sh scripts to start up totalview.  Or, it could be to run totalview   withe e3sm.exe executable, after setting up a salloc, and calling from   there.  Time to review the NERSC doc, below (5/11/2020).

5/13/2020

* Ah, I think I know how to set environment variables - it's hidden in the   xml stuff, which you modify with ./xmlchange:

    ./xmlchange --id STOP_N --val 1
    ./xmlchange --id DEBUG --val TRUE
    ./xmlchange --id ROOTPE_CPL --val 2
    ./xmlchange --id NTASKS --val 4
    ./xmlchange --id NTASKS_ATM --val 2
    ./xmlchange --id NTASKS_CPL --val 2
    ./xmlchange --id NTASKS_ESP --val 1
    ./xmlchange --id NTASKS_OCN --val 2

* This was under the "debugging on a workstation with ddd" in the   confluence page, but tihs does sound familiar.  So, this is how you set   up the ntasks and stuff like that, but (mainly), how to turn on DEBUG and   a few other things.  My guess is "STOP_N" tells you how many steps to   run, so limiting it to one makes a lot of sense in a debugging context.

5/11/2020

* Confluence page on debugging with totalview on NERSC:      https://acme-climate.atlassian.net/wiki/spaces/ED/pages/185008204/Debugging+with+Totalview+on+NERSC

@ Need to figure out the interactive queue on Anvil, if there is one.  So,   I need an anvil web page.

5/8/2020

* Need to modify the .../cime/scripts/Tools/Makefile to deal with dynamic libraries

* To add a shared library directory to the compile line, you need both   -L$(DIR) and to send $(DIR) to the linker via the -rpath command.  The   arcane incantation to make this happen is either of these:

  GLIBS := -Wl,-rpath,$(XDIR) -L$(XDIR) -lxerces-c    GLIBS := -Xlinker -rpath -Xlinker $(XDIR) -L$(XDIR) -lxerces-c -lstdc++

* The other possiblity is to force static linking for xerces, which makes   sense now with my local build:

  GLIBS := $(XDIR)/libxerces-c.a    GLIBS := -Wl,-Bstatic -L$(XDIR) -lxerces-c -Wl,-Bdynamic

* To keep the ldd from having netcdf not found, you have to do something   similar to the dynamic linking:

  SLIBS += -Xlinker -rpath -Xlinker $(LIB_NETCDF_FORTRAN)  -Xlinker -rpath -Xlinker $(LIB_NETCDF_C)

  I'm not sure if we need these different fortran and C links, but it   worked, so there you go.

  I was unable to statically link netcdf - it apparently dynamically links   with HDF libraries so it can't find them if you statically link.

* At any rate, I'm not sure why we don't have the dynamic load path already   by default when setting things up - I think that may be a failing of the   machine configuration for anvil.

* Regardless, after all that we still have these errors:

  [shippert@blueslogin4 bld]$ ldd e3sm.exe | grep found   ./e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by ./e3sm.exe)   ./e3sm.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by ./e3sm.exe)   ./e3sm.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by ./e3sm.exe)   ./e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by ./e3sm.exe)   ./e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /blues/gpfs/home/shippert/local/lib/libxerces-c-3.2.so)   ./e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /blues/gpfs/home/shippert/local/lib/libxerces-c-3.2.so)

  I don't know what this means or if it's a problem, but it doesn't stop   e3sm.exe from trying to run.

* I turned USE_CXX to TRUE as part of all this, so it's possible both the   error and the solutions above might be linked to that.  I'm testing what   happens if you turn it off again.

  Well, if you turn both USE_CXX off and remove the -lstdc++ argument then   it definitely fails, as it kind find the standard libraries.

  If you turn USE_CXX on and remove -lstdc++, it has no effect - the thing   compiles and runs the same way.  If you add -lstdc++ and remove USE_CXX,   it also compiles and seems to run the same way.  

  So, at least one of: -lstdc++ or USE_CXX; probably USE_CXX, which will be   most portable and standardized.

! I need to figure out how to set these options I'm seeing - things like   USE_TRILIONOS and DEBUG.  It looks like they are environment variables   set by whatever calls the make - which is probably, ultimately,   case.build.  Maybe one of those env_*.xml files - DEBUG is set in   env_build.xml, but just reading the inline documentation there are a lot   of variables set there that they say DO NOT EDIT.  Well, DEBUG isn't one   of them, so I guess I can force it that way, but I feel like there is a   configuration even further upstream that I need to set.  Probably one or   more of those config*.xml files scattered about - possibly in the compset   definition?

  Obviously, I need to set USE_GCAM somewhere in here, too.  

5/6/2020

* Can't find libxerces-c, and some other stuff:

[shippert@blueslogin4 iac_ZLND_case]$ ldd /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe | grep found /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe) /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe) /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe) /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by /lcrc/group/acme/shippert/acme_scratch/anvil/iac_ZLND_case/bld/e3sm.exe) 	libnetcdff.so.6 => not found 	libnetcdf.so.11 => not found 	libxerces-c-3.2.so => not found

  Working through the Makefile now to see how these shared libraries are loaded. 	
4/21/2020

* Finally merged in Xioying's compsets:

  $ git merge remotes/origin/acme-y9s/gcam_compset      ...modified this file        cime/config/e3sm/allactive/config_compsets.xml

* It looks like the GCAM compsets are:

  BGCEXP_BCRC_CNPRDCTC_20TR_GCAM   G_BG1850CN

4/17/2020

* Input files on compy corresponding to iac runs:

  /compyfs/dtn/divi553/iac_16apr2020.tar

  From Alan's email on 4/17/2020:

---      I have updated the NERSC e3sm/inputdata/iac directory to include the      new iESM files and all of the original iESM files. So this directory      should be able to support runs from the original iESM V1. I also tried      to make sure the needed files outside of the iac directory are also      present in the e3sm/inputdata directory tree (the only ones of these I      could find that were not gcam files are the      …/lnd/clm2mappingdata/maps/<grid>/ mapping files). 

     I then zipped the iac folder and transferred it to compy and unzipped      it in place. The iac tarball on compy is:      /compyfs/dtn/divi553/iac_16apr2020.tar. It is about 270GB. My dtn      directory is readable  - let me know if globus doesn’t let you access      it. 

     If you only need to cherry-pick the new files the best place to      identify these would be in the iac_glm_namelist_items.txt file (there      is a list at the end of the file), which is attached because I had to      update two lines (it is also updated on giac/aldivi/glm-update). The      updated lines are now: 

     gcam2glm_baselu = "\$DIN_LOC_ROOT/iac/giac/glm/inputs/initial/initial_state_LUH2_2015_v3.nc"

     I also transferred the relevant mapping files for 0.9x1.25 from NERSC      to compy, as the needed ones were not there. There are already newer      half-degree mapping fiiles on compy that may work, but so far it      doesn’t seem like we are going to support half-degree iESM in V2; we      never really supported it previously, except for historical runs,      which we can now do outside of iESM. ---

4/15/2020

* The new compsets are in branch:

     remotes/origin/acme-y9s/gcam_compset

  I'm tempted to simply merge:

     $ git merge remotes/origin/acme-y9s/gcam_compset

  ...but let me review this a little more to make sure it won't fubar   anything up.  I can always revert, but I don't really trust git (which is   dumb, git is a lot smarter than I am - I actually don't trust myself    not to defeat git's idiot-proofing).

* Merged in Alan's giac branch - go to component/gcam/src/iac, and:

     $ git merge remotes/origin/aldivi/glm-update

  It worked well - no conflicts, so it merged directly and launched an   autocommit.  There were a couple bugs to fix before it would build under   ZLND, mostly mixing up the 1-D gcam variables with 2-D glm variables.   (Actually, both have one more dimension, since we have a time dim for   next year vs. this year, since everything depends on the gradiant between   the two.)

4/13/2020

* Notes from gcam/atm phone call:

* Kate will provide 12 surface values (into cam_in(c)%fco2_iac, as before)   and 24 two-level vertical co2 flux values (which will require adding new   elements to cam_in(c) to carry them forward).

  To use this then:

1 split into 36 separate fields to couple in iac 2 in atm_import_export.F90 (probably), reconsitute into a 2D fco2_iac(i,t)   array and a 3D fco2_iac_vertical(i,t,2) array, to putin cam_in(c). 3 In atm_import_export.F90, or somewhere else, figure out how to extract   given values for the given month (i.e. figure out what month index t to   use). 4 Modify existing cam code to take fco2_iac_vertical() rather than read   from input file as we currently do. 5 Figure out what additional steps we need to make fco2_iac_vertical()   consistent and to error-check it, and to (maybe) interpolate onto the cam   vertical co2 grid in a reasonable way.

3/25/2020

* The issue is building with hector - I've tried the trick of putting all   the source and include directories in the buildlib, but I *think*, after   a bunch of flailing around, that the problem is that some of the include   files within hector are named the same as those within other parts of   gcam - in particular, logger.hpp.  If you put both directories where   these conflicting versions of logger.hpp live into the buildlib, then   either the gcam one or the hector one includes the wrong file, and that   causes a build clash.

* I believe this comes about because of the buildlib method.  If you build   hector into its own library, and then build gcam while linking with   -lhector, then this whole include file things takes care of itself.  But   since I'm putting all the source code into one big dumb libiac.a, then we   have this potential for include file clashing.

XXXXXXXXXXXX * An added complication is that (for hector, at least, and probably   elsewhere) there are partial paths included - so, things like

  #include core/logger.hpp

  This means that I have to include the directory *above* core into the   buildlib, which means I have to add a whole bunch of directories into   buildlib and thus increase the likelihood of a clash. XXXXXXXXXXXX

* Okay, let's at least check on this - find on logger.hpp and see if I can   make sure of the clash...as you can see, I've xxxed it out, so we do NOT   have a conflict with logger.hpp.  But we *do* have multiple logger.cpp.   Maybe that's the problem, you can't link with two files with the same   name?  I would think the namespace calling would prevent the actual   functions from conflicting - maybe we build all the .o in one place, so   we end up overwriting and/or not compiling one version?

  Rename the hector logger.cpp to hector_logger.cpp and see if that fixes   things. 

* Okay, as far as I can tell, other than main.cpp for test routines, these   are the only filename clashes:

  ./cvs/objects/util/logger/source/logger.cpp   ./cvs/objects/containers/source/dependency_finder.cpp

  ./cvs/objects/climate/source/hector/source/core/logger.cpp   ./cvs/objects/climate/source/hector/source/core/dependency_finder.cpp

* This really doesn't make any sense to me, but I'm going to try just   renaming these files and hoping for the best.

3/24/2020

* [UPDATE: 6/3/2020] The *ccsminterface* functions were replaced with   *e3sminterface* functions in   ./src/iac/gcam/cpl/source/GCAM_E3SM_interface_wrapper.cpp.  The hector   and java stuff is set in definitions.h, and probably should be set as   compiler options in the build system, once I figure that out.

* WTH!      This is in ~/iac_Z_case - building the straight Z case, which *just*   *worked* last week!

------------------------ Building e3sm with output to /lcrc/group/acme/shippert/acme_scratch/anvil/iac_gcam_submod/bld/e3sm.bldlog.200324-122658  /blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/coupling/gcam_comp_mod.F90:309: undefined reference to `deleteccsminterface_'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/coupling/gcam_comp_mod.F90:167: undefined reference to `initccsminterface_'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:456: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, Hector::message_data const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:538: undefined reference to `Hector::Core::run(double)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:803: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:804: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:805: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:747: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:750: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:706: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, Hector::message_data const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:707: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, Hector::message_data const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:708: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, Hector::message_data const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:709: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:348: undefined reference to `Hector::Core::shutDown()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:362: undefined reference to `Hector::Core::Core()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:363: undefined reference to `Hector::Core::init()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:365: undefined reference to `Hector::INIToCoreReader::INIToCoreReader(Hector::Core*)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:366: undefined reference to `Hector::INIToCoreReader::parse(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:367: undefined reference to `Hector::Core::addVisitor(Hector::AVisitor*)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:368: undefined reference to `Hector::Core::prepareToRun()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:404: undefined reference to `Hector::Core::run(double)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:404: undefined reference to `Hector::INIToCoreReader::~INIToCoreReader()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:365: undefined reference to `Hector::INIToCoreReader::~INIToCoreReader()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:353: undefined reference to `Hector::CSVOutputStreamVisitor::CSVOutputStreamVisitor(std::ostream&, bool)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:665: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, Hector::message_data const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:803: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:804: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:805: undefined reference to `Hector::Core::sendMessage(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:188: undefined reference to `Hector::Logger::getGlobalLogger()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:188: undefined reference to `Hector::Logger::open(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, bool, Hector::Logger::LogLevel)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:194: undefined reference to `Hector::Core::shutDown()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:196: undefined reference to `Hector::Core::Core()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:197: undefined reference to `Hector::Core::init()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:199: undefined reference to `Hector::INIToCoreReader::INIToCoreReader(Hector::Core*)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:200: undefined reference to `Hector::INIToCoreReader::parse(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:201: undefined reference to `Hector::INIToCoreReader::~INIToCoreReader()'

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/gcam/src/iac/gcam/cvs/objects/climate/source/hector_model.cpp:199: undefined reference to `Hector::INIToCoreReader::~INIToCoreReader()'

ERROR: BUILD FAIL: buildexe failed, cat /lcrc/group/acme/shippert/acme_scratch/anvil/iac_gcam_submod/bld/e3sm.bldlog.200324-122658 ---------------

* Okay, a bunch of hector stuff, which I guess we'll figure out.  But the   gcam_comp_mod.F90 stuff?  What happened here?

3/18/2020 2020-03-18 10:27:14

? in clm_varctl.F90, I define a logical 'iac_active', which I use to   determine whether clm knows to link with an iac component.  I think   that's the best place to define this, but this means I need to set it in   a namelist somewhere.

3/18/2020

* Working from home, so more junk in here as I try to keep track of things.

  From clm build, ZLND compset, E3SM_gcam_submod branch ====================================== /blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(72): error #6562: A data initialization-expr is not valid for this object.   [INDEX_L2X_SL_HR]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(73): error #6562: A data initialization-expr is not valid for this object.   [INDEX_L2X_SL_NPP]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(74): error #6562: A data initialization-expr is not valid for this object.   [INDEX_L2X_SL_PFTWGT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(25): error #6279: A specification expression object must be a dummy argument, a COMMON block object, or an object accessible through host or use association.   [IAC_NPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(72): error #6841: An automatic object must not appear in the specification part of a module.   [INDEX_L2X_SL_HR]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(73): error #6841: An automatic object must not appear in the specification part of a module.   [INDEX_L2X_SL_NPP]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(74): error #6841: An automatic object must not appear in the specification part of a module.   [INDEX_L2X_SL_PFTWGT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(317): error #6402: prPromoteSym : Illegal KIND & CLASS mix   [IAC_NPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(319): error #6404: This name does not have a type, and must have an explicit type.   [P]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(319): error #6063: An INTEGER or REAL data type is required in this context.   [P]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(319): error #6402: prPromoteSym : Illegal KIND & CLASS mix   [IAC_NPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(320): error #6404: This name does not have a type, and must have an explicit type.   [CPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(321): error #6362: The data types of the argument(s) are invalid.   [TRIM]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(321): error #6303: The assignment operation or the binary expression operation is invalid for the data types of the two operands.   [TRIM]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(322): error #6054: A CHARACTER data type is required in this context.   [CPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(323): error #6054: A CHARACTER data type is required in this context.   [CPFT]

/blues/gpfs/home/shippert/E3SM_gcam_core_submodule/components/clm/src/cpl/clm_cpl_indices.F90(324): error #6054: A CHARACTER data type is required in this context.   [CPFT] =================================

3/10/2020

* Made a new branch from Gautam's latest:

  git checkout -b shippert/gcam/active-gcam-submod

* The submodules and submodules of that submodule are listed as "HEAD   detached at a20adf2", whatever that means.  I don't know if I can push up   to the gcam repo like this - need to ask Gautam what is going on.

* Anyway, turned back on wrapper function in gcam_comp_mod.F90; there were   just some typos to cleanup (gcamoemis, elm2gcam_mapping_file).

* Now, I need to copy back in my glm updates and try to compile with those.

* Then, build with ZLND to compile and fix the lnd model builds.  

* Finally, merge with Xiaoying's atm branches and build to fix THAT.

* Also, update Kate with luc.xml/glumap incompatibilities, mentioned   somewhere below (or in my python script.)

! Need to revisit the iac_cdata%i and %c structures in gcam_comp_mod.F90 -   should be filled from namelist variables in gcam_init_mod().  I hacked a   lot of these out in the initial port, but we need to review what is   needed by gcam2glm_mod.F90, especially.

  Maybe do a find/grep for all cases where we (a) use iac_cdata%{c,l,i},   and then look for all cases where we (b) set iac_cdata%{c,l,i}, and find   some way of cross-referencing.

* Need to initialize things like nlon and lat

3/4/2020

* Gautam has been working on getting the libraries and build scripts   working on Anvil - he found some compile errors that he "fixed", but   mostly by commenting things out and mucking with argument lists, so it   probably won't work.

* Anyway:

===========    git clone git@github.com:E3SM-Project/E3SM.git E3SM_gcam_core_submodule    cd E3SM_gcam_core_submodule/    git checkout bishtgautam/gcam/gcam-core-submodule-2    git submodule update --init --recursive

   cd cime/scripts    ./create_newcase --case ~/iac_gcam_submod --compset Z --res f19_g16

   cd ~/iac_gcam_submod/    ./case.setup    ./case.build 

===========

* (I still don't quite grok git - it looks like the examples below (search   for git clone) are all about cloning a branch, then renaming or something   back to shippert/gcam/active-gcam-coupled.  Whatever)

! Ah, some of the below may be wrong if I haven't cleaned it up - you need   to have the --recursive tag on the submodule update to actually get the   gcam code.  Apparently, gcam-core (probably now just called gcam) is now   a submodule within the iac submodule, so, I guess, you need --recursive   to check it out.  Sheesh!

XXXXXXXXXXXXXX * This gives me Gautam's branch, and builds.  And it works.

* But! Gautam wanted me to test a new Macro.make file, a la this (from his   3/3/20, 11:38 AM email):

  cp /home/gbisht/Macros.make .

  This Macros.make differs from the one in ~/iac_gcam_submod mostly by   including the xerces    XXXXXXXXXXXXXXXX

  I think the raw make worked originally because there was no gcam code!   With the --recursive tag.  So:

1 try the case.setup and build with it as it is.

  We get a build fail - 

  /lcrc/group/acme/shippert/acme_scratch/anvil/iac_gcam_submod/bld/iac.bldlog.200304-115708:

    ...     catastrophic error: cannot open source file "jni.h     ...

2 try it with Gautam's /home/gbisht/Macros.make

  (original to Macros.make.orig)

  Same build fail...can't find "jni.h".

  lcrc/group/acme/shippert/acme_scratch/anvil/iac_gcam_submod/bld/iac.bldlog.200304-121336

* Gautam had an issue with __HAVE_JAVA__ vs. _HAVE_JAVA_ vs. HAVE_JAVA   yesterday - that's probably it, given the "j" in the name.  I was   confused by the discussion, but it appears you need to have one of them   (I don't know why macros aren't working with the others via

  CXXBASEOPTS += -DHAVE_JAVA=0

  in ...build/linux/configure.gcam).

  Anyway, .../gcam/src/iac/gcam/cvs/objects/util/base/include/definitions.h

  is where it's defined, so for this test define it as HAVE_JAVA 0

* Okay, the new Macros.make works - successful build.  The original, with   HAVE_JAVA 0, ...

2/17/2020

* It appears the lon/lat grid goes [0,360] in longitude, and [-90,90] in   latitude, starting at the lower left corner ([0,-90]).

  I'm trying to mock up some test data to check my glumap read code, and I   need to get a lat/lon array so I can use findloc to give the right   index.  The problem is, I don't know if we go [0,359] or [1,360] -   probably the former.  But [-90,90] or [-90,89]?

  The landuse code solves this problem by cutting to the middle of the   grid, so it actually got from [0.5,359.5] and [-89.5,89.5].

! But, regionmap_1x1.csv seems to go from [0,359] and [0,179]!  So, that's   a mess - we may need to shift to match whatever lat/lon we read from the   input file in gcam2glm_mod.F90.

2/16/2020

* Unfortunately, I have to do more refactoring of gcam2glm_mod.F90 - it   looks like the aezmap it used before had a one-to-one correspondence   between (lat,lon) and (reg,aez) - thus, the weights were all 1, and they   could use rAEZ_sites and/or raezs as the weights.

  Instead, I have to build something like this:

  glu_weights(g,lonx,latx)

  ...so that glu_weights(g,:,:) functions as the weights for calculating   cell areas for this particular g.

* I've been working this through, and I *think* the refactor isn't that   bad:

1 change double loop r,aez into single loop over g 2 replace rAEZ_sites/raezs with glu_weights(g,:,:) 3 Rework the reassignment section to deal with regions.

4 Write ingest functions for glumap_1x1.csv that builds the arrays I need   for this - glu_weights(:,:,:) in particular, also some kind of g->r   mapping.   

  Obviously, number 3 is the hard one.  Number 2 actually involves a lot of   deleting, because I no longer need to do this:

       where ( aez_regions .EQ. r .and. aez_zones .EQ. aez)

  stuff - if I do (4) right, then glu_weights will automatically function   as both a zeroing mask for lat,lon outside of this g, and as the weights   for lat,lon inside this g.

* Of course (4) is quite hard - I have actual lats and lons in   glumap_1x1.csv, so I need to convert those to indeces.  There's probably   a smart way to do that, but right now I don't know what it is - start at   -180,-90 and count up?

* Remember, we are looping over all g's and analyzing each one separately.   That's similar to the double loop over region and aez's within that   region, except now each individual g implies a region, and some regions   have just one g and some have lots.




12/17/2019

* Well, duh - Alan suggested just using the regions in the regionmap.csv   file to do the reassign.  That clearly makes sense.

  The indexing will still need to be refactored - the nested loop will   because a single glu loop, but we'll have to loop over regions for the   reassign block, and then extract the glus that are part of that region.   Each region will have a variable number of sub-glus, so we have to do   something like:

    do r=1,nreg        do g=1,nglu(r)

  Or maybe do a logical subsetting (where (regions .eq. r)) to count or   loop or mask the glus we are working with.

* Kate says the gcam index is in the order of lux.xml.  So I've extracted   the region column (i.e. glus) from that xml file into glus.ordered.txt.   From there, we extract the regions (up to the final underscore), and uniq   and count them:

    perl -pe 's/_\S+$//g' glus.ordered.txt | uniq | wc

  This returns 26 regions, and 392 total glus.

* But, I *think* I'm seeing region/glu pairs in region map that are not   listed in luc.xml - test that now.

   perl -ne 'if (/^\d+.*\"(.*)\".\"(.*)\"\,\d+/) { print "$1_$2\n"}' regionmap.csv

  ...seems to extract the region-qualified glu names in the same format.   This leads to 427 unique region qualified names, so there are some   missing from luc.xml.

* Okay, that gives 52 region/glus in regionmap.csv that do not correspond with   anything in luc.xml.

  Is this a problem?  I need to think about this - if gcam does not output   to these glus, then what does that do to grid cells that have weights in   gcam output space that do not sum to 1.0?

* More importantly, here are the glus from luc.xml that do NOT show up   anywhere in regionmap.csv:

  Africa_Northern_DeadSea   Argentina_ChileCstN   Brazil_OrinocoR   China_AmuDaryaR   China_SyrDaryaR   EU-12_DniesterR   EU-12_DnkGrmCst   EU-15_MeditE   Europe_Non_EU_EJrdnSyr   Pakistan_Tamir   Russia_BlackSeaS   Russia_CaspianSW   South Africa_AfrCstSW   South America_Northern_MagdalenaR   South America_Southern_Salinas   Southeast Asia_IdnE   USA_FraserR

  I can kind of understand regionmap.csv entries not being in luc.xml, if   those glus do not concern themselves with gcam landuse.

  I'm having a hard time figuring out what a region in luc.xml that is not   being mapped to any grid cell might mean.  And, in particular, I'm not   sure how to use these glus inside of gcam2glm - just ignore them when   looping over our 392 glus?

  Maybe they are glus that have very small overlaps with meaningful grid   cells, and thus have been trimmed out of the mapping somehow?

12/13/2019

* Trying to review what gcam2glm does.  

  Here's what the sorting does: for a given region/aez (let's call that an   raez) set of indices, we sort by the potential vegetation from the   "biomass" field of an input config file - so it's a property of our grid   point.   We use that to find a sorted list of indices (lat,lons), based on   potential vegetation, that our raez contributes to.

  We then take those sorted indices (v1/2d, v1/2u, depending on   whether they are sorted highest to lowest or lowest to highest in   pot_veg), and, if our crop_neg_f>0 (i.e. some crop land is recovered by   forest?  At least, crop land is shrinking), we take the cumulative sum of   glm_crop(old)*cellarea_forest, until that value hits crop_neg_f.

  Technically, we cumsum, then find the location of the minimum index of   the raez where the total exceeds crop_neg_f - that's the point where,   adding from cells with most pot_veg to least, we hit the crop_neg_f   threshold. 

  Then, for all the grid cells at or above that point, we subtract from our   current glm_crop(:,:,np1) value the glm_crop(old)*fnfforest.

  Then, we finally make another adjustment right at the threshold point to   make sure everything adds up (I think).

  Well, that doesn't seem right, because fnfforest looks to be another mask   (value 1 on points where pot_veg > 1), this line (c.697):

      where(sortsitesdn>0)          glm_crop(:,:,np1)= glm_crop(:,:,np1) - glm_crop(:,:,n) * fnfforest       end where

  seems to be saying: subtract the old crop value from the new crop value   everywhere that we have any potential vegetation (at least, whatever   pot_veg > 1 means) and where we need to remove crops to get to our   crop_neg_f. 

  I dunno - we are removing *all* the crop values in these grid cells?  I   mean, we must be scaling by raez weights somewhere that I missed, but,   still, we count down from highest pot_veg values and just subtract crops   in those raezs until we get there?

  Whatever, let's assume that part works - we sort so we can use the   regions with the most potential vegetation to recover crop land.

* Now, let's look at the sort inside the rassign block, the one where we   actually deal with aezs inside of regions, rather than raezs as atomic   units. 

  ...whew, that's really complicated.  We appear to be selecting which   aezs inside each reagion we are going to decrease crops and pasture in.   We sort by "reassign_ag", which is the minimum of avail_ag_farea,   avail_nfarea, and regional_farea_needed.  I want to say: available   agricultural forest area, available non-forest area, and forest area   needed for this region.  I guess the minimum of that is how much forest   area we need to move to forest - either what we got or what we need.

  And then we run only over those raezs that we need to hit our limit, to   do the same kind of calculation as before - rank by pot_veg and take the   ones at the top you need to fill a limit.

* Okay, so let's just cut out that interior raez calculation.  We'll just   run over nglus, turning our two loops:

  do r=1,nreg     ...     do zz=1,nreduced_aezs

  ...into just a simple nglu loop, and don't worry about restricting our   aezs since we don't have any more of them.

* Some stats on the GLUs: no grid cell has more than four glus, and   no glus as a weight of less than 0.25 for any grid cell.  I'm wondering   if some of this sort and cutoff stuff is extraneous - if it's designed to   deal with raezs that have a very minor weight in a grid cell having too   much influence over it.

  Perl/cli calls to do this:

    perl -ne 'if (/^(\d+,\d+)/) { $count{$1}++ ; }; END {foreach $k (keys(%count)) { print($k, "  ", $count{$k},"\n")}}' regionmap.csv

  ...gives the nglus for each lon,lat.  Flip $k and $count{$k} and you can   sort to find the max and min counts.

    perl -ne '@foo=split(/,/); print($foo[4],)' regionmap.csv | sort -nr

  ...gives the weights, ordered from top to bottom, to determine min and   max. 

12/11/2019

* Digging through Kate's original gcam-core to try and find some mapping   and initialization files - most of the gcam stuff is xml and/or .csv   files, rather than netCDF.  Hmm.

  (She says she had trouble with the c++ netcdf libaries.  Whatev.)

* This stuff is superceded by the comment following:

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

* Anyway, in gcam-core/cpl/mappings/regionmap.csv, there seems to be a   mapping between coordinates (given as "x" an1 "y" - "lon" and "lat"   indeces?), followed by "GCAM_ID", "GCAM_AEZ", and "Weight".

  My supposition is that GCAM_ID is the region id; for each region we have   multiple AEZs, so GCAM_AEZ lists that index, and finally the weight is   the fraction of that grid cell that holds that AEZ.  Note that gcam   regions do overlap grid cells

  	  $ egrep 200,148 regionmap.csv 	  200,148,1,7,0.497278816 	  200,148,1,8,0.211327696 	  200,148,1,14,0.285951158 	  200,148,2,7,0.000551974 	  200,148,2,8,0.00488998

  This finds all the weights for gridcell x=200, y=148.  The weights do sum   to 1, but note that we repeat AEZ's 7 and 8, because apparently a small   part of this grid cell is in gcam region 2, while most in region 1.

  So - gridcell (200,148) maps to regions 1 and 2, and contains AEZ 7,8,   and 14.  (I'm assuming aez and glu are interchangable here, at least as   far as mapping is concerned.)

  This file seems to be ordered by GCAM_ID, so looking at the end we see 14   regions.  This doesn't jive with NUM_GCAM_LAND_REGIONS = 384 in main.cpp,   since 14 is not a factdor of 384.  But if there are 16 AEZs, then 384/16   = 24.  Could there be 24 regions, and ~10 of them basically have no   landuse?  (Ocean, antartica?).  Hmm.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

* Okay, so what has happened with gcam5 is that the GCAM vector   representation has moved from nreg/naez to a single vector nglu - a   combined regional index, that encapsulates both location and land use   type.  (Similar to, say, "patches" in lnd - a big honking index that you   can decompose to find which grid cell and pft it represents).

  Which means that, unfortunately, I have to refactor gcam2glm_mod.F90 to   use this generalized GCAM vector index, and not do nested looping over   nreg/naez.

  It would be great if it were simply a matter of changing a nested loop to   a single loop, but unfortunately there are a number of variables who are   dimensioned by just [aez], and one dimensioned by [reg] in gcam2glm_mod -   c. line 431.

* So what this all means is that I can't black box this glm stuff - I have   to figure out what gcam2glm is doing so I can properly modify it to work   with the new gcam vector index.

* Hmm - the region.csv file for gcam5 is just a mapping from x,y to "region   name" and "GLU_name".  Using

    tail +2 regionmap.csv |  awk -F, '{print $3}'  | sort | uniq| wc

  we see that there are 33 regions and 235 glus listed.  I don't know how   this maps to the 384 values we get in main.cpp.

  If we look at luc.xml, we get 391 region names, that seem to be some kind   of combination of the "region name" and "GLU_name" in regionmap.csv: 

     Africa_Northern_MeditS

     vs. "Africa_Northern","MeditS"

  So if we do something like {print $3,4} in my awk script, I get 427 combo   region_glu values.

  Some of these are unclassified regions, though, so that takes us down to   409.  Man, I'm just not quite getting it...

* Some other notes from the call: 

  Kate will generate a 2015 baseline gcam file, that I can use to delta   off.

  We are probably going to run gcam every five years, and then interpolate   onto one year values for glm.  So, I need to work out how to do that -   call iac yearly, store the coupled inputs internally, run gcam for year+5   if necessary, then run gcam2glm to interpolate onto our target year using   the bracketing five years, and couple back to lnd.

  So, we'll need an alarm or some other mechanism for knowing when we need   another bracketing gcam run, and we need to store up our current lnd   inputs for use with the five year gcam run.

  Co2 coupling should be straightforward - just interpolate the flux values   from [year0,year5] to our target year.

* I mean, with interpolation we could update things at a faster rate,   right?  Monthly or daily?  If we ran iac monthly we could reconstruct our   "mean of monthly maxes" calculation (or was that the max of monthly means?)    12/10/2019

* Here is the bones of an email I need to edit down to send to Kate and   Alan, about how gcam2glm works.

---   The gcam output is on this regionalized grid, with zones further broken   up into agro-ecological zones for GCAM3.  The online docs say that for   GCAM5 we've replaced AEZs with a water-basin defined GLUs.  We have this   piece of code in iESM in gcam2glm_mod.F90 that is designed to take the   gcam nreg*naez output values onto a spatial [lon,lat] grid.  I'm assuming   that for this purpose the GLU subgrids are functionally the same as the   old AEZ subgrids  - we have our GCAM5 nreg*nglu output values, and want   to map them to [lon,lat]; presumably the mapping files Alan generated   facilitates this in the same way as before, and the gcam2glm regridding   code will work for that purpose without too much refactoring. 

  But gcam2glm does something else, to - it interpolates the gcam outputs   in time, as well.  So it currently expects *two* different set of gcamo   values, one for the start year and one for the end year, across which it   will interpolate.  My understanding of how this worked in iESM goes   something like this (my example will start in 2010 and assumes we are   past dealing with initialization issues - we are a couple time steps in): 

1 For model year 2010 (on January 1st), we decide we need to run gcam   again.  We use the 20100101 lnd model inputs to generate gcam outputs for   five years in the future, 2015.  2 We then send both the 2010 and 2015 GCAM outputs to gcam2glm, which   interpolates onto 20100101 and regrids them to [lat,lon]  3 We then run glm using the regridded [lat,lon] output, and send that back   through the coupler to the lnd model  4 We step forward to 2011 - we do *not* run gcam5 again, but instead send   the 2010 and 2015 GCAM runs to gcam2glm, which interpolates onto 20110101   and regrids, and use that to run glm.  5 Repeat with 2012, 2013, 2014 6 For 2015, we run gcam for 2020, and repeat this loop.

  This assumes we actually run gcam every 5 years, but we want to update   land use every one year. 

  Is that still the plan?  I have it in my head that we were going to run   gcam5 every year, which means we don't need this kind of yearly   interpolation.  And I get a little confused thinking about whether we are   running gcam in the future or not, and what that really means. 

  Anyway, looking through the (rather extensive) gcam2glm code, it seems   like the *change* in crop, pasture, and forest area from gcam run to gcam   run is the only way the gcam outputs are used.  There's some complicated   (and not particularly well commented) calculations involving reclaiming   shrinking pasture and crop lands, and it's all intertwined in with the   regridding code.  So to use gcam2glm as is, we really need to give it   gcam values for year1 and year2 and let it do it's thing. 

  So, would something like this work for E3SM?

1 On 20100101, we run gcam using 2010 lnd inputs 2 We send gcam 2009 and 2010 outputs to gcam2glm, and "interpolate" onto   20100101 and regrid  3 run glm and send back landuse output to coupler

---

* Too much detail for an email - I need to hit the high points first.

* Also, I need to ask about initalizing gcamo for our initial run   (i.e. 2005, I Think), so we have a year1 to apply to our year 2.

12/6/2019

* Here's a newish clone call:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_active_gcam_coupled   cd E3SM_active_gcam_coupled   git checkout bishtgautam/gcam/add-submodule-2   git submodule update --init   git checkout -b shippert/gcam/active-gcam-coupled   git pull origin shippert/gcam/active-gcam-coupled   cd components/gcam/src   git pull origin master

* Apparently, I need to checkout the add-submodule-2 branch before I can   get the submodules, and then I may need to do the pulls, especially if   I've just pushed things up.  I'm not sure I understand all this, but it   seems to have worked to get my latest updates to the local machine on   ~/anvil2 in prep for blues to go down this afternoon.

* The idea here is to do something like this, once blues is down:

    mv anvil anvil.orig #(should be empty, anyway)     mv anvil2 anvil

  ...and then my emacs buffers that grab ~/anvil should still work.  I can   modify locally, and then when blues is back up next week, I can push back   up, mv anvil back to anvil2, mv anvil.orig to anvil, resetup the sshfs   mount, and then I'm back to normal.

12/6/2019

* Checklist time for iac_run_mct() in iac_comp_mct.F90:

@XsetdensitycGCAM() updated to new GCAM_E3SM_interface_wrapper.cpp function

@Xupdate runcGCAM() call in gcam_run_mod()

@ update gcam_init_mod() for allocations for gcam2glm_data, gcam_emis_data   (outputs of runcGCAM()), to match what GCAM and GLM expect.

@ gcam2glm_run_mod() review and revision @ glm_run_mod() review and revision

@ iac_export() of gcam_emis_data and glm2lnd_data

* For gcam_run_mod(), we apparently do not need gcami anymore - no input,   it's all taken care of by setdensity. 

* Okay, this is what I was afraid of - glm uses the cdata structure, which   we've denuded mostly, to get some of the stuff it needs - in particular,   gcam2glm_run_mod() wants:

    year1=cdata%i(iac_cdatai_gcam_yr1)     year2=cdata%i(iac_cdatai_gcam_yr2)          naez=cdata%i(iac_cdatai_gcam_naez)     nreg=cdata%i(iac_cdatai_gcam_nreg)     ntime=cdata%i(iac_cdatai_gcamo_ntime)

  So, it looks like naez, nreg, and ntime(?), should be namelist   variables.  They might even match up with num_gcam_energy_regions and   num_gcam_land_regions, but I'm not sure.  So I'll have to review   how cdata%i(iac_cdatai_gcam_naez) gets set, again.  Where is that?

* Okay, here is the relevant section of iac_fields_mod.F90 (remember,   that's defunct), which has a lot of this stuff:

-------------------- iac_fields_mod.F90     ! !PUBLIC DATA MEMBERS:     !--- this should be namelist or something like it ---

    real*8,  parameter, public :: iac_spval = -999.0     integer, parameter, public :: iac_gcam_nreg =  14     integer, parameter, public :: iac_gcam_nsector =  12     integer, parameter, public :: iac_gcamoemis_nemis=  1     integer, parameter, public :: iac_gcam_naez =  18     integer, parameter, public :: iac_gcam_ncrops =  27     integer, parameter, public :: iac_gcam_timestep =  5     integer, parameter, public :: iac_gcam_ioyears =  15     integer, parameter, public :: iac_gcamo_ntime =  2     integer, parameter, public :: iac_glm_nx  = 720     integer, parameter, public :: iac_glm_ny  = 360     integer, parameter, public :: iac_iaco_npfts =  17     integer, parameter, public :: iac_iac_npfts  = 16     character(len=*), parameter, public :: iac_gcam2glm_map = 'some_map_file' -----------------

* Here is how these values are set into cdata inside of the old gcam_comp_mod.f90

================= old gcam_comp_mod.f90

    cdata%l(iac_cdatal_gcam_present) = .true.     cdata%l(iac_cdatal_gcam_prognostic) = .true.     cdata%i(iac_cdatai_gcam_nreg) = iac_gcam_nreg     cdata%i(iac_cdatai_gcam_naez) = iac_gcam_naez     cdata%i(iac_cdatai_gcam_timestep) = iac_gcam_timestep     cdata%i(iac_cdatai_gcamo_ntime) = iac_gcamo_ntime     cdata%i(iac_cdatai_gcamo_nflds) = iac_gcamo_nflds     cdata%i(iac_cdatai_gcamo_size) = iac_gcam_nreg*iac_gcam_naez * iac_gcamo_ntime     cdata%i(iac_cdatai_gcamoemis_size) = iac_gcam_nreg*iac_gcam_nsector     cdata%i(iac_cdatai_gcami_nflds) = iac_gcami_nflds     cdata%i(iac_cdatai_gcami_size) = iac_gcam_nreg*iac_gcam_naez*iac_gcam_ncrops        allocate(gcami(iac_gcami_nflds,cdata%i(iac_cdatai_gcami_size)))     allocate(gcamo(iac_gcamo_nflds,cdata%i(iac_cdatai_gcamo_size)))     allocate(gcamoemis(iac_gcamoemis_nemis,cdata%i(iac_cdatai_gcamoemis_size)))

==========================

12/5/2019

* Okay, after a couple days of wheel-spinning, I think I need to rework the   variables in iac_comp_mct.F90.

  The input to gcam is handled by gcam_set_density_mod(), which should take   the lnd2iac_vars structure on input, because the call to the interface   wrapper uses multiple 1D arrays (for npp, hp, pftfrct, area, landfrac) as   inputs.  So we might as well just pass in the structure, which has all   those things.

  Note that a gcami(nflds,*) array wouldn't work anyway, as npp,hp,pft are   all 3D (lon,lat,pft) and area and landfrac are 2D (lon,lat), so we'd need   either two different arrays or a structure.  So use the structure.

* That sets some values in the internal gcam object, so the runcGCAM()   function inside of gcam_run_mod() doesn't actually need inputs, just a   place to store outputs.  

* Now, depending on how the interface to glm works, we may need to take   those outputs and refactor them, but for now let's assume Kate was savvy   enough to provide output in the format I need them.

* Okay - I've moved the old gcam-core gcam_comp_mod.F90 function into the   regular gcam coupling area and out of gcam-core - so gcam-core will have   c++ functions for the interface, but no fortran.

   .../components/gcam/src/iac/coupling/

  The interface wrapper, that shows how the interface functions should be   called from gcam_comp_mod.F90, is in ~/gcam-core/cpl/source/ on anvil.   The inteface has CHANGED from the CCSM_E3SM interface, as well as from   the old original gcam-core files that Kate sent me originally - (those are   in ~/old/gcam-core).

1 So, up next is modifying gcam_comp_mod.F90 to have the right arguments to   the interface function, and to make sure those are passed in via   gcam_run_mod() and gcam_setdensity_mod(), and/or via the iac_data_mod.F90   module structures.

2 We need gcam_init_mod() to allocate the output arrays from   runcGCAM()/gcam_run_mod() correctly.  To see how this should be done,   look at ~/gcam-core/cpl/main/main.cpp, which I've printed out, which sets   up a run with the right dimensions and everything.

3 Then, we need to interface with glm, and make sure the outputs of   gcam_run_mod() are available (inside of gcam_run_mct()) to call with   gcam2glm_run_mod() - actually, maybe that function converts from gcamo   mode to glmi mode or something.  Then glm_run_mod().

4 Finally, review how the output of glm_run_mod(), whatever format it is   in, gets setnd back to the coupler.  I think I need to call   iac_export(z2x_z, ...), or something.  Dammit, I thought I was closer   than this...

! Okay, namelist variables - time to figure out how to set and read those,   too.  in gcam_init_mod() we'll need to read the namelist and store those   values - setdensitycGCAM() needs a mapping_file and a set of    read_scalars/write_scalars logicals as namelist variables.

12/2/2019

* Trying to grab Kate's updated gcam-core, via:

  git clone https://github.com/JGCRI/gcam-core

  I tried to do that inside of ~/gcam-dev, and that caused all kinds of   problems, putting stuff in ~/gcam-core/gcam-dev/gcam-core and then   getting messed up.  I tried it again from my home directory, and that   ?seems? to have worked - at least, it didn't fail.

* Anyway, Kate's original gcam-core is in ~/old.

* Um, okay, what I got now seems to be the original, original gcam branch,   with stuff in cvs and exe and input and output.  WTH?

  (See after XXs)

XXXXXXXXXXX * I tried using the branch Kate actually sent:

  git clone https://github.com/JGCRI/gcam-core/tree/e3sm-integration

  ...but that fails.

  fatal: repository 'https://github.com/JGCRI/gcam-core/tree/e3sm-integration/' not found

  I think that means I probably don't have access to this...

  Anyway, the above XXd section gives me something that has no fortran, so   that can't be it.

! So, I'm back to working through the original updated ~/gcam-core that   Kate sent me via tarball, with the proviso that once I get the new   gcam-core I may have to change things around. XXXXXXXXXXX

* Okay, I'm such a git noob - obviously I need to checkout the   e3sm-integration branch.

  ...well, it doesn't seem to have any fortran in it either!  Okay, so   either I do or don't have access to everything I need, and/or I do or do   not need to write my own gcam_comp_mod.f90 function to add to gcam-core.   Clear as mud!

* Well, at any rate, the new gcam-core, e3sm-integration branch, seems to   have everything in cpl/source EXCEPT the gcam_comp_mod.f90, so we got   that going for us.  This actually simplifies some stuff - I can make sure   the fortran works, using the old gcam-core as a template, and using the   new gcam-core reordering of lon,lat,p as the way to go.

* So, off to review the next bullet - iac_comp_mct.F90, gcam2glm_mod.F90,   and glm_comp_mod.F90, and also the old gcam_comp_mod.f90.

11/16/2019 18:07:57 PST

* WHERE TO START:

  Review gcam2glm_mod.F90 and glm_comp_mod.F90 and how they are called from   iac_comp_mct.F90. 

  In particular, review iac_comp_mct.F90 variables gcam2glm_data,   glmi_data, glm2lnd_data, glmo_data, etc.  Which do we need, what should   they be called, and are they all being initialized correctly in the init   functions?  (We have to assume the run functions are using them   correctly).

11/16/2019

* Okay, my plan is to simply move all the cdata/gdata stuff and everything   over to iac_data_mod.F90.  That way we can whittle it down without trying   to figure out if we need to re-get rid of iac_fields_mod.F90.  So we are   back to deleting iac_fields_mod.F90

* So, for the gcam/glm wrapper functions I've removed the EClock and cdata   arguments, instead relying on this module to make them global within each   pertinent file:

    use iac_data_mod, only : cdata => gdata, EClock => GClock

  I'm pretty sure that's okay - the only issue is if the dynamic parts of   gdata and GCLock somehow cause a problem if included in a module rather   than passed as an argument.  This also renames gdata and GClock to what   they were in the original functions, which lets me call them the G names   outside of that to avoid collisions with the E3SM names.

11/15/2019 09:45:20 PST

* So, the mct functions are part of the gcam dist, so it no longer makes   sense to worry about moving stuff into and out of iac_comp_mct.F90 to   keep the submodule separate - it's all submodule stuff.  So go ahead and   use gcam functions willy-nilly inside of mct.

* Annoyingly, I think we need iac_fields_mod.F90 after all - it defines the   gcam cdata, and I think some of the indeces and numbers are used in other   parts of the gcam code (maybe), and in the glm code (probably).  So I   have to add that back in...

? I do not know how to allocate or initialize gcam cdata (which I'm calling   gdata, to distinguish it from the E3SM cdata_z, in the same way I have   GClock and EClock).  It looks like various parts of cdata are set in   various functions, so, great.  Time to grep...

  ...great.  Lots of the cdata stuff is set and allocated in the unused   iac_comp_mod.F90 (from iESM, I think), assigning from namelist stuff.  I   also see a lot of allocations and assignments in the original   gcam_comp_mod.f90, from GCAM 3.  Oh, and some from glm_comp_mod.F90   wrapper, too.

  Revise my grep to just find assignments:

    cd .../components/gcam/src     find . -type f -exec egrep -inH 'cdata%.*=' {} \;

  Returns:

    gcam_comp_mod.F90 (old, in cvs/objects/ccsmcpl/source)     glm_comp_mod.F90     iac_comp_mod.F90 (in unused)     gcam2glm_mod.F90      ...plus some duplicate and renamed versions, things like the 5year   versions.   Okay, so all that needs to be set in either the new   gcam_comp_mod or glm_comp_mod.F90 (except the two lines in   gcam2glm_mod.F90, which seem to be a way to pass yearly information back   and forth), and governed by namelist variables.

? Another thing I'm beginning to worry about - there are a lot of text   .conf files in the glm stuff, pointing to specific files to read etc.  I   may have to refactor all of that to work with namelists, but hopefully   this is a residual from standalone glm and the iESM wrapper already set   up the namelists.  So, I should probably review the iESM nameslist files   again - I've done that before, right?

11/15/2019

* Trying to push up to gcam, which I forgot to do yesterday:

	modified:   cpl/gcam_cpl_indices.F90 	modified:   cpl/iac_comp_mct.F90 	modified:   cpl/iac_import_export.F90 	modified:   iac/coupling/gcam_var_mod.F90 	deleted:    iac/coupling/iac2gcam_mod.F90 	modified:   iac/coupling/iac_data_mod.F90 	deleted:    iac/coupling/iac_fields_mod.F90 	modified:   iac/coupling/iac_init_mod.F90

  Untracked files:   (use "git add <file>..." to include in what will be committed)

	iac/coupling/iac_io_mod.F90 	iac/coupling/orig/ 	iac/coupling/rename/ 	iac/coupling/unused/iac2gcam_mod.F90 	iac/coupling/unused/iac_comp_mod.F90 	iac/coupling/unused/iac_data_mod.F90.first_draft 	iac/coupling/unused/iac_fields_mod.F90 	iac/coupling/updateannuallanduse_v2.c 	iac/gcam/cvs/objects/sectors/source/hide/ 	iac/gcam/cvs/objects/technologies/source/hide/ 	iac/glm/README 	iac/glm/unused/

* So:

----------------   git add cpl/gcam_cpl_indices.F90 \       cpl/iac_comp_mct.F90 \       cpl/iac_import_export.F90 \       iac/coupling/gcam_var_mod.F90 \       iac/coupling/iac_data_mod.F90 \       iac/coupling/iac_init_mod.F90 \       iac/coupling/iac_io_mod.F90 \       iac/coupling/updateannuallanduse_v2.c      

XXX git rm iac/coupling/iac2gcam_mod.F90 iac/coupling/iac_fields_mod.F90

    (See above for 11/15/2019, I think we need iac_ields_mod.F90)

----------------

11/14/2019

* iac2gcam_mod.F90 and iac_fields_mod.F90 are, I believe, obsolete, which   means I need to move the internal iac_cdata_type declaration to some   other module (probably iac_data_mod.F90 or maybe iac_init_mod.F90).

* It seems like I should try to keep the gcam_comp functions out of   iac_comp_mct(), or at least call them iac_foo rather than gcam_foo, so we   could, in the future, sub in a non-GCAM iac module.  But for now I'm not   going to worry about it.

* Which means maybe calling gcam_init_mod() from iac_init_mct() is okay.   The other option is in iac_init(), from iac_init_mod().

  gcam_init_mod(EClock, cdata, gcami, gcamo, gcamoemis) allocates the gcam   variables, but EClock and cdata need to already be set.  I'm not sure   where any of that happens - it must be inside iac_init().  But since we   are calling these gcam_comp functions using Gclock and gdata inside   iac_comp_mct.F90, that means we either have to declare them as public in   some module, or declare them internally to iac_comp_mct.F90 and pass them   as arguments to iac_init() to get them allocated and set before   gcam_init_mod() needs them.

* Okay, I need to rework iac_comp_mct() to get the calling sequence right,   but I want to get some commits pushed out first, so Kate and Gautam are   working with mostly the same code.

  Here's my Git status:

============== On branch shippert/gcam/active-gcam-coupled Changes not staged for commit:   (use "git add <file>..." to update what will be committed)   (use "git checkout -- <file>..." to discard changes in working directory)   (commit or discard the untracked or modified content in submodules)

	modified:   cime/src/drivers/mct/main/cime_comp_mod.F90 	modified:   cime/src/drivers/mct/main/prep_atm_mod.F90 	modified:   cime/src/drivers/mct/main/prep_iac_mod.F90 	modified:   cime/src/drivers/mct/main/prep_lnd_mod.F90 	modified:   cime/src/drivers/mct/main/seq_frac_mct.F90 	modified:   cime/src/drivers/mct/shr/seq_flds_mod.F90 	modified:   components/cam/src/control/camsrfexch.F90 	modified:   components/cam/src/cpl/atm_import_export.F90 	modified:   components/cam/src/cpl/cam_cpl_indices.F90 	modified:   components/clm/src/cpl/clm_cpl_indices.F90 	modified:   components/clm/src/cpl/lnd_comp_mct.F90 	modified:   components/clm/src/cpl/lnd_import_export.F90 	modified:   components/clm/src/main/clm_driver.F90 	modified:   components/clm/src/main/clm_initializeMod.F90 	modified:   components/clm/src/main/clm_instMod.F90 	modified:   components/gcam/doc/NOTES.e3sm 	modified:   components/gcam/src (modified content, untracked content)

Untracked files:   (use "git add <file>..." to include in what will be committed)

	cime/config/e3sm/machines/config_machines.xml.dist 	cime/scripts/Tools/mkDepends.orig 	cime/scripts/lib/CIME/XML/env_mach_specific.pyc.dist 	cime/src/drivers/mct/main/cime_comp_mod.F90.ps 	cime/src/drivers/mct/main/cime_driver.F90.ps 	components/clm/src/cpl/lnd_comp_mct.F90.ps 	components/clm/src/iac 	components/clm/src/main/#glc2lndMod.F90# 	components/clm/src/main/.#glc2lndMod.F90 	components/clm/src/main/iac2lndMod.F90 	components/clm/src/main/lnd2iacMod.F90 	components/clm/src/shr_kind_r8 	components/clm/src/shr_log_errMsg 	components/gcam/doc/.#NOTES.e3sm 	components/mosart/src/cpl/rof_comp_mct.F90.ps 	components/mosart/src/riverroute/#RtmMod.F90# 	components/mosart/src/riverroute/.#RtmMod.F90 ==============

* So I see three commits:

1 git add cime/src/drivers/mct/main/cime_comp_mod.F90 \       cime/src/drivers/mct/main/prep_atm_mod.F90 \       cime/src/drivers/mct/main/prep_iac_mod.F90 \       cime/src/drivers/mct/main/prep_lnd_mod.F90 \       cime/src/drivers/mct/main/seq_frac_mct.F90 \       cime/src/drivers/mct/shr/seq_flds_mod.F90 

   Commit message:

--------------- CIME modifications to couple with iac/gcam

Changes to prep functions, cime_comp functions, and seq_flds to couple iac with lnd and atm ---------------

2 git add components/clm/src/cpl/clm_cpl_indices.F90 \       components/clm/src/cpl/lnd_comp_mct.F90 \       components/clm/src/cpl/lnd_import_export.F90 \       components/clm/src/main/clm_driver.F90 \       components/clm/src/main/clm_initializeMod.F90 \       components/clm/src/main/clm_instMod.F90 \       components/clm/src/main/iac2lndMod.F90 \       components/clm/src/main/lnd2iacMod.F90

============ CLM modifications to couple with iac/gcam

Added modules to do iac2lnd and lnd2iac coupling, added new iac-based coupler fields in the cpl stuff, and modified arguments in some main clm functions to include the iac coupling vars ============

3 git add components/cam/src/control/camsrfexch.F90 \       components/cam/src/cpl/atm_import_export.F90 \       components/cam/src/cpl/cam_cpl_indices.F90

------------ CAM modification to couple with iac/gcam

Modifications in cpl functions to grab and use co2flux from iac via coupler, and in rfexch structures to hold them. ------------

11/13/2019

* Last few things: get the iac calling sequence right:

1 call gcam_init_mod() somewhere that makes sense 2 declare and allocate the gcam internal structures correctly 3 declare and allocate the coupling structures used by iac_comp_mct.F90   correctly  4 clean up obsolete files and funcctions (i.e. iac2gcam_mod.F90) 5 update init_mod() read the new grid file format

* New compset from Xiaoying:

    I just pushed my branch with the fully coupled compsets with GCAM     turning on to git. My branch is acme-y9s/gcam_compset, it is ready to     be tested. I can use the new compset to create a new case but fail to     build as we expected.

* Okay, so the next step is how to merge these branches so I can use the   compset - I probably have to get my branch full committed (and pushed?),   and then it should just be 'git merge acme-y9s/gcam_compset'.  There   shouldn't be very many conflicts, as they hopefully just modified some    config files.

* Because I do not trust git to do things that I understand, mostly because   of my dumbness, I'll probably should do a git pull of master, or maybe if   I've got an old git master somewhere use that, and then git branch   acme-y9s/gcam_compset, and then do a diff or something to track down the   compset files.

  Doing that, there's GCAM labels in some compsets in   ./cime/config/e3sm/allactive/config_compsets.xml in the   acme-y9s/gcam_compset branch.  Um, is there anything else in defining a   compset?  What did I do to define ZLND and Z?  I'm grepping for:

  BGCEXP_BCRC_CNPRDCTC_20TR_GCAM

  ...which was the alias I found, to see if I can find that somewhere   (nope). I'll have to review my notes way below on how compsets are   defined - is it just creating a string, and that's it?  There has to be   something somewhere, right?

* The two aliases in her branch:

    BGCEXP_BCRC_CNPRDCTC_20TR_GCAM         - 20TR_CAM5%CMIP6_CLM45%CNPRDCTCBC_MPASSI%BGC_MPASO%OIECOOIDMS_MOSART_SGLC_SWAV_BGC%BCRC_GCAM

    G_BG1850CN         - 1850_CAM5_CLM45%CN_MPASSI_MPASO_MOSART_MALI%SIA_SWAV_GCAM

* I don't see anything other than the case string anywhere - not in the   config_component.xml or config_compsets.xml in gcam, or in the cime   allactive area.

  .../cime/config/e3sm/allactive/config_compsets.xml

11/12/2019

* Okay, at this point I I have to admit what the hell is happening in the   merging is beyond me.  All throughout these prep files for every   component, you see constructs like this:

  prep_glc_mrg():

    do egi = 1,num_inst_glc        ! Use fortran mod to address ensembles in merge        eli = mod((egi-1),num_inst_lnd) + 1        efi = mod((egi-1),num_inst_frc) + 1        x2g_gx => component_get_x2c_cx(glc(egi))        call prep_glc_merge(l2x_gx(eli), fractions_gx(efi), x2g_gx)     enddo

  So, you are grabbing the avects from glc(egi), the egi-th decomp of glc, and   merging them with l2x_gx(eli), the eli-th decomp of lnd.

  I don't know what that means.

  Don't even get me started on "fractions", here.

* The thing I'm getting messed up on is this: if glc and lnd have same   number of processors, then it's easy.  If glc has more, then we wrap   around and link up with lnd decomp elements more than once.  Okay, I   don't get that, but maybe it's a convention of some sort.

  But...if glc has fewer procs than lnd, eli never spans all num_inst_lnd   decomp blocks.  So, how are we merging things?

* I halfway think that there is some magic in setting up the mappings that   lets this happen - when you map from one component to the next, then mct   knows that (a) we have 4 glc procs, (b) we have 8 lnd procs, so we link   them up following this mod business so that l2x_gx(eli=1,4) has what we   need for x2g_gx{glc(egi)} to merge together onto the four glc procs.

  So x2g_gx spans the egi decomp, and the first four blocks of l2x_gx lines   up, and we just need to copy them over to x2g_gx and do whatever other   things (like multiplying fluxes by landfrac or whatever) to get them to   the point where x2g_gx is usable.

? But I can't really verify that is true - it seems like we have lots of   leftover parts of l2x_gx - why not just allocate it to span egi in the   first place?  Is it because of the "ensemble" configuration they talk   about, which might mean a specific way to run?

? I also don't know why we use x2g_g in glc and l2x_r in rof.  Something in   the ocn suggested maybe it's because we accum after merging in some and   the other way around in others?

* Anyway, I can't just keep banging my head against the wall, so I'll   follow rof, l2x_z, fortran mod, and hope the mapper magically fixes the   issues above.  It should work for nproc=1 for all components, at least,   and that should be enough of a smoke test to pass the PR.  But, yeesh.

* Okay, I've got all the prep_iac, prep_lnd, and prep_atm functions to hook   in iac with these other components and teh coupler.  I *think*.  

* Final thing, then, before I'm ready to build and test, is to review   iac_comp_mct.F90, follow the sequence there, and make sure the functions   are set up to use the gcam_core interface from Kate.  Then it's just a   matter of modifying buildlib to point to the new gcam tree and trying a   new build. 

11/11/2019

* Hey, it finally clicked - there's a seq_cplflds_inparm namelist - that's   where we have to put iac_npft.  Not only that, I *already* added it to   that namelist!  Why did I think there was something to search for that?

  I think the issue is making sure that iac_npft is consistent with the iac   and lnd namelists.  Presumably, that's for the configuration files and/or   bldnml to get right.

11/6/2019

* So, the plan now is to go step by step through the driver code and make   sure all the functions I need to call and run an iac time step exist and   are no longer just stubs.  Note that this means I have to make sure the   prep functions and everything for lnd and atm are also correct.

* Once I have that, I need to revamp buildnml to set the namelist, and then   try to build with the gcam_core stuff Kate sent me.  Hopefully, that will   be straightforward, as I've already done the hard work in getting   everything to build once before - but there are probably typos and   declarations I'll need to make sure are correct and consistent.

* I want to build with the new compset that they generated for me, so I'll   need to merge in that branch.  That will make sure we are coupling with   lnd (I think, maybe atm too).

* Then, I'm ready to try and run 

11/4/2019

* Now I'm trying to track down the domain and other elements of the   component_type, from component_type_mod.F90 - com_cx, gsMap_cx, x2c_cx,   c2x_cx, etc.  Are all these things getting initialized?  Do I have to   initialized them myself, or is there some kind of default behavior that   sets these up?

  Start with land, search for lnd%dom_cx, and see if I can figure out what   the hell that is and how to make sure it's correct for iac.

  There are _cx variables, which are (apparently) things done on the   coupler pes, and _cc variables, which apparently are things used on the   component pes, and then driver variables used on all pes.  Man, this   whole thing is a quagmire.

* The good news is that it seems like all this stuff is created and used   completely within the mct code - so it's internal structures set up   automatically, presumably from the global mapping info we set up   somewhere. 

11/3/2019

* So, after thinking about it all night:

  I think "merging" means - (a) putting all the input fields into one set   of AVects; (b) with the proper domain decomposition for the current   component; and (c) at least in the case of atm fluxes, scaled by the   appopriate grid cell fractions for the domain boundary you are fluxing   across. 

  It might also mean (d) interpolated onto the current component's grid,   which is what the mapping stuff seems like it should do, but I'm not sure   about that.  I still don't know where a lot of thise mapper_Sy2z   structures get used - if they *aren't* used unless you have a specific   purpose for them, or if they are automatically used, assuming they are   named correctly, by constructing the names with some kind of macro or   whatever. 

  Anyway, this means that the iac parts of prep_lnd_mod and prep_atm_mod   are *mostly* implemented - I just copied from other components, which has   to be the right thing.  There still may be some infrastructure stuff I   need to do to get, e.g., zfrac and fractions_az, and other things like   number of fields we are coupling and all that stuff.

* So, today I've got this:

@ Figure out seq_frac_mct.F90, and maybe seq_domain_mct.F90 @ Work through prep_iac_mod.F90 and see if I can figure out what "merging",   in the above sense, the l2z stuff might look like.

* So, looking at the top of seq_frac_mct, they do a big rundown of various   fractions.  The fundamental fractions are *frac, fraction of each   component on a grid.  So the idea is you build an set of fields   fractions_a(:) such that fractions_a(lfrac) is the fraction of land on   each grid cell in the atmosphere's grid.

  That example is loaded, of course, because I'm going to say that we can   assert this:

      fractions_a(zfrac) = fractions_a(lfrac)

  I honestly think that may be the only fraction we need to generate, at   least initially.  The only flux we have wrt to iac, either input or   output, is Fz2a_co2flux, so we just need to know zfrac on the atmos grid,   and since zfrac on any grid is identical to lfrac on any grid, and if the   iac grid is defined to be identical to the lnd grid, then there we go -   zfrac and lfrac are transitive in all these calculations.

? Now, just for fun, lets work through what would happen if the zgrid is   not the same as the lgrid, so there is interpolation involved.  In that   case, we would need the mapping_Fz2a to make this calculation:

      fractions_a(zfrac) = mapz2a(fractions_z(zfrac))

* But, that may not be *quite* how these things are calculated - well, sort   of.  First, they derive ocean and land fractions on their own grid:

      fractions_o(ofrac) = dom_o(frac)  ! ocean "mask"       fractions_l(lfrin) = dom_l(frac)  ! land model fraction

  ...then they use these to calculate ofrac and lfrin on the atmo grid:

      fractions_a(ofrac) = mapo2a(fractions_o(ofrac))       fractions_a(lfrin) = mapl2a(fractions_l(lfrin))

  ...then, finally, derive the land fraction on every grid from the ocean   fraction and subsequent mappings involving the land component

      fractions_a(lfrac) = 1.0 - fractions_a(ofrac) (zero for very small values)

      fractions_l(lfrac) = mapa2l(fractions_a(lfrac))       fractions_r(lfrac) = mapl2r(fractions_l(lfrac))       fractions_g(lfrac) = mapl2g(fractions_l(lfrac))

  This is all very confusing and involved, but I think the idea is to keep   everything self-consistent.  We generate a landfrac on atm by inverting   the ocean fraction and zeroing out very small landfracs.  So now, to make   all these calculations consistent, we then generate landfrac on land grid   by mapping the land frac from the atmos grid.  Then we map this new   landfrac on the land grid, to rof and glc grids using l2r and l2g   mappings, as well.

  Okay, if we assuming mapl2z is 1 (identity map), which is should be if   our grids are the same, then the way to make l,z, and a consistent is   what I said before:

      fractions_a(zfrac) = fractions_a(lfrac)

?  ...and if mapl2z is not, then we need to do this: ?  ?     fractions_l(zfrac) = fractions_l(lfrac)    ! since they are the same thing ?     fractions_z(zfrac) = mapl2z(fractions_l(zfrac))     ?     fractions_a(zfrac) = mapz2a(fractions_z(zfrac))

   It's that first one I'm not sure about - I *think* that's right, since   by construction both lfrac and zfrac measure the exact same thing, the   amount of land in this grid cell.  So fractions_x(zfrac) =   fractions_x(lfrac) by definition.   From there we map from l to the z   grid, and then from z to the a grid.

* Okay, so we now have fractions_a(zfrac), which is what we will need to   properly couple Fz2a_co2flux.  I already mocked in zfrac in   seq_frac_check, and I'm pretty sure at the moment I don't need   fractions_z for anything, since we aren't fluxing into the iac domain   space from anything.

! Okay, hold on a sec - we might need some mapping after all, becasue   mapping *might* also matter with domain decomposition.  I'm not really   seeing how the decomp comes into it - there are things like dom_l()   floating around, but I don't understand what they do.  

  I *think* I'm okay - zfrac is only calculated on fractions_a(), using   fractions_a(lfrac), so everywhere it shows up we are presumably using the   atmosphere decomp already.  But I don't really understand such things   well enough to declare for sure that's what's happening.

11/2/2019

* Holy crap.  I have *no* idea what prep_atm_merge() is doing.  It seems to   set lmerge(ka) to .false. if the field name starts with an "F", which,   what?  

  Oh, it has to do with the calculation - if lmerge(ka) true, you add the   current value in the l2x_a avect to the old value in x2a_a.  If it's   false, then  you replace the current value in x2a_a with the new value in   l2x_a.  So "merging" has to do with accumulation, I think, whether you   are summing (probably for average) or replacing.  I guess all the "F"   fluxes are replaced, rather than accumulated.

11/1/2019

* So, there are some examples (atm mapper_Sl2a, etc.) of the mapper being   generated and then not used for anyting other then seq_map_map() call.   This suggests either (a) that these mappers get called automatically, in   a way that the actually mapper pointer name isn't explicitly named (a   macro or lookup table or something like that; or (b) some mappers are   defined and then not really used - maybe they exist for some later code   extensions or options to make use of, or maybe they used to be used and   then later were not.

  Either way, I think it's obvious I need to define mapper_Fz2a (in   prep_atm_mod.F90) mapper_Sz2l (in prep_lnd_mod.F90), and mapper_Sl2z (in   prep_iac_mod.F90).  It looks like you map into the current component,   which makes sense, as that's the component currently doing the work.

* Note that the *flux* maps *do* seem to be called elsewhere, in   seq_domain_mct.F90 and seq_frac_mct.F90.  That also makes sense, as the   flux calculation between components needs more setup - getting fractions,   domain decompositions, getting the interpolations right.  We'll see if   this cime code maybe does something with a state mapper, too?  That would   suggest, then, that (b) is correct above, and we define maps that we   don't end up using for whatever reason.

* The prep_lnd_mod.F90 DOES do some additional stuff with their mapping -   calculating the "aream" stuff, which I don't understand yet.  I may run a   grep for that...See the aream bullet below.

* seq_domain_mct.F90 uses the mapping...somehow...to do domain checking?   across components?  As part of this it seems to set or calculate either   frac or mask, depending upon whether they are on the samegrid or not.  I   don't really understand that, and there wasn't a lot of information in   the comments.

  I need to track down some MCT/CIME documentation, at the function level,   to explain what problem these functions are trying to solve.

* seq_frac_mct.F90 is concerned with "surface fractions" - at least it has   some extensive comments at the top.  Whew, I did not get that at all.

  Okay, well, for each component there seems to be some mapping stuff, then   some fraction-based avects created, I guess having to do with the   fractions on all input component grids?  So if ice_present, we use   mapper_i2a and use the fractions_i and fractions_a to generate the ofrac   (ocean frac)...Man, I *really* do not get this at all.

  I really want to basically just say fractions_z are the same as   fractions_l - or maybe just *use* fractions_l for the mapping_Fz2a   directly.  

  I guess I need to review seq_map_map() again, to see what all these   calculations are. 

* I see copying values for fractions in specific cases:

  Man, I do NOT understand this frac stuff.

------ * Aream stuff:

1 aream is read in by rof_comp_mct.F90, with a note that it is filled in by   atm-lnd mapper...

2 There's some "area minimum" stuff for seaice and atmosphere in   mpas-source - do not know if that's related

3 ocn_comp_mct.F also appears to import aream

4 cam also has areamin, and then reads "aream" in atm_comp_mct.F90.  

5 lnd_comp_mct.F90 - reads aream again with a note that it's filled in in   the atm-lnd mapper.

6 Back in cime, we see it in seq_domain_mct.F90, component_mod.F90   (component_init_aream subroutine, I think) 

7 prep_glc_mod.F90 has a lot of aream_g stuff - described as "cell areas on   glc grid, for mapping".  Some more comments comparing aream_g and area_g,   which apparently can differ for "CISM with a polar sterographic   projection".   Anyway, this seems to at least have some comments about   what is going on here.

8 aream in seq_flds_mod.F90

* Apparently aream is a different representation of area, possibly used for   mapping?  Anyway, if I *do* need it for z2a mapping, I should be able to   grab it from the a2l calculation, like rof and lnd apparently do.

---------

10/30/2019

* Trying to track down the last little bit - the prep functions for   coupling, inside of cime/src/drivers/mct/main/prep_iac_mod.F90, and   probably modifications for prep_lnd_mod.F90 as well.

  Ah, nertz, I probably need to modify prep_atm_mod.F90, especially since   that one involves an actual mapping from the lnd/iac grid to the atm   grid. 

* So, mapping is complicated, but fortunately I'm mapping mostly state vectors,   which seems to be a little more straightforward.  Look at   prep_lnd_get_mapper_Sa2l() and the creation of mapper_Sa2l in   prep_lnd_mod.F90 - it looks like you simply call MCT to set up the state   mapping (also, there's a samegrid_lz (or whatever) argument, which I   guess simply sets an identity map, or at least that's what I hope).

  But, we still use mapper_Sa2l via prep_lnd_get_mapper_Sa2l() in   component_mod.F90, so after I generate similar mapper_Sz2l and   mapper_Sl2z structures, I will need to modify that file:

  .../cime/src/drivers/mct/main/component_mod.F90

* And, of course, I *will* have to do a flux mapping for z2a; I should   probably look in atm_lnd_mod.F90 for how to do that, but that's where the   fractions and stuff get used, hopefully.

! Okay, hold on.  I don't really know what these mappings do, and I can't   really find any instance of using mapper_Sa2l except to map aream,   whatever that is.  So I need to dig in deeper on whether we even need a   mapping at all from land to iac - it might be that we really don't.  

  Anyway - see what seq_map_map() and seq_map_init_rcfile() do, and see how   other components seem to be using them (hopefully, with some comments or   something).  

10/29/2019

* Coupling with cam:

1 cam_cpl_indices.F90: include index_x2a_Fazz_fco2_iac index

2 camsrfexch.F90: add fco2_iac(pcols) into cam_in_t type, and initialize to 0.

3 atm_import_export.F90:     read Fazz_fco2_iac into cam_in%fco2_iac.     Use cam_in%fco2_iac in cam_in%cflx(i,c_i(2)) (2 for fossil fuels)      tracer structure.

? Here we need to make sure we've correctly scaled by the iacfrac (which   should be the same as landfrac), AND that we are in the right units.   There's a note saying the cpl already multiplies flux by land frac - I   don't know if that's automatic or something triggered by how lnd   couples. 

  The units need to be kgCO2/m2/s - there's a conversion for ocean co2, but   land is apparently in the right units already, so hopefully that's what   we get from glm as well.

* Jeez, I think that's it - the fossil fuel tracer infrastructure is   already there, so we just need to copy in from x2a into the cflx element   and everything downstream happens automatically in cam.   

10/25/2019

* The surfdata on compy is in:

  /compyfs/inputdata/lnd/clm2/surfdata_map

  For some reason, you don't get ncdump when module load netcdf, I had to   manually add it to my path:

  PATH="$PATH:/share/apps/netcdf/4.6.3/gcc/4.8.5/bin/ncdump"

* It looks like all the grid data is in one of the surfdata files, although   the names are different (capitalized) and the dimensions are different.   Anyway, it has landmask and landfrac as well, so it should server as a   configuration file.

  But it brings up a point: the 2D grid cell dimension listed is in the   center of the bin; we have lone,lonw and lats,latn to define the edges.   Is any of this important?  Right now I'm thinking the center of the bin   is what I want as a coordinate, but it's possible the right thing is   (e.g.) the SW corner.

* New topic: coupling with cam.  This, thankfully, looks pretty easy.  

1 co2_cycle.F90 - c_i(ncnst) sets the co2 components: ocean, fossil fuels,   lnd, total.  

2 atm_import_export.F90 - c. line 118 - the cam_in structure has elements   to hole the co2 constiuents, so import them directly from x2a.  Note that   flux up to atm is negative, so they've been stored negative in the   coupler, and thus have to have a -1 mult here.

3 Then futher in, c. line 176, we combine these into a single cflux   structure, and then ultimately sum the constiutent elements to calculate   the total.

! So, the solution is: 

1 co2_cycle.F90 - set ncnst=5, add in constiuent elements for an IAC   component (name 'CO2_IAC', others the same).  My sense is this should be   i=4, with total moved to i=5, but I don't know if the 4th element is   hardcoded later on as the total co2.  It might be stupid to put CO2_IAC   as i=5, and sum 1-3,5 to make 4, but it's not really that big a deal.

2 Follow the lnd coupling model - if the index exists, fill the fco2_iac(i)   element of the cam_in structure, use that to set cflx(i,c_i(4)), and then   make cflx(i,c_i(5) = the sum of cflux(i,c_i(1..4)).

* Right now I'm grepping to see where c_i(4) is used...nuts, it looks like   it's hardcoded in contrl/camsrfexch.F90, line 491, probably where we sete   the co2 for use later on.  Okay

? ??? 3 Change that to c_i(ncst), so we always use the largest as the total? ? 

* Okay, grepped for when we use cflx, in these files:

  ./control/camsrfexch.F90 (line 245)   ./dynamics/fv/metdata.F90 (lines 513, 522, 539)   ./chemistry/modal_aero/aero_model.F90   ./chemistry/mozart/chemistry.F90   ./chemistry/bulk_aero/aero_model.F90

  So, that should be easy enough to track down and make sure that simply   ncnst++ for co2_cycle and moving total from 4 to 5 doesn't hurt anything.

* camsrfexch.F90 - this line is the problem, the only one that uses c_i(4)

     cam_out%co2prog(i) = state%q(i,pver,c_i(4)) * 1.0e+6_r8 *mwdry/mwco2

  ...where i loops over columns.  Need to look over state%q.

! Actually, c_i(1..ncnst) is a global index for constituents, probably in   some kind of global array of things to track.  So the state%q is a master   3D array of global variables and stuff.  I assume c_i(:) gets set to some   index, from 1 to ncnst, so it can be used as needed, during   initialization.

  Okay, so this should work:

  In camsrfexch.F90, cam_export:

    use co2_cycle, onlye: co2_transport, c_i, co2_ncnst => ncnst     ...     cam_out%co2prog(i) = state%q(i,pver,c_i(co2_ncnst)) * 1.0e+6_r8 *mwdry/mwco2

  This will always use the last element of c_i as the index of total carbon   field. 

* So, where is c_i used from co2_cycle?

  ./physics/cam/cam_diagnostics.F90:840:      ./physics/cam/cam_diagnostics.F90:1528:      ./cpl/atm_import_export.F90:17:      ./control/camsrfexch.F90:404:   

  So, we know the last two - look at cam_diagnostics.  Well, it looks like   it's in a 'do m=1,4' loop, so of course all this stuff is hardcoded.   More worrisome is the second instance, line 1617:

       do m = 1,4           call outfld(sflxnam(c_i(m)), cam_in%cflx(:,c_i(m)), pcols, lchnk)        end do

  So, look up sflxnam, from the constiuents module.

* Okay, it looks like, then the global index is this pcnst parameter, which   is the number of advected constituents, set by the environment or   configuration variable PCNST (I think).

  Also, we never use ncnst from co2_cycle.

! So, my conclusion here is that this is somewhat more complicated than I   had hoped.  I could add fco2_iac as c_i(5) and not worry about it.  Or, I   can add the iac co2flux as an addition to lnd co2flux, and not worry   about it.

  But, if I want to do something better, like include iac co2 as a tracer,   then I have to learn how tracers are set up in cam, add the   infrastructure to include iac_co2 as a tracer, and make sure PCONST is   modified to include this extra tracer somewhere.

! Wait!  One of the three component constituents of co2flux is "fossil   fuel", which you get by calling read_data_flux() on a file, and then each   step calling interp_time_flux on it.  My guess is this is standard model   - current fossil fuel contributions to co2, with 1% increase per year or   whatever. 

  But, this means we can *use* the fossil fuel flux tracer for the iac   constituent!  We don't need a new tracer, we are replacing fossil fuel   fluxes from a data model with the iac model for co2flux.  I should   confirm this with Kate, but it really seems consistent to me - I can't   imagine we would have a data-driven fossil fuel model *alongside* the iac   model of human factors.

  So, this means we put the iac_co2flux into c_in(2), and let the current   tracer infrastructure deal with it.  It's the simplest solution, and   it's, as a bonus, likely correct!

  It's possible it might be mislabeled, of course - iac might include other   factors beside strictly fossil fuel burning (biomass fuels, etc.)  Still,   I think that makes sense to me.

10/19/2019

* Well, I'm coming to the realization that we should probably order things   (lon,lat) in the fortran code, rather than (lat, lon).  In the netcdf   files we see (lat,lon), but netCDF follows the C row-major ordering,   which means lon varies fastest.  So when you extract a 2D array from   netCDF it will be in (lon,lat) order in a fortran array.

  Note, also, that the global indexing convention I see everywhere is:

a start at south pole b go west to east (lon) c go south to north (lat)

  Thus, lon varies faster than lat, in the global indexing.

  So all this suggests we need to keep things this way - (lon,lat,pft),   etc. Reviewing Kate's test files, she goes lat,lon,pft - lat fastest, pft   slowest.  Thus, all my current dimensioning should be inverted -   lon,lat,pft.  Mostly, this should just affect the initialization and   allocation right now, but also in iac_import.

@Xiac_init_mod.F90 @Xiac_import_export.F90

! Okay, I'm getting a little lost, yet again, in all the various places I'm   keeping things.

? Storing area and landmask in iac_ctl, along with lat,lon, and a global   index apparently from 1,ngrid=nlat*nlon.  My idea is to put everything we   need for the global seg map into iac_ctl.  NOTE, however, that it's   possible that we need the 2D global index of lat and lon, in which case   maybe we need a whole separate structure...

! HOWEVER! This means this structure is pretty big, so if we just want to find   the sizes nlat, nlon, npft, ngrid, we shouldn't drag this whole control   structure around.  Instead, that's what gcam_var_mod.F90 is for.  So   maybe we should move variables those over there, or duplicate them there,   or soemthing.

? Working on RTM ID key - what I *want* it to be is a mapping from   contiguous integer regions to non-contiguous global regions.  That way,   you can say (e.g.) we have 8 procs, so domain 1 is [1,ngrid/8].  But, when   looking at which gridcells those are, use ID0_global(n) to find them.   That way, you can distribute non-contiguous grid cells, while still   keeping a contiguous array of indeces to deal with.

  But!  I'm not at all sure that's what's going on.  It might be...

XXXXX   Well, that's how it ends up in rtm - a dnID_global(n) and an IDkey(n)   which go back and forth between each other.  If you give an ID to IDkey,   you get the global index.  If you give the global index to dnID_global,   you get the IDkey.  I assume this is all for domain decomposition, but I   don't really know. XXXX   I don't think that's quite right - see below (up to next bullet).

  If that's true, then none of it matters to me - we have no domain   decomposition for iac, so our global index is always contiguous and we   don't have to worry about any of this.

  Okay, going further - it looks like teh dnID stuff has to do with   determining land, ocean, and ocean outlet from land masks.  I really   don't understand what is going on c. 575 in RtmMod.F90, where it (a) sets   all points to ocean mask, then loops over all points, and if the   dnID_global(n) is between 1,ngrid it sets to gmask to 3.  Then it does   the same thing, and if dnID_global(n) is between [1,ngrid] it sets   gmask(n) to 1.

  I guess the dnID, read from a config file, lists grid points that are   downstream of a river basin.  The key keeps these all together,   contiguously - so basin is 1, downstream is 2, downstream is 3, outlet is   4.  The IDkey maps global indexes into this [1:4] numerical block, and   the dnID finds the next one in the block.

  Okay, so this part of the code marks everyting ocean by default; then it   finds anything that has a downstream and marks the downstream as outlet.   Then it finds anything that has a downstream and marks it as land (which   overwrites everything but outlet cells as land).  In this way, we have   gmask==1 is the lnd/river boundary cells, and gmask=3 the ocean/river   boundary cells.

* Once again, all of that doesn't matter to me at all, but it's nice to   finally figure it out.

10/18/2019

* Here's some grid output from a clm2.h1 file, which we either want to use   directly or could use to extract grid information for iac:

----------- - clm2.h1 ----------- 	float lon(lon) ; 		lon:long_name = "coordinate longitude" ; 		lon:units = "degrees_east" ; 		lon:_FillValue = 1.e+36f ; 		lon:missing_value = 1.e+36f ; 	float lat(lat) ; 		lat:long_name = "coordinate latitude" ; 		lat:units = "degrees_north" ; 		lat:_FillValue = 1.e+36f ; 		lat:missing_value = 1.e+36f ; 	float area(lat, lon) ; 		area:long_name = "grid cell areas" ; 		area:units = "km^2" ; 		area:_FillValue = 1.e+36f ; 		area:missing_value = 1.e+36f ; 	float topo(lat, lon) ; 		topo:long_name = "grid cell topography" ; 		topo:units = "m" ; 		topo:_FillValue = 1.e+36f ; 		topo:missing_value = 1.e+36f ; 	float landfrac(lat, lon) ; 		landfrac:long_name = "land fraction" ; 		landfrac:_FillValue = 1.e+36f ; 		landfrac:missing_value = 1.e+36f ; 	int landmask(lat, lon) ; 		landmask:long_name = "land/ocean mask (0.=ocean and 1.=land)" ; 		landmask:_FillValue = -9999 ; 		landmask:missing_value = -9999 ; 	int pftmask(lat, lon) ; 		pftmask:long_name = "pft real/fake mask (0.=fake and 1.=real)" ; 		pftmask:_FillValue = -9999 ; 		pftmask:missing_value = -9999 ; 	double grid1d_lon(gridcell) ; 		grid1d_lon:long_name = "gridcell longitude" ; 		grid1d_lon:units = "degrees_east" ; 	double grid1d_lat(gridcell) ; 		grid1d_lat:long_name = "gridcell latitude" ; 		grid1d_lat:units = "degrees_north" ; 	int grid1d_ixy(gridcell) ; 		grid1d_ixy:long_name = "2d longitude index of corresponding gridcell" ; 	int grid1d_jxy(gridcell) ; 		grid1d_jxy:long_name = "2d latitude index of corresponding gridcell" ; ----------

* I should plot out grid1ld_lon and grid1d_lat to make sure my global index   is right.

* Anyway, we need the landfrac and area for GCAM, but we also need landmask   (to turn into iacmask?) and the lat,lon grid for the global seg mapping.  

! Okay, here's a problem!

  It seems like from the way rtm is reading it's config file that it stores   things in the order lon,lat, rather than lat,lon like we see in clm2.h1.   Upon further reflection of the global index, it seems that might go   (lon,lat), too - start from lower left corner, and go across then up   (longitude, then latitude), and thus the lons vary fastest.

  But!  This all might be because netcdf stores in row major (C-ish) order,   ratehr than column major (Fortran-ish) order.  So maybe when you call the   fortran netCDF functions, you have to reverse the indeces?

  In other words: in fortran, you gotta dimension (lon,lat), while in C   (and netCDF) you dimension/store [lat,lon].  Both of them have lon moving   fastest, which matches with the way they describe the global index being   calculated. 

! So, what this means is that in fortran, I probably need to dimension with   lon first, then lat, instead of the way I've been doing it.  My C   background has caught me up!

! Furthermore, the 3D->1D flattening/unflattening we do to interface with   gcam is a problem: I need to send (lon,lat,pft), so Kate needs to expect   n=lon+nlon*lat+nlon*nlat*pft as the 1D index.

@ I need to review the global index calculation in rtm and lnd, and see if   it really is going i=lon, j=lat, with i the inner loop.

@ Also, rtm is doing something weird with the ID key for global index which   I don't get - review that with lnd.  Sheesh!  I thought I had this   figured out!

10/15/2019

* Kate's new gcam has a somewhat different (although better) interface tree   with E3SM.  Here's the calling sequence:

  iac_comp_mct() calls     gcam_setdensity_mod(...,gcami) - this is where inputs are used

       setdensitycGCAM(...,lndarea,lndfrac,pftwgt,npp,hr,nlon,nlat,npft,...)           - this call needs to be modified in the new gcam_comp_mod.f90

    gcam_run_mod(...,gacmoluc,gcamoemis,...)        - this runs gcam and sets up the outputs for use in glm (luc) and        (maybe?) to send back to atm (emis).  Need to track down what        happens with gcamoemis.

* Anyway, focusing on lnd->iac coupling then, gcami is (currently) what I'm   calling lnd2iac_vars(f,n), where n is the flattened [lat][lon][pft] index   and f runs over all the fields.  

* My current thinking is that area and lndfrac can be read from a   configuration file (not needing to be coupled), and thus lnd2iac_vars()   will have nfields=3 - hr, npp, weights.

! Okay, I need to talk with Kate about what format these inputs should   take.  Here is the setdensity call, cleaned up a bit:

  void setdensitycgcam_(int *yyyymmdd, 			double *aELMArea, 			double *aELMLandFract, 			double *aELMPFTFract, 			double *aELMNPP, 			double *aELMHR,  			int aNumLon, int aNumLat, int aNumPFT, 			std::string aMappingFile, 			bool aReadScalars, bool aWriteScalars)

  So, if I had to guess, aELMArea and aELMLandFract are static and   dimensioned [lat][lon], or n = lon+nlon*lat

  ...and, currently, aELMPFTFract, aELMNPP, aELMHR are [lat][lon][pft], or   n = pft+npft*lon+npft*nlon*lat

  But I might have those backwards, because of the column order thing.  The   thing is, I can order them however I want if I linearize myself rather   than passing in foo[i][j][p] and having C automatically format them.  In   other words, in iac_import() I can store them as a 1D row major order   array directly, then they'll be row-major when I pass them through.

* Well, in Kate's new gcam-core code, which contains a self-contained test   dataset in gcam-core/cpl/data/npp_mean_pft.txt etc., seems to suggest   that pft is slowest varying and lat is fastest varying.  That's the way   it would work in a fortran foo[lat][lon][pft] array, but (currently) in   iac_import() I'm building an n = pft+npft*lon+npft*nlon*lat, in which pft   is fastest and lat is slowest.

  In other words, it *appears* that kate is expecting a fortran-ordered   [lat][lon][pft], rather than a C-ordered [lat][lon][pft].

  Ultimately, we can flatten either via an nf or nc index:

    nf = lat + nlat*lon + nlat*lon*pft     nc = pft + npft*lon + npft*nlon*lat

! (Note - this is zero offset!  For one offset, you have to replace   anything with a non-one stride (anything multiplied) with x->(x-1)   (e.g. (lon-1) in both cases.  This suggests, maybe, it's better to just   store as a 3D array, since C will flatten automatically on Kate's end.)

  We just need to make sure GCAM and my iac code does it the same way!

! In main.cpp, the test executable that Kate mocked up, these things are   allocated in this way:

    double *gcamiarea = new double [NUM_LAT * NUM_LON]();     double *gcamilfract = new double [NUM_LAT * NUM_LON]();     double *gcamipftfract = new double [NUM_LAT * NUM_LON * NUM_PFT]();     double *gcaminpp = new double [NUM_LAT * NUM_LON * NUM_PFT]();     double *gcamihr = new double [NUM_LAT * NUM_LON * NUM_PFT]();     double *gcamoluc = new double [NUM_GCAM_LAND_REGIONS * NUM_IAC2ELM_LANDTYPES]();     double *gcamoemis = new double [NUM_LAT * NUM_LON]();

  The gcamo ones will also be allocated in E3SM, which brings up a point -   gcamoluc() is, I think, used internally by glm() and maybe other   downstream functions after the rungcam() call.  As such, I don't need to   worry too much about it - but what are num_iac2elm_landtypes and   num_gcam_land_regions? How are they related to pfts?

* In particular, after all this mess, what is going to give me the iac2lnd   variables?  I think that's the output of updateannuallanduse(), through   that fortran interface, after glm runs.

=======================================

* On another topic, I've been fighting with the gsl domain mapping stuff,   and I think I got it figure out - the domains (based on the processor   decomposition of the grid) are how you have to talk to the global mapping   - that's why there's all this stuff in (e.g.) rof_domain_mct() about   setting up "lat", "lon", "area", etc. attribute vectors via   mct_gGrid_importRAttr().  For each of your domains, you need to define   the grid for that domain - this is how MCT knows how to transform data   to and from this domain.  E.g. - I'm giving you this field on this   domain, now you take it and put it on (a) a new grid, and (b) a new   decomposition for whatever component requests it.  So you gotta build a   grid associated with the domain pointer dom_r, and give it the lat, lon,   area, etc. information that you probably read from a configuration file   during initialization .  Then whenever you couple you send that dom_r   along, and there you go. 

  Okay, so if this view is right, then the way it works for iac is: we have   ONE domain, but we still need to build a domain decomposition if only so   we can associate a grid with it.  I could use the whole iac_ctl%begg,   %endg construction, which would allow us to add a decomp later, but it   might just be easier to just run from 1,nglobal.  Or just make %begg=1,   %endg=nglobal and keep everything the same.  Honestly, from the way gcam   works, I'm not sure it makes sense to even consider a domain decomp...

@ Check rof init to see where rtmCTL%latc,lonc,area,etc. are read in -   just to verify that we do, in fact, read all this stuff from a config   file into a global structure, and that rof_domain_mct() is about   extracting that stuff into the dom_r structure.

? Also, look at rof_SetgsMap_mct() and rtmCTL%gindex into gsMap gindex -   I'm not sure what's happening, but maybe seeing how rtmCTL%gindex is set   will clear it up.  It's a lot more complicated in lnd - it sets the   global index into ldecomp%gdc2glo, which, hm.

* Some notes on rof_comp_mct.F90:

? The "iam" in c. line 484 of the ? mct_gsMap_orderedPoints(gsMap_r,iam,idata) ? is the processor number, and I guess it extracts into idata the global ? gridpoint number, which we then  set in dom_r...I still don't quite get ? this.  It might be handled automatically by MCT, assuming I've set ? everything up correctly for all that.  



10/11/2019

* I've been looking into how we get lat/lon and number of pfts and stuff   like that - whether that's all coupled (so we need to import lat/lon,   etc.) and my conclusion is - no.  It *seems* like the way this stuff is   set is via a configuration file.  I've sort of been assuming we could use   the clm2.h1 file for this, or extract what we need from an exisiting one,   but it's possible we have to construct it all ourselves.  Regardless,   then, we need lon(lon) and lat(lat) and npfts; from there, we should be   able to construct the global mapping from nlat and nlon, and from there   we should be able to go back and forth between the [g] indexed coupled   fields and the [lat,lon,pft] indexed fields we need and/or get out of   gcam.  

* Although, to be accurate, we want to flatten [lat,lon,pft] into a   one-dimensional index to pass to and from GCAM, if understand what Kate   wants.  I'm pretty sure we get a 1D linearized (flattened) array out of   the glm stuff (i.e. out of updateannuallanduse_v2.c), although I have to   review this.

  That actually helps bypass the col-major (fortran) vs. row-major (c/c++)   issue, as long as we indicate how we linearize stuff.  It also means we   need nlat, nlon, and npft available everywhere to go back and forth   between 1D and 3D.

  So, I'm thinking all this goes in the iac_ctl structure:

  iac_ctl%nlat   iac_ctl%nlon   iac_ctl%npft

  iac_ctl%ng - number of global indexed points, nlat*nlon

  iac_ctl%latg[g]   iac_ctl%long[g] - get lat and lon from g      iac_ctl%ijlat[g]   iac_ctl%ijlon[g] - lat and lon index from g

* (Look up ne30 grid, probably discussed below - we may have configuration   files that give all these grid values for that, in our unit test runs, etc.)

10/10/2019

* I can't quite figure this out:

  In iac2gcam_mod.F90 (orig) for iESM, we loop over n=1,npft (the   patch-level subgridded index), and extract i,j,k as (lat,lon,pft) using   the pfts1d_* variables from the .h1 file.

  We also extract the mapping from pft->col into n1=pfts1d_cols(n), which,   as I note below, seems to follow from the hiearchy - there's a c for   every p.

  So: clm p is "n" here, and clm c is "n1" here.

  Okay, look at the heating rate calc at line 1696: 

     hr_pft(i,j,k) = hr(n1,1)

  To me, this is saying - set the hr for this *pft* directly from the value   for that pfts column.  In other words, the hr(c) is the same for every p   that maps to that c.

  Shouldn't this be weighted by wgtscol(p)?

  Or, maybe since the calculation involves a max over the month, it doesn't   matter...?

  Anyway, I need to understand more about how this p->c mapping works.  I   get that (apparently) every p has a unique c, but I suspect the reverse   isn't true, so if the hr is a flux it really seems like it should be   weighted by the fraction its represented in each subgrid cell...

  Or, not up here - maybe we apply the wgtcol later, after all the "yearly   means of month max" type stuff is taken care of.

* I'm trying to understand the Vegetation types, as listed in   VegetationType.F90, vs. the gcam pfts, as indicated in (probably amongst   other places) iac2gcam_mod.F90.  I *want* to say that veg_itype maps   directly to pfts - but we have 25 of the former and 17 of the latter!   So, side by side, here's what I get for various index values:

   p    veg_type                %pftname   --    --------		-------    0	not vegetated		bareground    1	needle ev temp tree	needle ev temp tree    2	needle ev bor  tree	needle ev bor tree    3	needle dec bor tree	needle dec bor tree    4	broad ev trop tree	broad ev trop tree    5	broad ev temp tree	broad ev trop tree    6 	broad dec trop tree	broad dec trop tree    7	broad dec temp tree	broad dec temp tree    8	broad dec bor tree	broad dec bor tree    9	broad evergreen shrub	evergreen shrub   10	broad dec temp shrub	temperate shrub   11	broad dec bor shrub	bor shrub   12	c3 arctic grass		arctic grass   13	c3 non-arctic grass	c3 grass   14	c4 grass      		c4 grass   15 	c3 crop			corn   16 	c3 irrigataed		wheat   17 	corn   18    irrigated corn   19    spring temp cereal   20 	irr spring temp cereal   21    winter temp cereal   22	irr winter temp cereal   23	soybean   24	irrigated soybean

* Okay, so it looks like up to p=14 they match exactly.  After that, it   looks like they broke out corn and wheat into finer categories.

? So, what does this mean - can our gcam handle 24 pfts vs. 17?  If not, do   I need to remap some of these to our 17 pfts?  

  Also - note that the column on the left is just from the comments; is   there any reason to think it's accurate now?  Could it be from the   future, and everything now maps to just the first 17 elements?  Or could   it be old and there are even more pfts to deal with?

  Finally, note that in iac2gcam we map our pfts further into somewhat   different crop ids - c3 grass goes to Grassland, Pasture,   UnmanagedPasture, and FodderGrass, whereas c4 grass goes into those four   crops as well as "miscanthus".

? So, do we have to remap these p-values on the left to the iac gcam   related crop ids?

  (Note that pft 16 "wheat" currently maps to zero crops...).

* Well, looking at some clm2.h1 files I got lying around, I'm seeing   "natpft=17" in the header.  So it could be the 24 vegtypes aren't really   implemented, and we just go up to "c3 irrigated" or something.

! I think I've convinced myself that we should just create a   pft-dimensioned hr, and use hr_pft[p]=hr_col[pfts1d_col(p)].  In other   words, map the hr directly from c to p, using the p(c) mapping.

  The reason for this is that hr is a *flux*, which is stuff per unit area   per time.  Therefore, for all the p's for a given c, we are saying they   have the same hr.  We don't apply the weights here, because ultimately we   are going to use the p-to-gridcell weights - the reduced size of the pft   subgrid cell compared to the column-subgrid cell is handled automatically   by the using the p-weights themselves.  (Note that the sum of p-weights   for each p in a given column should equal the c-weight for that column.)

  Okay, so hr_c[c] -> directly to hr_p[p].  I think that means we should   couple on the pft subgrid, rather than the column subgrid - have the   lnd2iac_vars convert to %hr[p] right there.

? One question I have with all this - we are therefore coupling 51 new   fields (assuming npft=17), all on a global grid.  At 720x360 resolution,   that's 259,200 numbers per field, or 13,219,200 numbers total.  At 4 byte   floating point, that's 50 Megs of data we are sending through the   coupler.  Is that going to cause a problem?    10/9/2019

* Just a note up top here, clarifying some things below:

  clm has a number of different subgridding schemes based on land type,   that (I think) overlap and perhaps subset each other, depending on the   application.  There are 9 landunits (l), 11 columns (c), and 24   vegetation types (p - "patches"), all of which are possible in each   gridcell (g).  The VegetationType.F90 has conversions to map from p to   l,c, and g.  The ColumnType.F90 can map c to l and g, and   LandunitType.F90 provides maps from l to g.   So the hierarchy is   something like g -> l -> c -> p, from biggest (single) to smallest subunit.

  So I want to claim that patch-level fields, "p" map to our 17 pfts, but   the 24 vegetation types listed in VegetationType.F90 belie that a   little.  I need a list of our 17 pfts and see how they line up.

? Also, I need to review how we might go the other way, to build an hr[pft]   from our hr[c] output.  I no longer think I know how the iESM used hr -   I thought it was reduced down to a single value for the grid cell, and   then applied to all pfts, but now I wonder if there was some sort of   reverse calculation: for each [p], find the appropriate [c], get that   hr[c], then distribute to hp[p] via the weights of that p for that c   using %wtcol from the VegetationType.F90.

  But all that doesn't sound at all like what I remember from   iac2gcam_mod.F90.  So, back to reviewing that...how did we construct   hr[lat,lon,pft] out of what we got out of that clm .h1 file.

* I keep trying and failing to understand how clm does coupling, because   I'm trying to figure out the best way to do the same thing for iac.  So   I've been going through the lnd2atmMod.F90 and associated code to see how   we do l->a code.

* The problem is that there's a big chain of disparate functions that are   called to put this all together, and ultimately lnd_run_mct() (a) gets   the lnd2atm_vars variable from lnd2atmMod.F90, and then (b) calls   lnd_export() with it as a variable.

  Thus, to follow this path, we need to:

@ Create a lnd2iacMod.F90 module, to deal with building lnd2iac_var

@ Figure out where in the calling sequence we actually fill in lnd2iac_var

@ Modify the call to lnd_export() to include a lnd2iac_var argument.  We   might use an interface, so that if we have iac we have three arguments   and without we have two, but I hate that.

* Okay, the key issue I'm having is - I see where npp and hr and (I hope)   wgtcell are in the code, as discussed way below.  The question is how to   go from the patch level reduced 1D coordinates (begp:endp) into the   [gricell,pft] format we need to couple with.

  The lnd2atm coupling all deals with these c2g and p2g functions from   subgridAveMod.F90, which involve some complicated averaging - but that's   because the lnd2atm coupling is all about getting an average value for   the boundary of the gridcell to flux into the atm component.  Hence,   p2g() averages over all the pfts, whereas c2g() averages over...um, I'm   not sure.  Does "column" mean "vertical extent"?  That doesn't make sense   (to me) for the land, although I guess we have the orography subgridding ? to represent various heights.  Anyway, need to review c2g() again...

? (Aside: does that mean the hr[c] is a vertically expressed variable?  I   don't see that in how we use it iac2gcam() etc., but maybe I'm missing   it.) 

! Anyway, pft-averaging is NOT what we want to do for l->z coupling!  We want   to *keep* the pft distribution, not average over them!  In the context of   lnd2atm, pft  is "subgridding" - multiple values over the grid cell, that   you then need to condense down to a single value for that grid cell.   That's what subgrid averaging means.  So p2g() does something more than   what we want.

* But, looking at p2g() might tell us how to go from the 1D patch [p]   representation to the 2D [gricell,pft] representation.  It appears that   the veg_pp struction has these kind of conversion functions built in:

  c = veg_pp%column(p)   l = veg_pp%landunit(p)   g = veg_pp%gridcell(p)

  ...to go from our reduced, combined coordinate p over patches to the   components.  It also means the "column" is a different thing than the   "gridcell", so I suspect col_pp%gridcell(c) also exists to do the same   thing. 

* Anyway, this is promising - the question is, is "landunit" the same as   "pft"?  At the top of VegetationType.F90, we have a description of 24   "vegetation types" - I have no idea how this maps to pft, is it a   superset, a different decomposition, what?  

? So I need to figure out how to go from "landunit" to "pft", as I   understand the term.

! Okay, so looking at ColumnType.F90, we see 11 different "column types",   representing various broader categories - soil, ice, wetland, various   urban stuff, crops.

  Also, col_pp%pfti(c) and col_pp%pftf(c) represent the start and end pft   index for each column, and col_pp%npfts also give the number of "patches"   for each column.

? So, we take patch->column, then go from column to a continguous set of   pfts?  Yeesh.

10/7/2019

* I've been working on the coupling:

@ seq_flds_mod.F90 needs more work - there are conversions from the   specific (local) z2x_states, x2z_states etc. variables to global   seq_flds_x2z etc. variables.  I'm not sure of why everything takes three   steps to do, but whatever.

* So, we need to put all our coupled variables onto the 1D global index   grid, so the trick with coupling to and from clm is to go from the   reduced pft [p] and column [c] grid to the full global grid.  I'm   *pretty* sure that the global index simply runs from lat,lon=(0,0) to   (nlat,nlon) - the full global map, for the whole worlds.  That's why we   have those landfrac and landmask etc. variables that we send to and from   the coupling - to say which cells are land and sea and air etc., and to   find the boundary between domains to convert to fluxes, and to correctly   convert fractions and masks when we change the resolution of the grid.

  So that's what that gsl mapping stuff does - providing information about   each and every cell in teh world in your grid, so MCT can convert and   flux into and out of a different coupled grid.

* Now, since we only couple on the global [g] grid, we need to deal with   multidimensional variables on that grid - i.e. [lat,lon,pft] for us.  The   solution is to define npft different variables to couple, one for each   pft, and then convert each pft from [lat,lon]<->[g].  

  As it turns out, we have an example of exactly this - the glc to lnd   coupling involves different topo heights for each cell.  So they do   exactly what I say above: couple glc_nec separate fields.  In   seq_flds_mod.F90, you declare the fields you are coupling by string tags,   which are (internally) assigned a field index that all components and   coupler use to pull out of the big massive 2D array of AVects.  So in   seq_flds_mod.F90 you simply need to loop over npts (like necs) to   declare and track your npfts variables.  Then in the coupling functions   you build an array of field indeces to use.

  For example, see c. L265 in clm_cpl_indeces.F90 - you are building the   index_x2l_Sg_foo(num) index array, as num runs from 0 to glc_nec_max.   This makes it trivial to pull out the [g] for each [num], building a   [num,g] array of data, which you can then use to ultimately build a   [lat,lon,num] array to use internally.  (Or, I guess, in clm you build a   reduced [num,c] array or maybe a reduced 3D-to-1D topo-based [t] array,   or something like that).

* The glc 3D coupling loop is moved into a subroutine in seq_flds_mod.F90,   for some reason, and it seems to call a glc module function to find   glc_nec.  I was thinking we'd just have a namelist variable somewhere   that seq_flds_mod.F90 can find it - either in the drv namelist or in a   iac namelist that the coupler reads.  I'm not sure the latter makes sense   - it may mean we have to defined it whether or not we are running with   iac, which is not what we want to do.  I haven't worked a lot with   namelists, though - it *seems* like we can just by default turn the iac   namelist variables off, and only turn them on in compsets where we are   running with iac.  So, in non-iac runs, we set iac_npfts to 0, and   active_gcam to 0, and everything is okay.

* One advantage of definine iac_npfts by namelist is that it's a *variable*   - we don't want to hardcode 17 as npfts, because in later versions we are   going to want to define as many plant types as we want (Kate mentioned   this explicitly in the last phone call).  So, this lets us do that, as   long as the code knows how to deal with variable number of pfts.

* Here's a list of the fields we need to couple, in all three directions:

  l->z fields:

    Sl_hr (non-pft indexed)     Sl_npp_pft1..17     Sl_pftwtg_pft1..17

  z->l fields:

    Sz_pct_pft1..17  ("Percent pft of vegetated land unit for pft [p]")

  z->a fields:

    Fazz_fco2_iac

* I've probably taken notes on this below, but it is my contention that the   l<->z coupling should all use state variables rather than fluxes, even   though I think hr and npp both have units of a flux.

  My contention is that there is no actual domain boundary between lnd and   iac components - they essentially sit on top of each other.  So we have   no boundary to flux across, which is something the MCT would want to do   if you declare it as a flux.  I am not certain about this, though - the   other component boundaries don't have to be across grid cells - for   example, land and air boundary is vertical, so the "boundary" is simply   the overlap between landfrac and atmfrac in each grid cell (and atmfrac   is always 1, because we have atmosphere in every cell).  So perhaps the   100% overlap between iac and lnd is normal, and the "flux" between them   is a straight up null-op, just like it might be for l->a in cells where   there is 100% land cover.

  Similarly, the river and glc domains seem to be subsets of lnd, in the   same way lnd is a subset of atm.

  Anyway, if that's the case, then we need to change Sl -> Fzll ("flux into   iac from land, calculated by land), and Sz -> Flzz ("flux into lnd from   iac, calculated by iac).

  It may not matter - perhaps Sx and Fxyz give exactly the same answer in   cells where there is 100% overlap.  Still, it makes sense to me to keep   Sx, so we don't have any flux sign issues (there's a hierachy of   components that the flux arrow follows).

  Of course, the co2 flux from gcam into atm is a straight up flux, just   like it would be from lnd to atm, hence it has to be Fazz.

9/25/2019

* Okay, everything below this X line is wrong, down through 9/23/2019.   According to Gautam, you can NOT couple an arbitrary 1D array - it *has*   to be on the grid.  I do not know why, but it does.  Thus, sending a [p]   based array through the coupler doesn't work - you have to extract the   [i,j] stuff first (or put it on the [c] grid, or maybe the [g] gridcell   grid).

  This means that npp(lat,lon,pft) is a problem - it's 3D!  It is not   natively on the grid!

  So the suggestion is to make *17* different npp fields to couple -   npp(lat,lon,1), npp(lat,lon,2), etc.  Same thing with pft_weights,   probably.  The hr should be okay, once it's reconstructed back into [c]   and/or [g].

? Which, of course, brings up something else: what is [c] vs. [g] - column   vs. grid?  I think this has something to do with the "global mapping" and   global index stuff I sometimes see in reference to the global mapping.

* Anyway, this means my plan to initialize stuff via the .h1. file is kaput   - instead, I'll need to figure out how to convert from the global mapping   index to lat,lon, and reconstruct the 3D [i,j,k] array.

* Initialization is a problem, too - or is it?  Should I know from a   namelist or configuration how many pfts, lats, and lons to expect?   Assume that's true, then just allocate everything based on that.

* Okay, I need to reread the MCT and GSL papers again, and reevaluate all   the global mapping in lnd, rof, atm with an eye towards figuring out how   to deal with the global mapping, how to put coupled info on that, and   stuff like that.

* I think the ultimate idea was to put everything on their own   well-specified grid, and then MCT would know how to convert everything to   a different grid, so 

* I also need to modify seq_flds_mod.F90 to deal with the 17 npps and 17   pft weights.  Great.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

9/25/2019

* Aha, *finally* got it.  iac2gcam has a lot of cruft I need to get rid of,   but lots I need to keep - reading restart files, first run   initialization, stuff like that.

* Anyway, the key to going from coupler 1D indeces to [lat][lon][pft]   starts at line 1642.  We've read in our 1D arrays from the .h1 file,   along with mapping variables pfts1d_ixy(p), pfts1d_jxy(p),   pfts1d_itype_veg(p).  We've calculated a mapping from [p] to [c] as   pfts1d_cols(p) using cols1d_ix(c) and cols1d_jxy(c) - see below.

  Also looks like we need cols1d_itype_lunit(c) and pfts1d_itype_lunit(p). 

  So, we loop "n" over all our [p] values, calculate i (lat), j, (lon), k   (pft), and column index n1, and simply set npp_pft(i,j,k) = npp(n,1) on   line 1688 and hr_pft(i,j,k) = hr(n1,1).

  (I'm not sure why we need a pft based value for hr_pft, since it isn't   indexed by pfts on input, but whatever.)

? Also, I'm not sure what that second index is for npp(n,1) and hr(n1,1).   Right now, I'm assuming it comes from the netCDF read - see, eg., line   1617, where we read NPP using nf90_get_var().  My guess is the second   index is a dummy time index, and since we are (in this section) reading   monthly history files there is just one time to access.

? In fact, we never see npp as a variable before line 1617 - will Fortran   really automatically declare a variable/pointer thing simply from a nf90   call?   I don't understand that, but it seems to work...

  The rest of the calc_clmC() function calculates the average and all that   stuff, which we shouldn't need since we are pushing the aggregation back   to the coupler, hopefully.

@ So: instead of all this, use iac_init_mct() to copy coupler linearized   [p] data into 3D [i,j,k] data in iaco, and pass that in to   iac2gcam_run_mod.F90.  Do not call call_clmC().  Restarts and non-coupled   runs handled in iac_init_mct() - read from appropriate file values, stuff   iaco, and there you go.

  Also, rename iaco so it's something I understand.

  Also, figure out what part of iac2gcam_run_mod.F90 needs to go, and what   part is doing some kind of actual calculation I need.

* c = pfts1d_cols(p) - see line 1530.  It looks like we take our "n" index   (i.e. [p]), calculate i,j, then scan up all columns to find the   cols1d_ixy(c) and cols1d_jxy(c) that match these (and also of lunit = 1 -   probably indicating we are on land.)

! Here is the thing: I really really want these mapping variables to be   static, so we can ready them directly from a clm.h1 file as a   configuration.  (See line 1203+ in iac2gcam_mod.F90 in clm_calC() for   declarations - allocatable, with a "save", which probably means   persistent in this function.  I think I want to make them local to the   iac_comp_mct.F90 file, so save shouldn't be important.)

  pfts1d_ixy(p)   pfts1d_jxy(p)   pfts1d_itype_veg(p)   pfts1d_itype_lunit(p)

  cols1d_ixy(c)   cols1d_jxy(c)   cols1d_itype_lunit(c)

  npfts   ncols

  nlat   nlon   lat(i)   lon(j)   pft(k)

  area   landfrac

XXXX   pfts1d_wtgcell  XXXX

* There's a month-based weighting of pfts1d_wtgcell that turns it   ultimately into pft_weight_mean_g at line 1711.  This suggests the   pfts1d_wtgcell is dynamic, and changes from sample to sample, so we need   to couple that, too.

* Here's the dimensionality of the 1d coupler based arrays, just for 

  npp(p)   pfts1d_wtgcell(p)

  hr(c)

? Huh.  So, how do I do the reverse mapping - [i,j,k] -> [p]?  *Do* I need   the reverse mapping?  Maybe the landuse that we send back is fine in its   linearized i+ni*j+ni*nj*k (or the reverse) format?

  If not, then we'll have to do what I suggest below somewhere - make a   one-D array that calculates x=i+ni*j+ni*nj*k, and fills with values such   that p = xyk_pfts1d[x].  Then loop over [i,j,k] to fill it before sending   back to coupler.

9/24/2019

* Okay, I'm trying to track down and initialize everything that we need -   here are my conclusions:

1 The gcami (or iaci, or whatever) pointer(:,:) input to iac2gcam_mod.F90   does nothing - it was a stub to include the coupler stuff.

2 Every other gcami/o/whatever set of pointers, which I have often renamed   something like glm2iac_var or whatever, *probably* does something   internally in the long chain of functions originally called by   iac_run_mod.F90.

3 Therefore, it seems logical to extract the coupler variables, put them in   iaci, and then modify iac2gcam_mod.F90 to use those, rather than crack   the clm .h1 history file and read it.

4 Outputs of everything EXCEPT the very last bits are important, but of   course we don't return a gcamo or anything either, since we didn't have   the coupler to deal with.  Logically, following the above, I should make   the final gcamo (and possibly the gcamoemiss somewhere in there) actually   hold the arrays to send back to the coupler.

5 This means tracking down and initializing all these internal gcami/o/foo   and iac/i/o/foo arrays.

? Open question: what is the dimensionality of these arrays?  The internal   ones I can assume are being handled correctly, but iac/i/o/foo should   probably be linearized using the clm-reduced coordinates like [p] and   [c].  But!  If it logically comes out of gcam in this 3D arrays, then   maybe the whole point of lnd_import_export.F90, or perhaps teh setup   inside iac_run_mod.F90, is to extract the 1D AVects into the ND   iaci/o/foo variables, and inport the other way.

9/23/2019

* Phone call with Peter Thornton where we worked through a few things about   coupling to the land model.

* We discussed the dimensionality of the p- and c- based variables in clm.   These "column" and "pft" variables are 1D flattening of [lat][lon] and   [lat][lon][pft], as I suspected.  But, np is not nlat*nlon*npft, which is   something like 900,000 for the test case I have from iESM.  Instead, it's   something like 200,000 - it's a reduced, combined index, where   combinations of lat,lon,pft that do not hold information are removed.   This is obviously for cases where there is no pft of a given index for a   given grid cell; I *think* it's also for cases where there is no land in   a given grid cell - this is why ncol != nlat*nlon.  Or maybe I do not   understand the column index.

  Anyway, what this means is that:

1 we should be able to send (e.g.) veg_cff%npp[p] through the coupler   directly, since it's already reduced to a 1D array and should fit nicely   into a AVect.  

2 We, however, will need a mapping of [p] -> [lat][lon][pft].  In other   words, give me a p, and out comes the lat, lon, and pft corresponding.   From there, I can reconstruct the full, possibly sparse   npp[lat][lon][pft] array needed by gcam.

3 It might be possible to send the mapping through the coupler itself!   Make AVects of lat[p], lon[p], pft[p], so they are right there.   It   *seems* like we only need to send this once, maybe upon some kind of   initialization?  Are their init-only AVects?  Maybe AVects that get   coupled only on the first run?  Once I have this mapping, I don't need to   keep reading it each step.

4 Peter seemed to think it was hard to generate this lat[p], lon[p], pft[p]   mapping, but it's got to exist somewhere.  I was hoping there was a   config file somewhere that contained this mapping - unless it's dynamic   or something, in which case coupling makes the most sense.

5 I *also* need the reverse mapping - how to take the landuse we get out of   glm, and convert it back into a [c] or [p] index that clm expects from   its z->l inputs.  I think if I have p->lat,lon,pft, I can derive the   reverse mapping.  Just do something like x=pft[p]+npft*lon[p]+npft*nlon*lat[p],   and build a x2p[x] array to take unique triplets of pft,lon,lat back to   index p.  Or a hash table, although I don't know how to do those in   fortran (and I'd have to build my own in C).  Does c++ have a hash table?

* So, that's the idea, then - figure out where the [p] and [c] mappings   come from, and either reimplement them in gcam or send them along via the   coupler.

* I still need to work up the "max of monthly mean" aggregator - maybe just   start with a yearly mean and go from there?

* Finally, I'm still stuck on initializing everything.  Using the other   components as a guide may not be helpful - there's a lot of really   complicated (and science-based, that I don't understand) stuff that's   going on behind the scenes that need to be set up.  I'm trying to keep   things simple, but worried about missing something important that I   should be concerned about.

* I also need to talk to Kate about what changes she's making to   iac2gcam_mod - she claimed she's rewriting that in C++, but that's the   main link between the coupled stuff (iac) and the gcam stuff, which means   if it changes then I need to know how.  

  There are also a bunch of gcam coupling functions that work with these   (:,:) 2D pointers - I think that's just a poor mans AVect, with   (field,generic_index) as the dimensions.  So for npp that the [p] index,   and then iac2gcam_mod will recompose back to lat,lon,pft - heck, it   probably already does that.  Time to reread that file for the fiftieth   time or something.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

9/20/2019

* Okay, the problem is that when we link with fortran we have to tell it to   use the -lstdc++ library, which would be included automatically by a c++   compiler.  So:

  GLIBS += -L/home/shippert/local/lib -lxerces-c -lxqilla -lstdc++

* There was also a problem building glm - there was a lot of redundant   source code in that directory.  ALSO, you should NOT use dots (.) in the   file name EXCEPT just before the extention.  The problem is that   'glm.future.c' confuses the build system - one thing (cmake, probably),   knows to compile into glm.future.o, but the source file stuff just sees   'glm.f' and then looks for 'glm.o'.

  This is probably a problem with the build tool and it should be smarter   than this, but for now renamed to glm_future.c.

  Also, of course, I had to find the right files to move to a "unused"   subdirectory - I looked at the exiting Makefile and saw which ones it   compiled, and kept those.  So glm_future.c is actual what was in   glm.future.iesm.c in the dist.

  From the glm Makefile:

  SOURCES=glm.future.iesm.c ccsm_glm_interface.c glm_comp_mod.F90

* Hooray! I've built the executable!

9/19/2019

* Man, just building is a pain in the butt.  

* Had to modify iac_build7/Tools/Makefile by hand to include a link to   xercesc and xqilla:

  # These are needed for GCAM - need to have a USE_GCAM kind of tag here. XXXXXXXXXXXXXX   See 9/20/2019 above   GLIBS += -L/home/shippert/local/lib -lxerces-c -lxqilla XXXXXXXXXXXXXX     ...then add $GLIBS to end of $(EXEC_SE) target.

* But!  Of course I built these libs by hand, probably without the same   environment, so I sourced ~/iac_build7/.env_mach_specific.sh, then   rebuilt xercesc and xqilla.

* But!  .../xerces-c-3.2.2/src/xercesc/util/XMLAbstractDoubleFloat.cpp   didn't have DBL_MIN defined, which is crazy, since it should be in   <cfloat>, which is included in float.h, which is #included in this file.   The best solution is to just do what float.h should do, which is put this:

  #define DBL_MIN __DBL_MIN___

  ...in the .cpp file.  Apparently the latter is defined in some antecedant   #include, whatever.  I can never find anything with this module system (I   can't find float.h, for example).

* Okay, so xerces-c built.  You have to include where xerces is explicitly   in the configure command for xqilla:

     ./configure --prefix=/home/shippert/local --with-xerces=`pwd`/../xerces-c-3.2.2/

  ...assuming both are in the same ~/local/src/ tree.

XXXXXXXXXXXXX   Hopefully, this is just because I forgot to actually install xqilla after remaking.

* Ah, crap, I'm still getting a linking error I don't understand, in the   ccsm_scenario_runner.cpp file.  Maybe I just don't include that   directory as part of the build?  Can I do that?  Otherwise, I need to   track down what the error is.

  ld: 	/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build7/bld/lib//libiac.a(ccsm_scenario_runner.o): 	undefined reference to symbol '_ZTISt8bad_cast@@GLIBCXX_3.4' /blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/gcc-8.1.0-dc4tau6/lib64/libstdc++.so.6: 	error adding symbols: DSO missing from command line

* Here is my LIBRARY_PATH:   LIBRARY_PATH=/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/netcdf-4.4.1-4odwn5a/lib:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/netcdf-fortran-4.4.4-kgp5hqm/lib:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/mvapich2-2.3.1-dtbb6xk/lib:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/intel-mkl-2018.4.274-jwaeshj/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.7:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/intel-mkl-2018.4.274-jwaeshj/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/intel-mkl-2018.4.274-jwaeshj/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64_lin:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/intel-18.0.4/intel-mkl-2018.4.274-jwaeshj/lib:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/gcc-8.1.0-dc4tau6/lib64:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/gcc-8.1.0-dc4tau6/lib:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/gcc-8.1.0/intel-18.0.4-443hhug/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.7:/blues/gpfs/software/centos7/spack/opt/spack/linux-centos7-x86_64/gcc-8.1.0/intel-18.0.4-443hhug/lib XXXXXXXXXXXXXXXXXX

9/12/2019

* Two big things:

1 Review lnd and rof import/export functions, to see how we grab stuff and   put stuff into the coupler.  In particular, how to deal with the   dimensions we are working with, and converting them into what we stuff   into an MCT avect.

* My working theory is that the x2z input array is 2D, with attributes of

  [particular field,linearized array of values]

  Thus, you do something like x2z(index_x2z_Sl_npp,:) to get all the 1D   values of the npp array in linearized form.  Then we reform that array   into the dimensions of lnd2iac_vars%npp(:,...) and there we go.

* If we are decomposing, then we grab x2z(index_x2z_Sl_npp,begg:endg) (g   for grid cell, c for column, p for pft) and put them into

  lnd2iac_vars%npp(begg:endg,...) 

* However! It may be that lnd2iac_vars%npp should stay linearized   (i.e. just 1D), since we have no decomposition.  Or, conversely, maybe we   need dimensions of %npp(lat,lon,pft), so we have to loop over the   x2z(index_x2z_Sl_npp) values and reform them as we go.  See how other   components deal with this, as well as what we expect from   iac2gcam_mod.F90.

2 iac_comp_mct: initialization, running - review counterparts in other   components, plus these notes.  If I want to go with the build, I may have   to stub all this out, but it seems like it's time to actually figure out   what I need, and work back and forth between the init function, the   iac_data_mod modules, and the run function, to make sure we figure out   everything we are working with and where it all goes.

9/6/2019

* I'm running into issues where I copied things from clm - the problem is   that I can't use modules from other components, especially for this   initial build, because there are no other components, I'm just building   iac by itself with stubs for everything else.

  In particular, I can't use the clm bounds decomp, or decompMod.  And I   *shouldn't* - all this stuff should be handled within the iac component   itself.  If I need that functionality, I need to create it myself, for   use by this component - perhaps copying over, but always customizing for   the needs my component has.

  This is all obvious, I'm just trying to remind myself of some of these   issues. 

* The specific thing that's keeping me from building now is listing these   iac2lnd and lnd2iac variables and types and what not.  My sense is that   they need to be listed *separately*, in the lnd component AND the iac   component - we don't use the same module for each.  This means, of   course, that lnd sends what it sends to the coupler, and iac grabs what   it can from the coupler, and if something is missing it needs to replace   it with a config file.  Same with every interaction - everythign is with   the coupler, regardless of what the other components do or say.

* Here's what to do if .../components/gcam/src is in a headless state,   which apparently happens a lot with submodules:

  from https://medium.com/north-arrow-research/git-tips-what-is-a-headless-state-daa9bc4e1d5a:

==========================

  If you have made commit(s)   First of all don’t beat yourself up. It happens. Here’s what you want to do:

  Make a new temporary branch: git checkout -b temp

  Now you can rebase this new branch on top of the integration branch:   git rebase dev 

  (or git rebase master, to rebase my temp branch onto master).

  Checkout dev and merge with temp. Now that you’ve rebased temp should be   stacked directly on top of dev so the merge should happen cleanly in a   fast-forward way and without a commit. 

  Last step is to delete your temporary branch: git branch -d temp    If that last step complains don’t force it. It means you haven’t done   something right and temp isn’t merged properly. 

===========================

  I need to review what to do if I've been modifying a bunch of code and   then discover I'm in a headless state.  If I check out master I lose all   my changes; if I commit I'm doing it to a different branch.

  Well, first I need to create a new branch for my changes - I think git   checkout -b temp makes a new branch with all my changes.  Then git rebase   master should make this new branch based off of master - then checkout   master and merge with temp, and now I've updated my master and should be   able to push back to the repository.

9/4/2019

* Okay, there's a problem running mkDepends on the GCAM source .cpp files.   The problem happens at line 268 of mkDepends, which seems to be an   attempt to recursively add in all the include files included in the   include files... 

  Anyway, it seems to be going into an infinite loop - probably a includes   b includes c includes a, or something like that.  I'm not certain how   this line does this, since it's a simple push:

    for ($i = 0; $i <= $#expand_incs; ++$i) { 	push @expand_incs, @{ $include_depends{$expand_incs[$i]} };     }

  So, $#expand_incs is just 2, so it should just try to push three (sets?)   of include files into this list.

! Ah. Every time we add something to $expand_incs, $#expand_incs itself   changes!  So, the first couple add a few files to include, so we want to   scan them too; so after the initial 2 include files, we now scanning the   first expanded include file from include file 0.

  So, here is my infinite loop: a includes b includes c includes a - a adds   b and c to the list, b adds ..., c adds a, scan a again.  Loop.  I see a   comment about preventing this kind of thing somewhere, and it tries to   rm_duplicates after all this is over, but we have the infinite loop right   there. 

  Okay, so, call rm_duplicates inside the loop - that way, if we've added   something we've already checked, we remove it.

  Well, not quite - rm_duplicates builds a null hash and then returns the   keys from the hash - but I don't think that keeps the order of things   correctly. 

* Okay, so I need to fix mkdepends.  Here's what I did - for each   prospective file to add to expand_incs, I grep to see if it's already   there, and only then push.  This should maintain the order, so the $i   loop works, and add only unique files to the list, and grep should work   pretty fast:

    # Expand     for ($i = 0; $i <= $#expand_incs; ++$i) { 	#push @expand_incs, @{ $include_depends{$expand_incs[$i]} };         # The above lead to an infinite loop with gcam, so grab each         # element and grep to see if it's already in the list.  Grep should         # be pretty fast for arrays of less than 1000, so I hope this isn't         # that big of a hit.         foreach $fff (@{ $include_depends{$expand_incs[$i]} }) { 	  unless (grep( /^$fff$/, @expand_incs ) ) { 	    push @expand_incs, $fff 	  } 	}     }

  Now we test it with the full Srcfiles, and see how it goes.

  Well, it only took 43 seconds to finish, so that's good.  A lot less than   that the second time through.  So, let's pretend that worked.

* You know, a hash would probably be a faster check than grepping the   building array list each time:

    #initialize expand_hash     my %expand_hash = ();     foreach $fff (@expand_incs) {        $expand_hash{$fff}=1;     }       

    # expand     for ($i = 0; $i <= $#expand_incs; ++$i) {         foreach $fff (@{ $include_depends{$expand_incs[$i]} }) { 	  unless ($expand_hash{$fff}) { # not already expanded 	    push @expand_incs, $fff; 	    $expand_hash{$fff}=1; 	  } 	}     }

  

9/3/2019

* I've been having a lot of problems getting the various gcam modules to   actually generate .mod files.  Here is what I think is happening:

1 In the .../gcam/src/iac/coupling directory, we build in lexical order -   so glm2iac_mod.F90 before iac_fields_mod.F90. 2 But! glc2iac_mod.F90 has a "use iac_fields_mod" line 3 So glc2iac_mod.F90 fails, saying it can't load the module iac_fields_mod,   because the iac_field_mod.mod file hasn't been created.

* So, my working theory is that somewhere in case.setup or case.build,   there's logic to look for module files to build first (plus, you have to   build them in the right order, since you can use modules in other   modules).   XXXXXXXXXXX   I *believe* that it does this by looking for *Mod.F90, since that's the   way most module files in clm are listed, and the ones not named like that   give problems in .../gcam/src/iac/coupling - mksurfdat.F90,   iac_fields_mod.F90, etc.

  So, I'm going to rename anything with a module declaration *Mod.F90, do a   full case setup (to iac_build7) and case build, and see where that gets   us. 

  Here's how I find them:

--------------

[shippert@blueslogin4 coupling]$ egrep -i module *.F90 | grep -v '\!' | grep -v 'Mod.F90' gcam2emisfile_mod.F90:Module gcam2emisfile_mod gcam2emisfile_mod.F90:end module gcam2emisfile_mod gcam2glm_mod.F90:Module gcam2glm_mod gcam2glm_mod.F90:end module gcam2glm_mod gcam_var_mod.F90:module gcam_var gcam_var_mod.F90:end module glm2iac_mod.F90:Module glm2iac_mod glm2iac_mod.F90:end module glm2iac_mod iac2gcam_mod.F90:Module iac2gcam_mod iac2gcam_mod.F90:end module iac2gcam_mod iac_fields_mod.F90:Module iac_fields_mod iac_fields_mod.F90:end module iac_fields_mod mkabortutils.F90:module mkabortutils mkabortutils.F90:end module mkabortutils mkfileutils.F90:module mkfileutils mkfileutils.F90:end module mkfileutils mkncdio.F90:module mkncdio mkncdio.F90:     module procedure ncd_ioglobal_int_var mkncdio.F90:     module procedure ncd_ioglobal_real_var mkncdio.F90:     module procedure ncd_ioglobal_int_1d mkncdio.F90:     module procedure ncd_ioglobal_real_1d mkncdio.F90:     module procedure ncd_ioglobal_int_2d mkncdio.F90:     module procedure ncd_ioglobal_real_2d mkncdio.F90:end module mkncdio mksurfdat.F90:module mksurfdat mksurfdat.F90:end module mksurfdat mkvarctl.F90:module mkvarctl mkvarctl.F90:end module mkvarctl mkvarpar.F90:module mkvarpar mkvarpar.F90:end module mkvarpar

-------------- XXXXXXXXXXXXXXXXXXXX

* Okay, forget all that, it doesn't make any difference what the files are   called.  My new conjecture is that the original "make Depends" error I   get while doing a case.build (rather than a case.build -b iac) is key -   somehow we need to create a Depends file that says the order we need to   compile things in, including modules.  And, somehow, I have not yet   generated the information for that to happen yet.  So, review the   Makefile and mkDepends files in the iac_build7/Tools directory, to see if   I can figure out what is happening here.

8/6/2019

* This environment seems to work to compile with boost, using the raw   includes in /home/shippert/local/boost_1_70_0/boost

  Currently Loaded Modules:     1) intel/18.0.4-443hhug           3) mvapich2/2.3.1-verbs-dtbb6xk   5) netcdf/4.4.1-4odwn5a     2) intel-mkl/2018.4.274-jwaeshj   4) netcdf-fortran/4.4.4-kgp5hqm   6) cmake/3.13.4-354d6wl

* I need to catch up on my notes here, but it's been frustrating as I don't   really understand what I'm seeing.  But in a nutshell the default anvil   environment doesn't work with boost - it barfs on the std::complex   declarations, even with -std=c++11 compile option, which is what some   googling suggested.  You can see the default environment in   software_environment.txt, as per 7/15/2019:

  Currently Loaded Modules:     1) intel/17.0.4-74uvhji           3) netcdf/4.4.1-magkugi           5) mvapich2/2.3.1-verbs-m6cblfk     2) intel-mkl/2017.3.196-v7uuj6z   4) netcdf-fortran/4.4.4-7obsouy   6) cmake/3.14.1-n2wvesr

* So, now I'm going to try to load the newer versions in   config_machines.xml, but in the past that hasn't worked for reasons I   don't understand.  But, once more...

8/2/2019

  /blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/intel-17.0.4-74uvhji/include/complex

7/15/2019

* So, issues with the build.  First, here is the software environment and   the error message I get from building gcam:

============================ software_environment.txt  Currently Loaded Modules:   1) intel/17.0.4-74uvhji           3) netcdf/4.4.1-magkugi           5) mvapich2/2.2-verbs-lxc4y7i   2) intel-mkl/2017.3.196-v7uuj6z   4) netcdf-fortran/4.4.4-7obsouy   6) cmake/3.14.1-n2wvesr

============================ 

---------------------------- iac.bldlog.flarp (copied over from the build directory) ... mpicxx  -c -I. -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/mct/noesmf/c1a1l1i1o1r1g1w1i1e1/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-4.4.1-magkugi/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-fortran-4.4.4-7obsouy/include -I/blues/gpfs/home/software/climate/pnetcdf/1.6.1/intel-17.0.4/mvapich2-2.2-verbs/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I. -I/blues/gpfs/home/shippert/iac_build2/SourceMods/src.gcam -I/soft/xerces/c-3.2.1/include -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/logger/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/database/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/base/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/curves/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/sectors/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/reporting/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/land_allocator/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/investment/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/consumers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccarbon_model/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/policy/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/agLU/fortran_source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/target_finder/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/demographics/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/marketplace/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/containers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/technologies/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/climate/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/util/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/solvers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/emissions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccsmcpl/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/coupling -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/cpl -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lib/include -std=c++11 -fp-model source   -O2  -O2 -fp-model precise -std=gnu99   -O2 -debug minimal   -DLINUX  -DFORTRANUNDERSCORE -DNO_R16 -DCPRINTEL -DNDEBUG -DMCT_INTERFACE -DHAVE_MPI -DPIO1 -DHAVE_SLASHPROC -D_PNETCDF  /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp ... In file included from /soft/xerces/c-3.2.1/include/xercesc/util/XercesDefs.hpp(46),                  from /soft/xerces/c-3.2.1/include/xercesc/dom/DOMNode.hpp(25),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/iinput.h(47),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/ifunction.h(48),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/ademand_function.h(54),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp(48): /soft/xerces/c-3.2.1/include/xercesc/util/Xerces_autoconf_config.hpp(122): error: identifier "char16_t" is undefined   typedef XERCES_XMLCH_T				XMLCh;           ^

... compilation aborted for /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp (code 2) --------------------------

* There are other things that fail in the ... lines, but I think that's   because of the parallel build processing - and they all seem the same,   not having char16_t defined.  There are *other* different build failures   as well later on, but lets deal with this one first.

* To simulate this, I put this load in this sequence in my .bashrc, and   went to ~/iac_build2 and tried to compile the mpicxx line myself:

====================== module load gcc/7.1.0-4bgguyp module load icu4c/58.2-rv2fe73 module load xerces/c-3.2.1 module load intel/17.0.4-74uvhji module load intel-mkl/2017.3.196-v7uuj6z module load netcdf module load netcdf-fortran module load mvapich2 module load boost module load cmake =======================

-------------------------

mpicxx  -c -I. -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/mct/noesmf/c1a1l1i1o1r1g1w1i1e1/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-4.4.1-magkugi/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-fortran-4.4.4-7obsouy/include -I/blues/gpfs/home/software/climate/pnetcdf/1.6.1/intel-17.0.4/mvapich2-2.2-verbs/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I. -I/blues/gpfs/home/shippert/iac_build2/SourceMods/src.gcam -I/soft/xerces/c-3.2.1/include -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/logger/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/database/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/base/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/curves/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/sectors/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/reporting/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/land_allocator/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/investment/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/consumers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccarbon_model/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/policy/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/agLU/fortran_source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/target_finder/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/demographics/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/marketplace/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/containers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/technologies/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/climate/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/util/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/solvers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/emissions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccsmcpl/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/coupling -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/cpl -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lib/include -std=c++11 -fp-model source   -O2  -O2 -fp-model precise -std=gnu99   -O2 -debug minimal   -DLINUX  -DFORTRANUNDERSCORE -DNO_R16 -DCPRINTEL -DNDEBUG -DMCT_INTERFACE -DHAVE_MPI -DPIO1 -DHAVE_SLASHPROC -D_PNETCDF  /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp In file included from /soft/xerces/c-3.2.1/include/xercesc/util/XercesDefs.hpp(46),                  from /soft/xerces/c-3.2.1/include/xercesc/dom/DOMNode.hpp(25),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/iinput.h(47),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/ifunction.h(48),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/include/ademand_function.h(54),                  from /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp(48): /soft/xerces/c-3.2.1/include/xercesc/util/Xerces_autoconf_config.hpp(122): error: identifier "char16_t" is undefined   typedef XERCES_XMLCH_T				XMLCh;           ^

compilation aborted for /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp (code 2) --------------------

* So, same errors.  But, if you then reload gcc and icu4c, and remove the

    -fp-model source  -fp-model precise -debug minimal

  ...options from mpicc, it will build.  I assume the above CLO are   specific to the intel compiler, which is replaced by gcc by the load:

=========================

shippert@blueslogin1 iac_build2]$ module load gcc/7.1.0-4bgguyp icu4c/58.2-rv2fe73

Lmod is automatically replacing "intel/17.0.4-74uvhji" with "gcc/7.1.0-4bgguyp".


Inactive Modules:   1) intel-mkl/2017.3.196-v7uuj6z     2) netcdf-fortran

Activating Modules:   1) icu4c/58.2-rv2fe73

The following have been reloaded with a version change:   1) boost/1.63.0-m64qkx4 => boost/1.66.0-yezpm5j     4) mvapich2/2.3b-gk6kdue => mvapich2/2.3a-avvw4kp   2) cmake/3.13.4-354d6wl => cmake/3.14.4-kmpms6b     5) netcdf/4.4.1.1-prsuusl => netcdf/4.6.1-6z2nuae   3) git/2.13.0 => git/2.19.1-gy6rfgf

[shippert@blueslogin1 iac_build2]$ mpicxx  -c -I. -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/mct/noesmf/c1a1l1i1o1r1g1w1i1e1/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-4.4.1-magkugi/include -I/blues/gpfs/software/centos7/spack-0.12.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/netcdf-fortran-4.4.4-7obsouy/include -I/blues/gpfs/home/software/climate/pnetcdf/1.6.1/intel-17.0.4/mvapich2-2.2-verbs/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/intel/mvapich/nodebug/nothreads/mct/include -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lnd/obj -I. -I/blues/gpfs/home/shippert/iac_build2/SourceMods/src.gcam -I/soft/xerces/c-3.2.1/include -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/logger/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/database/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/base/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/util/curves/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/sectors/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/reporting/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/land_allocator/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/investment/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/consumers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccarbon_model/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/policy/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/agLU/fortran_source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/target_finder/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/demographics/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/resources/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/marketplace/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/containers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/technologies/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/climate/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/util/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/solution/solvers/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/emissions/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/ccsmcpl/source -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/coupling -I/blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/cpl -I/lcrc/group/acme/shippert/acme_scratch/anvil/iac_build2/bld/lib/include -std=c++11   -O2  -O2 -std=gnu99   -O2   -DLINUX  -DFORTRANUNDERSCORE -DNO_R16 -DCPRINTEL -DNDEBUG -DMCT_INTERFACE -DHAVE_MPI -DPIO1 -DHAVE_SLASHPROC -D_PNETCDF  /blues/gpfs/home/shippert/E3SM_active_gcam_coupled/components/gcam/src/iac/gcam/cvs/objects/functions/source/ademand_function.cpp cc1plus: warning: command line option ‘-std=gnu99’ is valid for C/ObjC but not for C++ [shippert@blueslogin1 iac_build2]$ 

=============

* Okay, that build line is huge and messing things up - but the issue is   that reloading gcc makes it compile, minus some clos.  I *think* the   issue is in mvapich2 - if you replace it with openmpi...

-------------- [shippert@blueslogin3 ~]$ module load intel-mpi/2017.3-dfphq6k

Lmod is automatically replacing "mvapich2/2.3b-gk6kdue" with "intel-mpi/2017.3-dfphq6k".


The following have been reloaded with a version change:   1) boost/1.63.0-m64qkx4 => boost/1.65.1-pka2h73     2) netcdf/4.4.1.1-prsuusl => netcdf/4.6.1-c2mecde --------------

  ...then run the mpicxx line, it works.  Although, if you throw back in   the -fp-model options it still crashes, with the g++ (gcc) errors:

============== g++: error: source: No such file or directory g++: error: precise: No such file or directory g++: error: minimal: No such file or directory g++: error: unrecognized command line option ‘-fp-model’ g++: error: unrecognized command line option ‘-fp-model’ ==============

? So, does replacing "mvapich2/2.3b-gk6kdue" with   "intel-mpi/2017.3-dfphq6k" mean we are now compiling with gcc?  Or does   openmpi mean the intel compiler preprocessess with g++?

@ I need to update how I got here - building the GCAM code itself.  I've   been in the soup and not updating my notes as much as I should.

* First, buildlib put all the different gcam source code and include file   directories into the tmp_directories file - then, the gmake will recurse   down them, compile anything it knows how to compile (source code,   including fortran and c and c++ files), and includes them in the -I   compile line.  This works to compile a lot of the actual gcam code,   although of course I haven't gotten to the link part yet.

  .../cime/components/gcam/cime_config/buildlib

* It fails to compile the above files as well as some others, because the   xerces include files are incompatible with these particular build   options.  It appears that xerces was built with gcc, because to load the   xerces package you have to load gcc and icu4c - it won't do it with just   the intel and mvapich2.  I've tried to get around this by loading gcc and   icu4c *first*, then loading xerces, then loading the intel/mvapich2   stuff, all of which doesn't pitch an error and bomb out or anything, but   doesn't really solve the problem either, as indicated above.

* The cpp files are compile via mpicxx, which is a shell script setting up   the envrironment and who knows what else - but it seems to change   depending upon which packages are loaded (in particular, mvapich2   vs. openmpi, and maybe intel vs. gcc).  Just one more piece of   obfuscation - scripts calling scripts depending on packages depending on   config files depending on makefiles depending on command line options   depending on scripts.  The archeology on this is literally, physically   destroying my brain and will to live.

* Because I couldn't figure out (yet) how to load the packages inside the   build script (I need the .xml file that generates env_mach_specific.xml   in the build directory), I got this far by including the xerces include   directory (/soft/xerces/c-3.2.1/include) in the tmp_filepath directory -   see buildlib in gcam/cime_config.  This at least put it in the long list   of -I... options to mpicxx, which allowed us to get to the compile error   we are seeing.

* So, for now here's the plan:

@ Try to build with gcc a  find which .xml file and/or command line option tells you how to build   stuff, modify that, do a new case.setup into iac_build3. @ Try to figure out how to load boost (which at least was build with   intel), and the xerces stuff, although the latter won't work, of course.   I don't know if we'll be able to link with boost when compiling with gcc,   but maybe.

@ Try to build xerces myself as part of the gcam build - probably put it in   a utils directory somewhere and include that in buildlib.

* Alright, this is what I did - from cime/scripts:

  ./create_newcase --compiler gnu --case ~/iac_build.gnu --compset Z --res f19_g16

  That seems to have worked.  Now I modify env_mach_specific.xml to load   xerces, do a case.setup, and finally a case.build -b iac and see what   happens. 

* Of course that did not work - the thing barfs on the case.setup, saying   it can't find the modules I want to load by adding or changing   .../cime/config/e3sm/machines/config_machines.xml.  I *can* load the   modules in my shell with a straight module load, but instead of doing   that for some reason the loading happens via this monstrosity:

========   export MODULEPATH=/blues/gpfs/software/centos7/spack-0.12.1/share/spack/lmod/linux-centos7-x86_64/Core;/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/libexec/lmod python load gcc/7.1.0-4bgguyp intel-mkl/2018.1.163-4okndez hdf5/1.10.5-uhezugo netcdf/4.6.1-6z2nuae netcdf-cxx/4.2-mjgpj4k icu4c/58.2-rv2fe73 xerces/c-3.2.1 mvapich2/2.2-pu237k7

========

  If you look close, you can see that it's calling   .../lmod/lmod/libexec/lmod somewhere inside the lmod module.  I've looked   at that, and, of course, it's as obtuse as every other single piece of   code or script or package I've looked at in this godforsaken project.  I   don't know what lmod is doing, but it might? use some kind of cache or   list of approved packages or some other such nonsense, and my attempts to   figure out how to make it not do that do not work.

* You'll also notice that because xerces was build with gcc/7.1.0 I had to   modify the load commands for all the other packages to make them   compatible with that version of gcc.  Thus, it simply cannot find ANY of   these packages, because ALL of them are either out of cache or otherwise   non-approved and/or not available.

! So.  I simply cannot load the xerces package as it stands.  There is   LITERALLY NO WAY to make it go.  You CANNOT configure ANYTHING to make it   try to build with the modules and versions it needs to build with this   thing.  JUST GIVE UP.  It is PHYSICALLY AND METAPHYSICALLY IMPOSSIBLE.   You are a FOOL to keep trying in the face of such INSURMOUNTABLE   FUNDAMENTAL OBSTACLES.

* So, I'ma gonna have to build my own version of xerces.  Yay.

6/5/2019

* Some delays as I got pulled into ARM ASR stuff

* Mostly ready with build-namelist, trying to get gcam stuff to build.

* We have boost and xerces libraries on anvil, which Kate says are the ones   we need to build gcam proper - to find versions and info, do:

     modules spider boost      modules spider xerces

  There are some prereqs (i.e. intel compiler) that you have to load first,   and actually I need to learn how to load new packages during the build   process - that has to be hidden somewhere in the buildlib stuff, or   maybe it's right there and I missed it.

* We do build with intel on anvil, so the intel builds of these libraries   will work.  Kate says there's something funky about how they piggyback   off gcc to find the right library or do precompiler stuff or something,   so I need to figure that out, too.

5/28/2019

* So, I modified config_files.xml to have an "iac" section in the   COMPSETS_SPEC_FILE; I'm not convinced that is either necessary or   sufficient, but at least it will let me move forward.  I also need to (at   the very least) add a "gcam" section to CONFIG_IAC_FILE area.  So now I   need to create/modify:

  components/gcam/cime_config/config_component.xml

  ...so it has what it needs.  So, time to do what I always do, look at the   lnd and rof versions and see what they got.

  So, it looks like configuration build options go in here, ways for the   build system to associate various actions and build variables and options   with compset variables.  So, if you go something like:

  <entry id="PANTS_MODE">   ...     <value compset="_GCAM%PANTS">ON</value>   </entry>

  ...that's a way to associate the GCAM%PANTS compset tag with setting   PANTS_MODE.  It looks like a regex, with wildcarding like:

    <value compset="_CAM.*_DOCN">ACTIVE</value>      <desc compset="_CLM50%[^_]*BGC" >clm5.0 bgc (cn and methane):</desc>

  ...and stuff like that.

* It looks like the <file> tells you which env file you should put these   options in, which, apparently, is where everything will look while   building your case.  Maybe the scripts convert to environment variables?   I mean, that would make "env" make sense.

  Config files to create config files - this thing is a maze, and I'm   feeling more and more like a rat who is not going to get the cheese.

  Okay, so we dump things like COMP_ROF and COMP_IAC into env_case.xml, and   stuff like "MOSART_MODE" and "CLM_CONFIG_OPTS" go in env_build.xml, and   then I guess command line options to the model execuatables would go in   env_run.xml

* In other words, I need to set the rest of everything up (the case setup,   the build, the run commands) before I know what kinds of configuration @ options go in here.  So, up next is my special iac config_compsets.xml   file, which, if it doesn't work, at least lets me build the section to   stuff into the clm compsets section or whatever.

  Okay, how about something simple:

  <compset>     <alias>ZLND</alias>     <lname>2000_SATM_CLM45_SICE_SOCN_SROF_SGLC_SWAV_GCAM</lname>   </compset>

  It looks like the all the compsets in the clm config_compsets.xml set   MOSART, but you see others in the cam config_compsets.xml that have clm   plus SROF, so let's put it in this way.  I just want to build both land   and GCAM, somehow - we'll worry about how things go later.  

  Maybe a more straightforward

  <compset>     <alias>Z</alias>     <lname>2000_SATM_SLND_SICE_SOCN_SROF_SGLC_SWAV_GCAM</lname>   </compset>

  ...everything stub but GCAM.  I doubt this would actually work - I'd have   to include reading the climatological .h1 file for land inputs in case of   a stub, which is a thing I don't know how to do, but I'm sure it works.

  But I'm going to start with clm and gcam together, which will help me   track down build issues, and later coupling issues.

  ...eh, maybe not - just try to get GCAM built, to test buildnml and   buildlib?  I told Kate that was a zero-level build.

! Okay, created Z (all stubs except GCAM) and ZLND (add CLM45) compsets,   and tried this:

    ./create_newcase --case iac_build --compset Z --res f19_g16

================   I moved the results into ~/iac_build, since it built it off the scripts   directory.  First line in the output of create_newcase (see newcase.out)   is:

  Did not find an alias or longname compset match for Z 

  So, instead it did this:

    Z_SATM_SLND_SICE_SOCN_SROF_SGLC_SWAV_SIAC_SESP 

  Okay, fun.  Maybe force the component part?

    ./create_newcase --case ~/iac_build2 --component iac --compset Z --res   f19_g16

  No, can't specify the component.  Okay, fine, put the Z cases in the   allactive config_compsets and hope for the best.

  Well, it *also* might help if I put the right directory name (gcam, not   iac) in COMPSETS_SPEC_FILE...

  okay, there you go. ================

* So, ~/iac_build3 and ~/iac_lnd_couple_build contain the Z and ZLND cases,   with newcase.out containing the build issues.  There were no errors, so   it created the whole build structure, which is nice.

* Okay, case.setup in iac_build3 - barfs when it can't find buildnml for   gcam.  So touch a buildnml and buildlib, chmod +x, and try again...

  So, the fake buildnml worked for case.setup, but case.build fails because   there's no way to create libiac.a without a buildlib that actually does   something.

@ So, the next steps are making a real buildnml and a real buildlib.

5/24/2019

* It is now time to work off: bishtgautam/gcam/add-submodule-2, which   hopefully is based off the latest E3SM with the CIME upgrades.  My plan   is:

1 checkout bishtgautam/gcam/add-submodule-2 2 create new branch shippert/gcam/active-gcam-coupled 3 Merge over from active-gcam, and commit everything. 4 Work in active-gcam-coupled from now on.

* Okay, "merging" is not right - a better solution was to go to   E3SM_active_gcam_coupled/components/gcam/src, which is the head of the   submodule, and do a git pull origin master to update my submodule.  That   should be most of the things that are different between my original   active_gcam and the active_gcam_coupled branch.

* Now do a diff -rq E3SM_active_gcam E3SM_active_gcam_coupled, to see what   files are different, and full diff -r to find the actual differences.  I   think any changes to CIME files should probably use the new branch, as   they've modified some of my original implementation.  But anything iac   specific, like the prep and build stuff, I just copy over from   E3SM_active_gcam.  Then, finally, commit and push everything, and go from   there. 

* Okay, *that's* not going to work, either - we have 373 non-git files that   are different between the two branches.  So there has been a lot of   regular E3SM modifications since then.  Maybe just grep for things with   iac in the name?

  ...better, only five differences, and three of them are garbage files (~   and an "unused").  So:

    Only in E3SM_active_gcam/components/clm/src/main: lnd2iacMod.F90     Files E3SM_active_gcam/components/gcam/src/cpl/iac_import_export.F90 and E3SM_active_gcam_coupled/components/gcam/src/cpl/iac_import_export.F90 differ

  That's fine, I should copy those over directly anyway -   iac_import_export.F90 is still under review about how to extract the lnd   imports (I think I don't need to loop over the decomp, as the mapping   should put it on iac decomp), and lnd2iacMod.F90 is obviously still a   work in progress.

  Also, I should move this NOTES.e3sm file over to the new branch, too.

* Once again, here is the CIME documentation:

  https://esmci.github.io/cime/users_guide   https://esmci.github.io/cime/users_guide/introduction-and-overview.html

* From that, it looks like I need to define a compset with case string.  I   want to say this should happen in some of the .xml files in the gcam   cime_config directory - perhaps this users_guide will tell me how to   create a new compset.  

  Once I have that, then, I do something like this:

  cd cime/scripts   ./create_newcase --case iac_build --compset Z --res f19_g16

  ...or something simlar.  Perhaps I do not need teh compset name?  Sure,   let's try just setting the case to iac_foo...nope, you are required to   have --compset and --res. 

  Okay - well, the good news is I don't think compset Z has been taken, so   I'll start by building that.  

* Eh, it looks like the compset config files are defined in:   cime/config/e3sm/config_files.xml in the COMPSET_SPEC_FILE section.  Man,   I just don't know, do I modify this to let me find a config_compsets.xml   in gcam/src/cime_config, or do I add IAC components to the clm config   file.  I'd like to do as little damage as possible to the code base, but   either I'm mucking with cime or mucking with clm.

  I think the cime mods are easier, letting me screw up my case   configurations all in my own area without running the risk of mucking up   another component.  So, I think the solution is to modify CIME to look   for iac stuff for these configs.

5/23/2019

* Here is more cpl7 documentation, to help me track down what I need to do   to build:

  http://www.cesm.ucar.edu/models/cesm1.2/cpl7/

  I've printed out the user's guid and coupler flow diagram, which, yowza.

* As it turns out, the CIME driver buildnml is the one that apparently   takes namelist options (or something) and builds the seq_maps.rc file,   which is a namelist-ish format describing how all the mappings will   work.  So, I just need to figure out how to describe the coupling I'm   doing in the right namelists and that part should be taken care of.

  The other part of the build is tracking down all the makefiles we used in   iESM, and putting them in the right spot so that buildlib works.  Jeesh,   this is gonna be a long day putting this all together.

  I should review some of the builds I've made for various test runs and   see if I can identify what everything is, what it is doing, and how it   got to be there.

* Okay, so check out:

  ~/anvil/E3SM_active_gcam/cime/doc/source/users_guide/

  ...and the ".rst" files, which is some kind of formatted documentation   that both looks good and looks bad in emacs (each sentence is on a   different line.  Maybe there's a ReST-viewing mode?)

5/22/2019

* Okay, I finally internalized a couple things while rereading for the Nth   time the MCT papers:

1 "Merging" means taking two or more inputs and combining them onto the   grid/decomposition of the output.  This is where the "fractions" stuff   come in - atm takes lnd, ocean, and ice fluxes as input, so a "merge"   means meaningfully combining the same fluxes from different inputs onto   the boundary.  If a grid cell has two or more different surface types,   then you need to weight by the fraction of that grid cell that has that   type: i.e. atm_flux = land_flux*land_frac + ice_flux*ice_frac +   ocean_flux*ocean_frac, for each grid cell.

  Lnd takes rof and glc as input, so we need to merge those.  The other   components that take multiple inputs through the coupler need merging as   well.

  The state variables also need weighting - surface temp in the grid cell   would still average across land types, for example.

! This means that IAC does NOT need any merging - we are taking just a   single component (lnd) as input.  It's likely the atm Co2 flux will   need to merge *in* from iac as well as lnd.  I don't know about land,   though - we are cogridded, and I don't think we provide inputs that   other models do, but I'll have to think about that.

2 "Mapping" means "interpolating" - i.e. putting my stuff on your grid.   There is a whole calling sequence to do such mapping, and I'm not   convinced we can just skip it, even if we are on the same grid as lnd,   but MCT handles a lot of it.  It's mostly, from our perspective, a matter   of setting it up right.

? An interesting thought here - rather than have the glm grid interpolated   by gcam/iac onto the lnd grid, maybe we could use MCT to do that?  It   might be easier, or it might be harder.

3 But I *still* don't know how we go from one comp's decomposition to   another - in particular, take our lnd->iac coupled AVects from the lnd   ninst_lnd decomposition to iac's ninst_lnd=1 degenerate case.  I expect   to use something like like: loop over input instance, set   output%rAtt(:,n)=input%rAtt(:,i), where i=1,lsize_input for that   instance, and keep n counting over the whole instance loop.  This   would combine all the decomposed variables in lnd into a single vector   for iac.  But I haven't seen anything similar so far - taking M input instances   and mapping them to N output instances.  It may be that that never   happens - we always have 4 natm and 4 lnd and 4 glc etc, so there is   always this one-to-one decomposition mapping.  But that won't work for   iac, so, hmm.

  I mean, I just do it my way and see if it works - it's straightforward   enough, just do something like this:

    n=0     do eli = 1,num_inst_lnd        l2x_lx => component_get_c2x_cx(lnd(eli))        lsize=...        x2z_z%rAttr(index_x2z_Sl_npp,n:n+lsize) = l2x_lx%rAttr(index_l2x_Sl_npp,:)        ...        n=n+lsize     enddo  

  Is there any reason why this wouldn't work?

! I *think* this kind of thing (domain mapping) comes under the   globalsegmap stuff, which I never really understood.

* On our weekly phone call, I had trouble expressing exactly what my issues were,   because I don't understand them well enough to not sound like maroon -   but from my conversation with GB I think the idea is that "mapping" stuff   is both a grid and domain mapping.  In other words, during   initialization, the mapping functions describe how to send stuff to and   from components, including interpolating onto a different grid and   redecomposing for a different decomposition.  So that means I *do* need   to set up some kind of mapper_Sl2z, using a seq_maps.rc file and the call   to seq_map_init_rcfile().  

  There are options for saying "samegrid_xy", so something like   "samegrid_lz = .true." should tell it it doesn't have to interpolate.  

  My problem is - npp is domain split by pft/patch (begp:endp), whereas hr is   domain split by column (begc:endc).  Do I need separate maps for each   one?  Or is there a way of turning (begp:endp) into (begc:endc,<other   dims>) or (begg:endg, <other dims>)?

* Anyway, check this out:

  ./cime/doc/source/driver_cpl/namelist-overview.rst

  I found that by grepping for seq_maps.rc, which is a thing that we call   in every seq_map_init_rcfile() function I've seen, that has some way of   describing the mapping.  Maybe that will clarify things, as well as   everything in the cime/doc subdir.         5/16/2019

* Sadly, there is no mct_aVect_max function, so my attempt to quickly use   MCT to do the accum->monthly avg->max of monthly averages is going to be   a little more complicated.  I can accum until the month break, then call   average.  I could copy the mct_aVect_avg code and simply replace with a   max instead.

* So, the idea is I create a "monthly" aVect, and accumulate the l->z   inputs into that.  Then, on the month break (can I access dates?  I   should be able to), I average that Avect, then accumulate *that* into a   third "monthly accum" aVect.  Then, on the yearly break, I finally run my   custom "find a max" code.

  l2z_lx -> l2zacc_lx -> l2zmonth_lx -> l2zmax_lx

* It looks like "accumulators" do this: extract the entire land coupled   variables, via l2x_lx => component_get_c2x_cx(lnd(eli)), where eli runs   oer the number of land instances - thus, this grabs the decomposed   coupled aVectors on the land grid.

  Then, it does an mct_avect_copy/accum(l2x_lx, l2acc_lx(eli) - I'm pretty   sure this will only copy over and accumulate the exact aVects that are   defined the in l2acc_lx aVect list, as well.  So once we initialize the   correct l2acc_lx list, we don't have to worry about tracking down which   exact fields to copy over.

* I don't know what aVect "merging" is - I want to say it's pulling all the   info in from the domain decomposition and merging them into one big   array, but, honestly, I don't see anything that looks like that in either   the rof merge or the glc merge in prep_rof/glc_mod.F90.

  I don't think merging means pulling in inputs from multiple components -   I think it's more about pulling in from multiple decompositions.  I know   glc is on the same domain decomp as lnd - what about rof?  I don't know -   it seems like I have lots of different things on lnd and rof decomp that   I don't understand.

* Okay, let's go into prep_rof_init(): first thing, calls 

     x2r_rx => component_get_x2c_cx(rof(1))

  ...so it pulls out the attribute vector from the first component_type   rof, where rof(num_inst_rof) - basically, it tells you extract x2r_rx   from the first instance of rof.  I'm not sure, but the naming suggests   this is the stuff we are pulling *into* rof, from other components "x"   that pushed it "out" to the coupler first.  so x2c->c2r = x2r.  I guess.

  Anyway, this particular x2r_rx instance is just to find if the   irrig_flux_field is defined, and get the lsize_r.  (lsize_r) is "local   size", so my guess that's the size of the arrays in this particular   instance.  

! Maybe that's what the merge does - string them back together?   If you have a variable like this foo[a][g], and you decompose on [g],   that means putting blocks of size n*a into each instance.  Then, with the   merge, you simply tack them back on top of each otehr - run down the   instances and append them back together.

* Anyway, then we do this:

     l2x_lx => component_get_c2x_cx(lnd(1))      lsize_l = mct_aVect_lsize(l2x_lx)

  So, that's to find the c2x - what the land send out to the coupler on   it's decomposition, and find that first local size.  That that point we   allocate our l2racc_lx accumulator avect by the number of *land*   components, and then 

       do eli = 1,num_inst_lnd           call mct_aVect_initSharedFields(l2x_lx, x2r_rx, l2racc_lx(eli), lsize=lsize_l)           call mct_aVect_zero(l2racc_lx(eli))        end do

  This takes two input Avects and inializes a third: so, finds the shared   fields between these two - the intersection of all the stuff that land   sends out in l2x_lx and just the stuff that rof wants in x2r_rx, and sets   up l2racc_lx to have those.

  We *then* allocate l2r_rx, which I think is what our final   accum/averaged/merged input.

  There's a "samegrid_lr"  check, and then a mapper_Fl2r setup.  I *think*   that stuff is for when we have to send fluxes across a boundary, which   I'm pretty sure we do *not* have to do here.  (Not for l2z input - the   z2a output is a flux.  z2l is landuse, so that should be a state vector,   as well).

* Hmm, for z2a mapping, can we just use mapper_Fl2a?  Or do we need a   domain decomposition, as well...

* Now, on to prep_rof_accum() - for each eli (land instance) call copy or   accum from component_get_c2x_cx(lnd(eli)) to l2racc_lx(eli).  That makes   sense - the stuff we pulled out of lnd, accum into l2racc.  \

* prep_rof_accum_avg() - Now, this is a little weird.  It runs over the   *rof* instances in eri, and calculates a corresponding eli (land   instancE) via: 

?   eli = mod((eri-1),num_inst_lnd) + 1

  So, this means if we have num_inst_rof > num_inst_lnd, we map multiple   eris onto the same eri.  If we had 8 rof instance and 4 lnd instance,   then eri=1,5 gives eli=1, eri2,6 gives eli=2, etc. 

  This is weird - what happens if we then:

    call mct_avect_avg(l2racc_lx(eli),l2racc_lx_cnt)

? ...for the same eli?  What happens when we hit eri=5 and we do eli=1   again?

* Moving on, this prep_rof_accum_avg() gets called in the driver during the   setup to run rof, of course - we accum affter the land run, and average   just before the rof run.  (See line 4051 in cime_comp_mod.F90). 

  Then (still in teh drive) call prep_rof_calc_l2r_rx(), and finally   prep_rof_mrg().  So:

* prep_rof_calc_l2r_rx(): So, this appears to actually fill in the l2r_rx()   aVect, which we allocated as the last step in init.  So, it has   fractions_lx and deals with the mapper_Fr2l - interesting, the flux   mapper for going rof -> lnd, not the reverse.  Maybe it's the same with a   different sign or something.

  Starts by looping over num_inst_rof, and then using the mod trick to find   the matched eli and efi ("fractions") instance.  Huh, num_inst_frc - we   have instances devoted to fractions?  More mysteries.

  I don't understand the call to seq_map_map(), but it has something to do   with mapping fluxes between the l2racc (the accumulators on the lnd   decomposition), and l2r_rx(eri) (the final input to rof, on the rof   decomposition).  Then it does somethign manually for the irrigation   field.  Okay.

* I reviewd prep_glc_calc_l2x_gx(), to see how we handle state vectors,   since I claim that's what I'm using (although the carbon fields are   actually described as "fluxes", there's no domain boundary between gcam   and lnd, at least as far as I can see).  It *does* suggest a mapper_Sl2g,   so that suggests we still need to do this infernal "mapping" - it   ultimately calls map_lnd2glc, to map one field from lnd->glc grid, which   is part of map_ln2glc_mod.F90, in the CIME/drivers/mct area.  Yeesh.

  I think this glc remapping is because we have different elevation   classes, and ice-free vs. ice-covered.  So they have some additional   information they have to extract and use.

  It really seems like "mapping" means regridding, with some infrastructure   handle different domain decomps across components, and making sure the   fluxes are conserved at the boundaries between components.  The fractions   seem to be a scalar used when mapping?  Are they land/sea fracs, or am I   thinking of something else.  

  Jeepus, prep_rof_merg() seems to: loop over each rof instance, and send   l2r_rx(eri) and fractions_rx(efi) - then for each att in l2r_rx, it   scales by the fraction in fractions_rx, and then sets to x2r_r.  So it   seems like a straight copy, instance to instance, scaled by whatever is   in the fractions_rx that mapes to that particular rof instance.  

  I mean...what?  What problem is this solving?  What kind of calculation   is going on here?  Why is this called "merging", what is up with that   infernal mod index, why weren't the land values already scaled?  This is   very frustrating - I don't understand the math of what is being   accomplished here.

  Perhaps does this fractions stuff mean: the fraction of *this* land cell   that contains river runoff contributors?  So we scale grid cell wide   values by lfrac, because the flux coming into the rof part isn't the   entire flux of the whole cell, it's the 

* So, let me conjecture for a bit - if we define iac on the same grid as   lnd, then this fractions nonsense doesn't come into play, does it?   There's no fraction of each land grid cell that applies to gcam, so we   need to scale like this.  Right?

  Anyway, the fractions structure is an *attribute vector* that holds   ifrac, lfrac, ofrac, depending on I guess what component we are dealing   with. 

* Note that prep_atm_merge has a whole section for attributes that do not   need to be "merged" - they just get hooked up via mct_aVect_sharedindeces   structures (somehow - see line 508 of prep_atm_mod.F90), and then copied   over via mct_aVect_copy().  So, maybe do something like that?

5/15/2019 15:26:02

* I'm making a separate header because we had the phone call today.

* Rather than have the coupler aggregate and map to gcam grid, we decided   that we would let gcam do that, and just couple on the lnd grid.  This is   important because transforming the weights was seen as something of a   nightmare, and the gcam wrapper code already did it, so there.

  Gautam wanted to aggregate in the coupler, because that would facilitate   GCAM running in a spatially decomposed (i.e. multiproc) fashion, but Kate   pointed out that we can't run GCAM in parallel in the spatial domain,   because GCAM is all about trading across zones - there is strong lateral   coupling there.  The parallel version of GCAM runs across *calculations*,   so it's process-based parallelism, not spatially-based parallelism.  That   is unlike the other climate (i.e. physical) components.

  So we don't get anything by forcing the coupler to do all the   aggregating, so for now we will just run on lnd grid.  That simplifies   things, because it means I can use the same method for mapping patch/pft   -> column -> lat,lon as we currently do.

* The time step issue is a problem, though.  Peter said we really do want   to run the way we set it up before: yearly max of monthly averages, which   is basically choosing the month with most npp and using that as   representative for the year.  We'll table discussion about the five year   averaging/interpolating, and running five years in the future for now -   right now we just want to get it so we run a gcam every year, and figure   out if that's what we want or not.

! But, there's now the issue of how to accumulate - the MCT accumulators   (as I understand it) do averaging and max over the whole input period. I   want to average monthly then max across months, so I need to build my own   accumulator.  The question is, how do I implement this?

  I could build an internal accumulator inside of lnd2iacMod.F90 or   something - we update on clm scale, and the aggregator function then does   the right thing.  But, how does this couple, then?  When we call   lnd_export() on lnd2iac_vars, does it check to see if we are at the end   of the year before doing the max and then sending to coupler?  That seems   straightforward, although I'm not sure how to get the timers and stuff   ready. 

  You know, the prep_iac_accum() function inside the driver area is under   my control - rather than calling MCT functions perhaps I can simply   develop there.

  Or, maybe there is a way inside of MCT to build your own accumulator method -   in that case, lnd_export() sends it up and accumulates, and prior to   coupling with iac we call my custom reduce function.  Hmm...

  Anyway, I'm read something in the code that the glc component couples   with lnd every year, so I will dig in and see how that goes.

* Kate is very anxious to finally get some gcam building going, and that's   long overdue.  So I also will need to setup the buildlib and buildnml   scripts, which I'm hoping will be straightforward.  I know the namelist   options, so just learn how to store those in XML, and then figure out the   building - this might get a little complicated because GCAM is in C++,   but that can't possibly be a very big deal.

5/15/2019

* I finally learned a thing about dimensions: i've been doing it all   wrong.  Here is the header of the .h1 file:

------------   netcdf b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001.clm2.h1.2091-06 {   dimensions: 	lon = 288 ; 	lat = 192 ; 	gridcell = 20975 ; 	landunit = 28113 ; 	column = 45704 ; 	pft = 277784 ; 	levgrnd = 15 ; 	levlak = 10 ; 	numrad = 2 ; 	string_length = 8 ; 	hist_interval = 2 ; 	time = UNLIMITED ; // (1 currently) ------------

  So, notice a couple things: column is not gridcell, and pft is not 1:17.   So, the "patch" level coordinate includes both *position* and *plant   type*.  That's what the "do n=1,npft" loop at c. line 1638 (of my current   version) of iac2gcam_mod.F90 is about - we are looping over the 277784   plant *and* location information, and extracting each.  We can get the   i,j,pft and column information from this simple index, presumably by   using the pfts1d_* mapping elements from the same clm.h1 file.

* Okay, so first of all: is this patch-level mapping to location and plant   type constant?  Can I load them in a configuration file?

* Second, I now believe the begp:endp and begc:endc decomposition is   consistent - each chunk would have a consistent set of both p and c   values.  So I should be able to export them simply, via begp:endp and   begc:endc ranges as I've done.

! ...that is, as long as the veg_cf and col_cf structures exist on the time    scale in which I need to dump things.

* I still don't know what the difference between "gridcell" and "column"   is. I can infer from the calculation that's possible to go back and forth   between "column" and "pft", although apparently the calculation is not   easy (see c. line 1526 in iac2gcam_mod.F90).  It looks like you run   through the pfts, find the lat and lon, check the pfts1d_itype_lunit to   see if it's got vegetation (==1), then loop over all columns to find the   one with the same lunit and and same i and j, and then finally build the   mapping between pft1d_cols(patch) and column.

  It therefore looks like "column" includes land unit type information (the   clm2.h1 file suggests "(vegetated,urban,lake,wetland,glacier or    glacier_mec)" as types, so a fairly broad characterization of what the   column represents.

* If I had to guess, then, I'd say something like: gridcell = grid points   with non-land masked out; columns are are gridcells + land unit type; and   pft/patch are columns + plant type.  But it might not be a direct   relationship between these three, like "pft contains columns contains   gridcells".  For example, consider these three pft weight variables, from   clm2.h1: 

----------------- 	pfts1d_wtgcell:long_name = "pft weight relative to corresponding gridcell" ; 	pfts1d_wtlunit:long_name = "pft weight relative to corresponding landunit" ; 	pfts1d_wtcol:long_name = "pft weight relative to corresponding	column" ; -----------------

  So, I don't know what "weights" means in all this.  We do use   pfts1d_wtgcell in gcam, so that's something.  Anyway, this suggests that   there is a mapping between patch/pft and gridcell, landunit, AND column.

* It seems like all this mapping between column, pft, gridcell, and lat/lon   should probably be setup during the lnd initialization and available for   Use in lnd modules somewhere.  Alternatively, if they are set up via a    configuration file for each grid configuration, then we could crack and   extract that file to find these things.

* So, pft_weight_mean_g is the average of pft_weights[lon,lat,PFT] on the   "calc_avg" time scale, which is five years.

5/14/2019

* So, obviously, the coupling should be really really easy after all -   obviously, just export veg_cf%npp and col_cf%hr in lnd_export.  That will   automatically happen at the same time scale as it should, which is   probably "radiation time scale", which is probably something like 30   minutes. 

  So, something like:

  if (iac_present)    do g = bounds%begg,bounds%endg       i = 1 + (g-bounds%endgg)       l2x(index_l2x_Fzll_hr,i) = col_cf%hr(g)    end do

   do p = bounds%begp,bounds%endp       k = 1 + (p-bounds%endgp)       l2x(index_l2x_Fzll_npp,k) = veg_cf%npp(p)    end do   endif     ? Okay, I'm *still* not sure about the p index here - veg_cf_summary()   takes bounds but also num_soilp and filter_soilp as inputs.  So we build   veg_cf%npp(p) looping over filter_soilp(1:num_soilp), which is described   as a "patch loop".  So (a) p stands for "patch"; and (b) I'm not sure   what patch means, or how it's related to pft.

  Okay, well, in decompMod.F90, begp:endp are described as "beginning and   ending pft index".  So I'm going to go with that - veg_cf%npp(begp:endp),   and we'll used that in lnd_export().

* They seem to have mods in clm dedicated to lnd2atm and lnd2glc, etc., so   maybe I really should put it in a lnd2iac.  It should be simple enough to   copy over veg_cf%npp and col_cf%hr, but it's just this extra step that   seems extranous.  But it *would* give us a place to extend the coupling,   which I'm certain we will want to do at some point.

  It looks like the clm_drv() function calls lnd2atm() and   update_lnd2glc() near the end, like you would expect.  Then   lnd_comp_run() calls lnd_export() right after clm_drv() finishes.  All   this makes sense.  So perhaps copying to lnd2iac_vars is necessary, if   clm_drv() clobbers veg_cf and col_cf as part of cleanup or something? 

  Ah, we use atm2lnd_vars in our restart file - so it's certainly important   to store z->l coupling in some structure that we can access in a couple   different ways.  But the l->z coupling should only be used to send back   to MCT.

* Now, how to accumulate those values into something meaningful is another   question - I still need to review how MCT aggregators work, when they are   called, etc.  I presume the final reduction step happens in   prep_iac_coupling, or whatever that function is; hopefully the aggregator   mechanism allows us to store 30*365 values in a meaningful way that we   can use for that reduction.

* Similarly, iac_export should just send out our landuse[p] array from   glm.  It's a bigger question what to do about the grids, since the glm   grid is hardcoded as 360x720 - do we use MCT interpolation, or do I crack   iESM/clm and see how it dealt with glm output on a different grid than   lnd.  I think internally interpolating is probably the simplest, since   otherwise it means we have a different input and output grid for iac,   which seems goofy.

* The lnd model has it's own accumulators, internally - see line 1370 of   clm_driver.F90.  I think those are used internally, not for coupling -   for making yearly averages, that kind of thing.

! Okay, I've almost convinced myself - create an lnd2iac() module and   routine to update lnd2iac_vars, which is then sent to lnd_export().  It's   a little more invasive, but at least it's not hidden.

  It looks like lnd2atm_vars and lnd2glc_vars are defined in   "clm_instMod".  For now, I'm going to stuff it all into lnd2iacMod.F90 -   the variable, the updater, that kind of thing.

* It looks like cime_run_lnd_recv_post(), which is presumably run after the   land model runs, calls prep_iac_accum() - so that's good.  Thus, lnd   model runs, couples back to cpl, pre_iac_accum() takes the lnd2iac_vars   from the coupler and accumulates them.  Then cime_run_iac_setup_send()   calls prep_iac_accum_avg() to reduce the accumulators into what we want. 

  (This is a modification of how cime named things before, I think.   Hopefully it's better.)

  Looking at some of the comments, it appears that glc runs once a year,   too, with coupling and accumulating from lnd.  So look at   prep_glc_accum(), and prep_rof_accum() as well, because we apparently   average lnd2rof as well.

  So, I need these things:

@ lnd2iacMod.F90, with functions and vars to set lnd2iac_vars from   veg_cf%npp and col_cf%hr @ prep_iac_accum() to accumulate the values returned from land @ prep_iac_accum_avg() to finally transform/reduce the data into the single   arrays I need for input to gcam.

* A stray thought on restarts - there was discussion in the code about what   to do if we stop mid-year.  We've accumulated a partial year of l->z   variables, which we will lose if we don't save it somehow.  Does that   mean we need to include the accumulated l->z variables in their own   restart file?  I mean, a deliberate restart almost certainly will start   on the year boundary (or at least we could require that), but a crash or   something could happen anywhere.

  I think this was in the lnd2glc coupling, which also happens on a yearly   time scale, I think.  Anyway, what might this imply - a restart file on   every lnd time?  N days worth of l->z vars?  Or do we have to build our   own accumulator?  That seems crazy, MCT must have thought of this somehow.

5/13/2019

* A bunch of things about trying to extract npp and hr for l->z coupling.

* First of all, in the clm.h1 file, we have npp[t,pft] and hr[t,col].  So,   apparently npp doesn't involve any location info.  

* In the clm ColumnDataType.F90, we have a column_carbon_flux data type   that holds x%npp(c) and x%hr(c).  In VegetationDataType.F90, we have a   vegetation_carbon_flux which holds x%npp(pft)

  So, naively, it seems like we want column_carbon_flux%hr and   vegatation_carbon_flux%npp.

* Note that these names are *fluxes*, rather than *states*.  I'm not sure   treating them as a flux is meaningful, given that I don't think we have a   domain boundary between lnd and iac that we need to cross - it's kind of   collocated.  But, perhaps I should move them out of Sl avects and into   Flzl avects, or something like that.

  Anyway, more important than that is figure out where these values are   calculated, and how we might extract them into our avects. 

* Okay, well, in VegetationDataTypes.F90, it looks like the routine   veg_cf_summary() calculates the npp from gpp and ar, whatever there are,   at line 8111.  It then goes on to update annual NPP accumulator for use   in the carbon allocation code, so perhaps I insert my own accumulator   right here - find a monthly max or something, to match what I do with the   .h1?

  But I'm not seeing anywhere where we calculate an "NPP" and write it to a   clm.h1 file - I should figure out how that happens in iESM/clm, and see   if there's some clues in there.

  Also, track down when and where veg_cf_summary() is called.

* Also, I need to do the same sort of thing with hr, only this time it's   apparently by column.

* So, the we have a veg_cf and col_cf structure to pass this stuff around,   defined in VegetationDataType.F90 and ColumnDataType.F90.  So we are   looking to accumulate veg_cf%npp and col_cf%hp.

  veg_cf%npp in veg_cf_summary(), line 8111   col_cf%hr in col_cf_summary(), line 6501

  At the bottom of these functions (or after the assignment) we could call   accumulators.

! Both of these functions return immediately if we use_fates, so either   FATES will calculate them or that calculates something else that contains   the same information, possibly in a transformed way - or, perhaps, they   aren't calculated at all.

* Both of these functions are called in EcosystemDynMod.f90, line 220, in   function EcosystemDynLeaching() (again, if .not. use_fates), so at least   they are on the same time scale.  The EcosystemDynLeaching() description   says it runs on the "radiation time step", which, whatever.

* Okay, broad strokes: 

1 create an IacCoupling_mod.F90 module, to hold all the routines that deal   with lnd/iac coupling.  I originally thought it would be party of the lnd   module, but maybe make it part of gcam?  Hmm. Anyway, wherever it is:

2 Have functions to be called to accumulate_npp() and accumulate_hr() -   these will be called from veg_cf_summary() and col_cf_summary() to grab   the appropriate values.  Maybe an overloaded function, or with optional   args or something:

  accumulate_iac(npp=veg_cf%npp)   accumulate_iac(hr=col_cf%hr)

3 This function either works completely in clm space, until we have a   yearly coupling alarm, that sends the results to the coupler; or we use   the MCT aggregator mechanism to couple every time step, sending the new   npp and hr values, and then have the iac prep functions complete the   aggregation into whatever it is we need to run on our yearly time step. 

4 Either way, we need to figure out how to get these regular values and   turn them into something we want.  We also need to figure out how to get   the other things we read frm clm.h1 - landfrac, landmask,   pft_weight_avg_g.  I *think* those things are static, so we can read them   from the clmbase file as a configuration - but if not, and/or if we need   the aboveg and belowg carbon stuff that we read but presumably don't use   in iESM, then we'll need to do some more archeology to figure out where   to get these values from.

@ Review how MCT aggregators work @ Review how things get put into the coupler, and how they get taken out -   something about the prep functions, I think. @ Review iac2gcam_mod.F90 to see how we use these npp_avg[p] and hr_avg[c]   values  @ Review the driver, to review calling sequences from there - see   prep_iac_mod.F90 stub, and try to find examples of some other coupling   accumulators from there.  Then, try to find examples of where such things   are called from.

@ Try to follow some flux coupling, from l->r, for example.

5/10/2019

* Hidden away in gcam/src/iac/shr is iac_fields_mod.F90, with some useful   definitions.  I'm not sure, but it seems like a "shr" directory is   designed to be shared between various elements, but since I mostly need   this stuff internal I'm going to move it into the coupling directory.

* Anyway, in iac_field_mods, we have this defined:

    integer, parameter, public :: iac_glm_nx  = 720     integer, parameter, public :: iac_glm_ny  = 360

  So now this clicks into place - we use two grids, basically an input and   an output grid - clm_nx,clm_ny matches the land grid, and is used on   input clm.  iac_glm_nx,iac_glm_ny defines the glm grid, which is 720x360,   the one used by updateannuallanduse(), and the output of the glm code.

@ So this means we probably *don't* have to transform the input data, and   can define the iac grid to match the clm grid.  But it means we *do* have   to transform the glm grid back to the iac/clm/cam grid.

* It seems like iESM has a somewhat different EClock interface, and   probably a cdata interface as well.  For EClock, at least, rather than   hunt down all the uses of EClock inside the gcam wrapper code I'm just   going to define a GClock just like the gcam-Eclock, and translate to and   from E3SM EClock.  (From now on, EClock means what E3SM says, and GClock   is the other thing.)  It's possible the iESM calls are compatible with   EClock, but redundancy like this isn't the worst thing in the world.

  GClock basically just has elements for holding the various alarms to run   things, which are moot in my implementation, and then ymd, tod, and dt.   Still, there we go.

* Back to iac2gcam_run_mod(), anything that cracks clm_base   (iac_base_clmfile) should stay  that way - it's basically a configuration   file.  What I need to figure  out now is how we use those values when we   aren't doing the goofy  interpolate stuff.

* iac2gcam_run_mod() does do a lot of calculations aside from just cracking   the file and calling calc_clmC().  But calc_clmC() mostly cracks and uses   the clm history file, which should be replaced with the lnd2iac_vars that   we've imported from the coupler.  THUS, I now need to go back to   iac_import() and figure out how to turn an AVect into useable variables   in calc_clmC().

  Another thing calc_clmC() does is read/write restarts, and writes out   an iac history file, both of which it should keep doing.

  The final thing calc_clmC() does, and where it gets it's name, is turn   this analysis of monthly clm files into the weird "5-year mean of yearly   maximums" fields.  My initial plan was to simply have the MCT aggregator   generate whatever yearly means we need, but if it's a complicated and   weird calculation like that, then...eh, it's probably still best.

! Okay, so the above paragraph clarified something: here is what   we end up using, I think, in gcam, for npp and hr:

  The five year *mean* of the yearly *max* of the monthly *averages*. 

  So, that's a mean of the maximum of means.  Yeesh.

* Working on iac_import - the idea is we extract the npp and hr fields, and   maybe pft_weight_g into iac2lnd_vars.  

5/9/2019

* I've been banging my head again on iac2gcam_mod.F90 again, the function   iac2gcam_run_mod().  This is the key function that includes what will   become the l->z coupling, and, right now, reads the clm history file.  I   need to make some notes here about how this is structured and what it   does, so I can figure out what parts I keep and what I dont. 

? The big question mark for me is how the time scales work.  This is the   time scale for how things run in iESM (see logicals set at c. line 441 in   iac_comp_mod.F90).  This is the order they are called in the code, so   when these time scales match up they will happen in this order.

1 Every *month*, iac_run_mod() calls iac2gcam_run_mod() to run calc_clmC(),   which reads the clm h1 (monthly) history file, and does some stuff with   it - at the very least, it returns npp and hr, but it might do some   additional calculation or accumulation. 

  Okay, calc_clmC() file basically accumulates inputs and writes history   and restart files.  See below...

2 Every *five years*, on January 1, this iac2gcam_run_mod() call goes on to   do "calc_avg" section of the code, and then iac_run_mod() does some stuff   in the AGcamsetden section.  ("long_gcam_timestep").

  It looks this AGcamsetden section is actually really important, because   the call to gcam_setdensity_mod() is how we set the carbon data for the   defined year.  I need to review gcami vs. gcamiold, but we seem to be   doing yearly interpolations of gcami over the next five years, using the   previous five years of clm.h1 data that we accumulate in calc_clmC().

3 Every *year* (one year), iac_run_mod() calls gcam_run_mod(), apparently by   advancing the model year by 5 years.  (There's some stuff if the   long_gcam_mod is set to 15 by setting sneakermode, but I'm going to   ignore that for now).

4 Every *five years*, iac_run_mod() will then call gcam2emisfile_run_mod(),   to send CO2 emissions to the atm model.

5 Every *year*, iac_run_mod() will then call gcam2glm_run_mod(),   glm_run_mod(), and glm2iac_run_mod().

6 Finally, iac_run_mod() dumps a history file

--------------

* So, what does this mean?  First of all, we will now run   iac2gcam_run_mod(), and/or calc_clmC() (or some other function to do   those things) not every month, but every regular gcam time step (1 or 5   years).  I think we *also* will do a lot of what the calc_avg section   does, since it really is some additional calculations that get sent to   the gcam subcomponent, but is also some averaging.  All that averaging   stuff should be handled by mct, now, so when we run iac_run_mct() the   input AVects should already be averaged over the year.

* I need to go back and revisit the model times we are running here.  I   *believe* what we end up doing for year yt, we run gcam at yt+5.  We do   that every year, I guess so that we have a running list if inputs to send   back to clm every year?  That doesn't seem quite right, so I need to go   back and track what the EClock(iac_Eclock_ymd) gets set to before calling   all these functions.

* So, my initial guess is that we appear to be reading and accumulating   data on a monthly/5-year time scale - but we *run* gcam every year for   five years in the *future*.  Hmm.

* Quick note - from the restart file in Kate's sample run,   (iac_clmC_file.r.2090-11.nc), we can see that the "iac_iac_nx,   iac_iac_ny" we are using in calc_clmC() is the same as clm_nx and clm_ny   from the namelist.  (clm_nx=288, clm_ny=192).  So, the iac input grid is   the same as the clm grid - good to know.  (We must convert to the glm   output/updatelanduse() grid of 720x360 later on).

* A review of calc_clmC() shows that it does this:  reads clm.h1 file on a   monthly basis and accumulates the *yearly maxes* of these inputs, and   then every five years computes the *average* of those yearly maxes, and   sends that back out and writes a history file.  (It also manages some   restart read/write stuff.)

* Back to iac_comp_mod.F90, then - after doing calc_avg inside of   calc_clmC(), we then interpolate the gcami over the next five years and   push that carbon data to gcam for those years via gcam_setdensity_mod().   Whew! 

5/6/2019

* So, I need to rename some of my stuff.  I've been using "iac" to mean the   generic coupled component, and "gcam" the specific model; the iESM code   base uses "iac" to mean the specific coupled component, with "gcam" (and   "glm", etc.) as a subcomponent of the overall thing.

  When I say "iac" I mean it in the context of MCT, but iESM uses it as a   wrapper for the fortran interface coming out of the (gcam,glm,...)   submodels. 

  I can't avoid using iac now, and I don't want ot rename the iESM files,   so we'll just deal with that: try to keep iac_ as a prefix for files that   are seen outside of (gcam,glm), and understand that iac2x or x2iac means   interaction with the submodel.

! But, I can't call my functions and modules "gcam_", because that's now   means the submodule, the interface with the C code a layer down, not the   interface with the coupler at this layer.  So, specifically, I can't have   gcam_run(), gcam_init(), gcam_var_set(), which I've stubbed into   iac_comp_mct.F90.  Instead, I need a new tag to indicate "internal   (gcam,glm,...) group layer functions to interact with mct() rather than   submodules".

  Maybe "giac" - for gcam/iac layer?  That would actually be a better tag   for what iESM calls iac, but that ship has sailed.  Maybe I could go   ahead and rename iac_comp_mod.F90 into giac_comp_mod.F90.

5/5/2019

* Jeez, I keep losing track of the calling tree inside of iESM, which I'm   trying to lift as much as possible over to E3SM.  Fortunately, I did make   a map of it - so, to remember it, this is the way the tree should work in   E3SM:

   driver ->    iac_run_mct() [ gcam/src/cpl/iac_comp_mct.F90 ]  [[ iESM: lnd_run_mct() ]] XXXX ?  -> gcam_run() ? [ see below - do we need gcam_run(), or will       iac_run_mct() do everything? XXXX    -> iac_run_mod()         [ gcam/src/iac/coupling/iac_comp_mod.F90 ]     ->   iac2gcam_run_mod() [ gcam/src/iac/coupling/iac2gcam_mod.F90 ]     ->   gcam_run_mod()     [ gcam/src/iac/coupling/gcam_comp_mod.F90 ]   ( ->   gcam2emissfile_run_mod() [ gcam/src/iac/coupling/gcam2emisfile_mod.F90 ] )     ->   gcam2glm_run_mod() [ gcam/src/iac/coupling/gcam2glm_mod.F90 ]     ->   glm_run_mod()      [ gcam/src/iac/glm/glm_comp_mod.F90 ]     ->   glm2iac_run_mod()  [ gcam/src/iac/coupling/glm2iac_mod.F90 ]          -> updateannuallanduse()

* Okay, so this is kind of a nightmare to keep track of all this.  A lot of   this is to have fortran calls to connect better with the coupler and   other models.  But there's a lot of ambiguity when we have both iac and   gcam floating around - I think that logically they considered "iac" a   shell around (gcam,glm, and emissivity) subcomponents, and that's why we   do stuff like iac2gcam and glm2iac.

  So, hopefully, this means I can mostly just set things up in   iac_run_mct() to do this calling sequence, and I'll be good.  But once   again I need to review what all these code elements are doing - there is   E3SM-like stuff Eclock and attribute vectors inside the iac-named routines   inside of gcam/src/iac, and probably elsewhere.  

* Okay, where else am I?  I need to write these these modules:

  gcam_var - just public variables like gcam_lon, gca_lat, gcam_Active stuff like that   gcam_mod - gcam_init() and gcam_run()   iac_ctl_mod  - the iac_ctl control structure, which I don't completely     understand. It's called rtmCTL in rof (mosart), and seems to be just a     structure to hold a lot of information taht you pass back and forth.  I'm     trying to find the analog in lnd - clm seems to store the boundary     information itself in a separate bounds structure. 

    For now I'm goign to put the big ctl everywhere so I can identify     it easily, in case I search differently.

* It's hard to track down all the things we need to configure when   everybody has a different idea of what those things are, and how they   should be organized.  

* So iac_init_mct() is (roughly) finished.  Here's the next things to do:

@ gcam_init() - big one, figure out how to make grids, etc. @ iac_run_mct() - figure out what this does - does it the (gcam,glm,emiss)   functions as above, or does it call gcam_run(), which does all thato?

  See, the thing is I think the iac_run_mct() function is basically for   coupling.  I don't know if it should do the model flow, or if it should   farm that out to a separate function.  Again, question of organization -   what I'm trying to do is separate all this E3SM stuff from the actual   model stuff.  That may be handled appropriately just by these fortran   functions inside the coupling directory.

  I'm tempted to rename them all, because it's all very confusing, but   probably not - just remember that we are kind of treating gcam,glm,emiss   as subcomponents of iac, which is why we have functions to go back and   forth.     5/1/2019

* From Gautam: 

  The landuse data is available on Anvil at

  /lcrc/group/acme/public_html/inputdata/lnd/clm2/surfdata_map/landuse.timeseries_ne30np4_ssp5_rcp8.5_simyr2015-2100_c190411.nc

* Phone call today - talked about gridding, and the transformations between   lnd->gcam, gcam->glm, glm->lnd.  Here is my understanding:

  The glm internal grid is hardcoded to 360x720. In iESM, we took lnd->gcam   regional somehow, but lnd was at 1 degree, which is not the hardcoded   360x720 glm grid.  So there must have been some interface to read at lnd   resolution, convert to gcam regions, then convert onto the glm grid, and   finally write out the dynamic crop file.  Then clm would read that file   and convert to the resolution it needed.

* Thus, for E3SM/GCAM, we need three conversions:

1 lnd->gcam 2 gcam->glm 3 glm->lnd

  Internally, E3SM and MCT accomplish this simply by linear algebra: if   you've got inputs x need outputs y, each on their own grid, define   a mapping T from the x grid to the y grid and then simply run y=Tx for   all x.  This interpolation is almost by definition a very sparse matrix,   since the only thing that contributes to y[i] are the points x[j] that   are around i, so there are sparse matrix multiplier tools that make this   go fast (and, I think, in a distributed manner).  One of the MCT papers   talks about this.

  I believe that (2), gcam->glm, happens internally as part of the gcam,glm   calling sequence - gcam works on its regions, and then glm scales it to   the grid it needs to work on, somehow.  So that leaves defining   transformation mappings for (1) and (3), and doing the regrid ?  transformation somewhere in the processing flow.  Logically, this might   mean we run y[gcam]=Tx[lnd] whenever gcam gets lnd vars from the coupler,   and similarly when lnd reads the glm stuff.  But it could mean we   transform *before* sending to the coupler...

4/30/2019

* I've been rereading and working through the MCT papers again, because I'm   trying to understand the gsMap thing.  I think it tells everybody what   segment of global ids is on which MPI process, so that communication when   you want to find those global ids is possible.  Thus, even for Gcam,   running on nproc=1, it needs a gsmap so that lnd and atm can find the   things they need from the coupler.

  At least, I *think* that's what's going on.  I'm still a little unclear   on how it's actually used in practice - is this how we interpolate from   one grid to the other?  Also, how we map from one domain decomposition to   another...      I've spent a lot of time on this, the *theory* of how the coupler works,   and so I'm getting behind on actually implementing this stuff, so maybe I   should table the reread and start pounding some code again.  But this is   important to understand.

* Ah, at last - the GeneralGrid class is how we define the actual grids we   use in our attribute vectors - "a literal listing of each mesh point's   coordinates and geometric attributes".  So, somehow in the initializating   we need to use this to describe how our GCAM mesh works.

* Note that GCAM's "grid" only makes sense in the interface with lnd and   atm - internally it has a region based location description, and it's   only in the sense that it gets input from and output to other components   (and files and stuff) that we need to deal with a grid.  Nevertheless,   we'll define our grid by the pftdyn/surfdat file, and use MCT to interact   with other components.

4/23/2019

* Okay, I was wrong - MAXINPIX and MAXINLIN are not actually used in   updateannuallanduse.c.  It looks like the in and out arrays are all on   teh same grid - dimensioned by MAXOUTPIX*MAXOUTLIN.  

  So, it seems clear that this version of GCAM was designed to run on the   720x360 grid, only.

  Also, I'm pretty sure a lot of the input arrays are not actually used,   although things like incurrentpftval[][] is read in from the surfdat   file. 

* Yeesh, I'm still having trouble figuring out the grid here, it all seems   set elsewhere and assumed.  At any rate, plodata[][] is clearly on a ? 360x720 grid, hardcoded.  Could we simply change the values of MAXOUTPIX   and MAXOUTLIN and have this code work, assuming we had a valid   surfdat/mcrop file for the new resolution? 

! Okay, now I really have no idea what gets called and what doesn't inside   updateannuallanduse_main().  It looks like they crack some netCDF files,   maybe with dynamic values, maybe written out by glm?

  Okay, so all this stuff seems to be on the same grid, which suggests, and   this might be crazy, that this whole gcam development tree can *only* run   at 360x720 gridcell resolution.  Could that be true?  I just don't see   how anything in this file would work if you changed the grid.

* So, what would this imply then - GCAM itself works regionally, and then   Kate has mentioned something about downscaling.  So maybe we just say the   GCAM grid is always this 360x720 one, and then, maybe? MCT can do the   interpolation and stuff onto whatever the land grid is?

? Stray thought...those "mapping" functions that befuddeled me when I first   started all this archeology - those might be how you interpolate from one   grid to the next.  Nb. my comments about prep_lnd_get_mapper_Sa2l() from   6/27/18 or prep_lnd_calc_r2x_lx() from 1/5/18.

  Could something like this be used to go from our require 360x720   resolution to whatever we have for lnd?

---------------

* Okay, for now I'm going to let this percolate in background and go back   to trying to get all the framework stuff done to actually get a gcam   component running.   I'm in the middle of setting up iac_mct_mod.F90,   which contains the functions the driver calls.  From there, we need to   modify iac_run_mod.F90, which is the fortran interface to calling GCAM   with all the right bits and pieces.

* I've stubbed in a gcam_init() function, but I'm looking at init functions   for other components to see what is going on in there.  Typically, the   comments suggest that's where you read in the grid and namelist   information.  The lnd init was pretty complicated, so I'm reviewing   Rtmini() from mosart (I know I've done this before), just to get an idea   of what kinds of setup you expect to need.  Of course, I am especially   interestd in anything having to do with the component grid, since I have   yet to figure out how that figures into things.

* Well, Rtmini() is ~1000 lines long.  So much for simple.

  My hope is that most of this model based init has already been written by   the gcam functions.

  Anyway, here is what Rtmini() does:

* namelist /mosart_inparm/ so, there's that. * Read in namelist in masterproc, then mpi_bcast() to other nodes with   values.  That make sense. * Some logging if (masterproc) - write(iulog,*)... * Switch on do_rtm/rtm_active and a flood_active logical * ...error checking, logging * Time manager init - ncd_pio_init(), check for restart, then   timemgr_init() if starting or custom RtmRestTimeManager() if restarting,   I think.  Also - here is where we read and set dtime_in - the coupling   period.  * Initialize rtm_trstr, for tracers - I think it's just a string saying   what they are, colon separated so maybe it's a formatted list of tracers   or something. * Ah, here is where we set the grid.  ncd_pio_openfile() on the   "frivinp_rtm" namelist option, which is a filename.  Crack that file,   read in "rtmlon" and "rtmlat", the dimensions of the grid.  Then allocate   stuff - first lat and lon based, then flattened 1D arrays.  Then read   "longxy" and "latixy" to fill in rlon[] and rlat[] and others in control   structures and whatever.  Also read "area" and "ID" to fill in other   arrays.

  There's a clue in here - the area_global[g] array is flattened 1D from   [lon][lat] - the area array from the netcdf file is 2D, so we build a   flattened index n=(j-1)*rtmlon+i, for i over lat, j over lon.  So,   obviously, this is a normal column major linearized index calc.

! So, we actually *crack a file* to figure out our grid!  This makes sense   - you set which file you want to use in the namelist and from there   extract the grid.

! So, the mozart file has something called dnID, which is then set to a 1D   array dnID_globa().  But, then we have this comment (c.470 in RtmMod.F90)

===================================     !-------------------------------------------------------     ! RESET dnID indices based on ID0     ! rename the dnID values to be consistent with global grid indexing.     ! where 1 = lower left of grid and rtmlon*rtmlat is upper right.     ! ID0 is the "key", modify dnID based on that.  keep the IDkey around     ! for as long as needed.  This is a key that translates the ID0 value     ! to the gindex value.  compute the key, then apply the key to dnID_global.     ! As part of this, check that each value of ID0 is unique and within     ! the range of 1 to rtmlon*rtmlat.     !------------------------------------------------------- ===================================

  Okay, so there we go - global grid indexing is 1,lon*lat, starting at (lon,lat)   (-180,-90) and going up to (180,90).  I'm going to have to draw this out,   because my row-major instincts will get it backwards, but I'm pretty sure   we vary lon fastest (that's why n=j*nlon+i).

  Anyway, it looks like they build this IDkey() array that takes the 'ID'   field for each grid cell (read from the file) and assigns it the global   grid index going from 1,lon*lat.  This isn't important for me - they just   have a different ID for grid cells, so this is how you go back and   forth.  So we do that with dnID_global(), so it is now ordered by global   index rather than 

* Back to Rtmini:

* Calculated edges of grid cell - why do this? * Grid mask - land, ocean, ocean outlet from land.  I *totally* do not   understand what is going on in the loops at c. line 574, but whatever,   figuring out which grid points are what kind.  It has something to do   with the dnID_global() we read from the file, so whatever.  

* From here, calculate various values having to do with the grid, which is   very rtm specific.  Basic idea: find all the river basins, and then   allocate them to pes.  That makes sense - you parallelize river models by   distributing the river basins around.  Anyway, the idea is the init   function also figures out how to distribute it's calculations.  Then,   assign cells to different chunks accordingly. 

  Basically, this sets rtmCTL%begr and rtmCTL%endr for each processor   running this init, and thus you allocate arrays accordingly.  Then do   some more initialization based on the decomposition.

* Then, at the end, they do stuff like run nr over rtmCTL%begr,rtmCTL%endr,   and set rtmCTL%lonc(nr) and rtmCTL%latc(nr) accordingly.  So, this is how   you build your grid - but it's not really global, is it?  Maybe it is for   anybody attempting to work with rtm data, include rtmMod and here's the   array.  But even then, it's pe dependent - rtmCTL clearly is different   for each processor, whcih is why rtmCTL%begr,rtmCTL%endr works, so for   the moment we've build a local set of lonc and latc values for each grid   cell nr.  How do we use that grid external to rtm - i.e. how do we couple   with this information?

  I *think* the answer might be in the functions we call right after   Rtmini() in rof_comp_mct.F90 - rof_SetgsMap_mct().  Similarly, lnd does   the same thing - call lnd_SetgsMap_mct().  Okay, so my guess is that is   the function that somehow informs MCT of how all these x2z and z2x   attribute vectors are dimensioned.

@ iac_SetgsMap_mct() - figure out how to do this.

* But, back to Rtmini(): line 1051 calls mct_gsMap_init() as part of   "Compute Sparse Matrix for downstream advection".  Also, calls to   mct_sMat_init(), which, I'm at a loss as to what they do.  sMat obviously   means "sparse matrix", so some kind of linear algebra re-decomposition?

  Whew, a lot of stuff I don't get - using MCT to, I think, do some   distributed linear algebra, which is why they need to stuff things into   attribute vectors apparently outside of any "coupling" considerations.  I   guess anything you distribute can use MCT functionality?  

@ Review lnd() initialization and see if it does somethign like this.

* Okay, now back to something I understnd - line 1328, if restart crack and   read restart file.

* Initialize history handler and fields.

* That's it.

4/22/2019

* Just a reminder - the gcam submodule is in:

    /home/shippert/E3SM_active_gcam/components/gcam/src

  This means: 

    /home/shippert/E3SM_active_gcam/components/gcam/cime_config      ...will be part of the main E3SM branch.

  So, to commit and push to the gcam submodule, go to gcam/src and do your   thing.  

  Apparently, to connect this version of gcam with a version of E3SM, you   go up to E3SM and add .../gcam/src, commit, and push - it will then   codify into the E3SM commit which submodule commit you are using, so it   knows which commit tot grab from the submodule repository.

4/22/2019

* The function updateannuallanduse.c returns its output into   plodata[][pft+7] - look at copy2plodata(); it fills in the   plodata[outgrid][pft] from outhurtpftval[pft][outgrid] (you have to flip   indeces because C->f90 row vs. column major order), then adds in some   additional outhurtt###[][] stuff on top of that.  So all that stuff is   used for gcam/glm.

  Anyway, this means I don't have to muck with updateannuallanduse.c at all   - it *already* returns what I want in plodata[g][pft].  Just a straight   copy into the already allocated avects!

! Except, the plodata[][] is on a hardcoded grid of MAXOUTPIX=720 and   MAXOUTLIN=360.  

XXXXXXXXXXXXXXXXXX see 4/23/2019   I think this is the "downscaling" Kate has talked about,   since MAXINPIX=7200 and MAXINLIN=3600. XXXXXXXXXXXXXXXXXX

  ...so it might be that I either make MAXOUTPIX and MAXOUTLIN dynamic,   which *would* require refactoring of updateannuallanduse(), or we have to   do some kind of transformation ourselves explicitly to put everything on   the land grid. 

* For now, don't worry about that - instead, it's finally time to start   pounding the iac_run_mct() function, to call the iac_run_mod() functions   that do the GCAM calling routine, and just make sure we put in hooks to   push our output back to the avect as we can.

4/17/2019

* Okay, well, I got it all wrong. The *only* thing that changes in the   mcrop_dyn.nc file that we read is PCT_PFT.  All the PCT_WETLAND,   PCT_URBAN, etc. stuff is static, and the same from run to run.  We aren't   actually modifying the land!  We are simply tracking how much of the land   has vegetation on it - crops and deforestation, I think.

  Because it is the only field that changes, we *only* need to extract and   couple with PCT_PFT!  In mcrop_dyn.nc, PCT_PFT has the dimensions:

  	float PCT_PFT(time, lsmpft, lsmlat, lsmlon) ;

  So: PCT_PFT[time][pft][lat][lon] - it's an array of percentages of each   of the 17 pfts at each time and location.

  That's what is goign on in writepftdynfile() - it   first tracks down the "nvarspcnt" index (probably the "percent" vars),   finds PCT_PFT, appends the current time stamp, then goes over each grid   and each pft and stores the 17 values - it looks like it's trying to   write it out in flattened notation, which maybe is how you do straight   netcdf, I can't remember anymore.

  Okay, so the values[] array at line 3038 is simply size 17 for the 17   pfts (17th is unused).

  So this means that: outhurttpftval[pft][lat,lon] is exactly PCT_PFT, and   I simply need to bring that out and stuff into an AVECT.

? I *think* that updateannuallanduse.c *hardcodes* the grid that the mcrop   file uses, so that suggests we have one mcrop file with its own grid, and   thus we need to transform this crop data onto whatever grid clm is   running on.  But, clm probably already does that!  So, what grid do I use   to transmit this out, something else?

@ Also, fix seq_flds_mod.F90 - I'm not sending that PCT_WETLAND stuff.

4/16/2019

* Back to clm reading surfdat file - it looks like the routin   surfrd_get_data() in srfrdMod.F90 is what we want.  This is in the iESM   tree, so who knows if current clm's do the same thing, but whatever, I'm   following this rabbit hole as far as it goes, at least to figure out what   it pulls out of the file.  

  Also, maybe in srfrd_wtxy_special(), and surfrd_wtxy_veg_all(), and   surfrd_wtxy_veg_dgvm()? 

  A bunch of functions called by srfrd_get_data(), which seem to read from   the cracked file, which have something to do with calculating the   "weights". 

  Also, then, srfrd_get_grid() and srfrd_get_globmask().  Whew, okay, a lot   to unpack here - so, grid is about the coordinate system and globmask is   about (I think) how much land on each cell.  Jeez, this is a mess.

! Okay, so, here's a clearinghouse of every single field I think we are   trying to read from the surfdat file.  I'm going to just list them here,   and then try to match them with what I'm seeing in the test iESM run file   listed below (way below), and see if they match up.  If so, then let me   track backwards and see where the GCAM stuff modifies those things, and   *then*, finally, I'll have a list of what things need to be z->l   coupled.   

  From: egrep 'subroutine|ncd_io' surfrdMod.F90, then cleaned up a bit: ===================================   subroutine surfrd_get_globmask(filename, mask, ni, nj)        call ncd_io(ncid=ncid, varname='LANDMASK', data=idata2d, flag='read', readvar=readvar)           call ncd_io(ncid=ncid, varname='mask', data=idata2d, flag='read', readvar=readvar)        call ncd_io(ncid=ncid, varname='LANDMASK', data=mask, flag='read', readvar=readvar)           call ncd_io(ncid=ncid, varname='mask', data=mask, flag='read', readvar=readvar)

  subroutine surfrd_get_grid(ldomain, filename, glcfilename)        call ncd_io(ncid=ncid, varname= 'area', flag='read', data=ldomain%area, &        call ncd_io(ncid=ncid, varname= 'xc', flag='read', data=ldomain%lonc, &        call ncd_io(ncid=ncid, varname= 'yc', flag='read', data=ldomain%latc, & X      call ncd_io(ncid=ncid, varname= 'AREA', flag='read', data=ldomain%area, & X      call ncd_io(ncid=ncid, varname= 'LONGXY', flag='read', data=ldomain%lonc, & X      call ncd_io(ncid=ncid, varname= 'LATIXY', flag='read', data=ldomain%latc, &        call ncd_io(ncid=ncid, varname=trim(vname), data=rdata2d, flag='read', readvar=readvar)        call ncd_io(ncid=ncid, varname=trim(vname), data=rdata2d, flag='read', readvar=readvar) X   call ncd_io(ncid=ncid, varname='LANDMASK', flag='read', data=ldomain%mask, &        call ncd_io(ncid=ncid, varname='mask', flag='read', data=ldomain%mask, & X   call ncd_io(ncid=ncid, varname='LANDFRAC', flag='read', data=ldomain%frac, &        call ncd_io(ncid=ncid, varname='frac', flag='read', data=ldomain%frac, & ?      call ncd_io(ncid=ncidg, varname='GLCMASK', flag='read', data=ldomain%glcmask, &

  subroutine surfrd_get_topo(domain,filename)     call ncd_io(ncid=ncid, varname='LONGXY', flag='read', data=lonc, &     call ncd_io(ncid=ncid, varname='LATIXY', flag='read', data=latc, & ?   call ncd_io(ncid=ncid, varname='TOPO', flag='read', data=domain%topo, &

  subroutine surfrd_get_data (ldomain, lfsurdat) ?   call ncd_io(ncid=ncid, varname= 'PFTDATA_MASK', flag='read', data=ldomain%pftm, &     call ncd_io(ncid=ncid, varname=lon_var, flag='read', data=surfdata_domain%lonc, &     call ncd_io(ncid=ncid, varname=lat_var, flag='read', data=surfdata_domain%latc, &

  subroutine surfrd_wtxy_special(ncid, ns) X   call ncd_io(ncid=ncid, varname='PCT_WETLAND', flag='read', data=pctwet, & X   call ncd_io(ncid=ncid, varname='PCT_LAKE'   , flag='read', data=pctlak, & X   call ncd_io(ncid=ncid, varname='PCT_GLACIER', flag='read', data=pctgla, & X   call ncd_io(ncid=ncid, varname='PCT_URBAN'  , flag='read', data=pcturb, &     if (create_glacier_mec_landunit) then          ! call ncd_io_gs_int2d ?      call ncd_io(ncid=ncid, varname='GLC_MEC', flag='read', data=glc_topomax, & ?      call ncd_io(ncid=ncid, varname='PCT_GLC_MEC', flag='read', data=pctglc_mec, & ?      call ncd_io(ncid=ncid, varname='TOPO_GLC_MEC',  flag='read', data=topoglc_mec, &

  subroutine surfrd_wtxy_veg_all(ncid, ns, pftm) X   call ncd_io(ncid=ncid, varname='PCT_PFT', flag='read', data=arrayl, &

  subroutine surfrd_wtxy_veg_dgvm() ===================================

* Okay, now cross reference these with the mcrop_dyn.nc file from the iESM   run...(see ~/PIC/surfdata_360x720_mcrop_dyn.dod, the ncdump -h of the   mcrop_dyn.nc file in the test run). XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX (see 4/17/2019)   I see:     AREA     LONGXY     LATIXY     LANDMASK     LANDFRAC     PCT_WETLAND     PCT_LAKE     PCT_GLACIER     PCT_URBAN     PCT_PFT

  I'm missing:     GLCMASK     TOPO     PFTDATA_MASK     GLC_MEC     PCT_GLC_MEC     TOPO_GLC_MEC

* So, from this it looks like I want to focus the PCT_landtype vars, and   maybe the landmask and frac, plus the coordinate description XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ! Now, finally, cross reference with what the GCAM stuff in iESM actually   writes out.  See below at 1/18/19 for more information about how this is   used.

  It looks like the function calchurtt() calls sethurttcrop(),   sethurttpasture() and sethurttlanduse() to modify the outhurttpftval[][]   array, which is then used to write out a new mcrop_dyn.nc file.  Okay!

  Looking at sethurttcrop(), there are a lot of output PFTs being   generated...

  STOP.  This is all very obfuscated, so it will take some time to track   down.  But!  I see in all of these cases where we read in

    LANDMASK     LANDFRAC     PCT_WETLAND     PCT_LAKE     PCT_GLACIER     PCT_URBAN     PCT_PFT

  ...via lines c. 5220 in updateannuallanduse.c:

		readlandmask(); 		readlandfrac(); 		readlakefrac(); 		readwetlandfrac(); 		readicefrac(); 		setvegbarefrac(); 		 		/* now read the reference year pft data */ 		readcurrentpft(); 		readcurrentpftpct(2000);

  I think readcurrentpft() simply maps the pfts to the grid, and   setvegbarefrac() is simply what fraction is left from each grid cell when   you subtract ice, lake, and wetland. 

  Okay, so I'm pretty sure what we are writing out has to be these values,   however they are calculated.    4/15/2019

* Jeepus.  I simply can never remember how to do this:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_active_gcam   cd E3SM_active_gcam   git checkout bishtgautam/gcam/add-submodule   git submodule update --init   git checkout -b shippert/gcam/active-gcam

4/10/2019

* I need to make a branch off of bishtgautam/gcam/add-submodule to do my   testing; it may have the configuration mods necessary to run on anvil, as   well as the gcam submodule and other stuff.

* Dumb reminder: to create a new branch from an old master:

  cd <mdir>   git pull (very important!)   git checkout -b <new branch>

  Push up to github      git push origin <new branch>

  See:  https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches

* Because it appears the cime developer group is taking over the automatic   optional stubbing for siac, until I get their updates I'm going to do   what Gautam did, and track down the compsets for every unit test and   simply add an _SIAC after the wav component.  If there is an ESP   component, then, hmm.

  Initially, it looks like a lot of the unit tests are in teh cam   config_compsets.xml, so there we go.

* The tests are listed in cime/config/e3sm/tests.py, and e3sm_developer   inherits lnd and atm developer test lists, so that's a start.  I can run   down the others, too, once I link their test names with the compsets they   are running.  I still don't like hammering in a _SIAC, but it seems like   that's the only way I'm going to verify that siac works - it's the   automatic adding of siac to the list that was the problem.

* I could go back and add in my auto stuff in case.py, now that we have a   new branch that might be fully caught up.  That would require *removing*   this _SIAC nonsense, which is a pain, but actually easier than going the   other way (also, I did backup to a .dist file, just in case.)

! Interesting!  It appears that while clm and cam config_compsets.xml   changes to explicitly add _SIAC work, the mods to the otehr   config_compsets.xml are not enough:

    FAIL ERIO.ne30_g16_rx1.A.anvil_intel RUN time=15     FAIL ERP_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=1943     FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=20     FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=20     FAIL HOMME_P24.f19_g16_rx1.A.anvil_intel MODEL_BUILD time=101     FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=345     FAIL SEQ.f19_g16.X.anvil_intel RUN time=11     FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=1773     FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=126     FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=19     FAIL SMS.ne4_ne4.FC5AV1C-L.anvil_intel.cam-cosplite RUN time=147     FAIL SMS_P12x2.ne4_oQU240.A_WCYCL1850.anvil_intel.allactive-mach_mods CREATE_NEWCASE     FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=11

  These are (some? all?) of the non-clm, non-cam tests listed in   cime/config/e3sm/tests.py for e3sm_developer.  And it appears they are   failing with the 'area' and 'aream' AVects, too:

    0: MCT::m_AttrVect::indexRA_:: ERROR--attribute not found: "area" Traceback:       0: aa area ->MCT::m_AttrVect::indexRA_     0: MCT::m_AttrVect::indexRA_:: ERROR--attribute not found: "aream" Traceback:       0: aa aream->MCT::m_AttrVect::indexRA_     ...

  ...from the run log.  Hopefully, the CIME guys will figure that out while   adding in support for the optional_stubs, but if not I may have to dig   through again and figure out how default area and aream are set for stub   components.

  It might be that the cam and clm test cases don't do any actual coupling,   while these do.  That doesn't sound likely - there are tons of land cases   in e3sm_land_developer, for one thing, and what are they all testing?  So   maybe it's something else?

! Also, be careful - I might be crashing out because I'm running too many   jobs on anvil, and they are stuck "pending" until they are killed or   something, before actually FAILing the run.  So it might be that clm and   cam are not working with _SIAC after all.

  I need to wait until tomorrow, then find the command to tell me what jobs   I have scheduled, and if none are there but I'm still listing some as   PEND then I need to resubmit.

! Anyway, the 'area' and 'aream' stuff might still be affecting me, but for   now I'ma wait until the CIME folks get their optional stub stuff ready;   perhaps some of the stuff Gautam has been pushing up takes care of this   initialization or something.

4/5/2019

* Trying to work with Gautam's rebased version, in E3SM_gb.  He added _SIAC   to all the (clm) compsets, but that's because he didn't realize I'd   modified case.py to have an optional 'siac' stub.  So, in order for the   non-clm tests to work, I backed out the SIAC tag on all compset files,   and added case.py back in - but there have been some additional issues to   deal with from that:

1 E3SM_gb/cime/scripts/lib/CIME/XML/component.py, line 203, it looks like   we need to add 'iac' to the components that may not have a description @ For these unit tests, that's probably okay, but we may need to do   additional work to makes sure we use the description if it *does* exist. 

3/14/2019

* Okay, so I'm not really getting how to run ddt - I can't figure out what   goes in each element to make it submit the case and connect to it.  When   I put in the qsub line I get from ./preview_run, it runs the job but it   never seems to enter the debugger.

  It's possible that I'm using the wrong software in my ~/soft-ddt.sh file,   which is what we load when we start up a new remote ddt sesssion.  To   check this, here is a typical compile line from the mct build:

===================   mpif90  -c  -I. -I../ -DLINUX -DNDEBUG -DMCT_INTERFACE -DHAVE_MPI   -DFORTRANUNDERSCORE -DNO_R16 -DCPRINTEL -DHAVE_SLASHPROC -DSYSLINUX -DCPR   -convert big_endian -assume byterecl -ftz -traceback -assume realloc_lhs   -fp-model source  -O2 -debug minimal  -free   -I. -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/include   -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/MCT/noesmf/c1a1l1i1o1r1g1w1e1i1/include   -I/soft/spack-0.9.1/opt/spack/linux-centos6-x86_64/intel-17.0.0/netcdf-4.4.1-gpk22cidfgknxbc6wjuimdkqifhfhg2j/include   -I/soft/spack-0.9.1/opt/spack/linux-centos6-x86_64/intel-17.0.0/parallel-netcdf-1.7.0-zmjpi4rqpzhvul5o5alk2a2ytgxrrxp6/include   -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/include   -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/util   -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/include   -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/RandNum/include   /blues/gpfs/home/shippert/E3SM_dev/cime/src/externals/mct/mpeu/m_mpif.F90  ====================

* So, I see intel-17.0, netcdf-4.4.1, parallel-netcdf-1.7.0, all built for   intel-17.  Add in mvapich2, some softenv | grepping, and looking for the   acme builds...

  +pnetcdf-1.7.0-intel-17.0.0-mvapich2-2.2-acme   +mvapich2-2.2-intel-17.0.0-acme   +netcdf-c-4.4.1.1-f77-4.4.4-intel-17.0.1-mvapich2-2.2-parallel-acme


3/13/2019

* So, the goal for today is to get the debugger working and start running   through one of the test cases.  There was some discussion on the   confluence page for anvil about this - also, I obviously need to figure   out the interactive queue.

* Debugging documentation (ddt remotely):

  https://acme-climate.atlassian.net/wiki/spaces/SE/pages/178847811/Debugging+on+blues+with+ddt   https://www.allinea.com/user-guide/forge/ConnectingtoaRemoteSystem.html

  Some interesting notes abouut how to build and run debugging:

==============================

  1) create a small case, and modify the pelayout (to run on fewer tasks,   and make a more interesting pelayout, in which coupler pes are disjoint   from atm pes) 

  /lcrc/project/ACME/iulian/ACME/cime/scripts/create_newcase \     --case /lcrc/project/ACME/iulian/CASES/SMALL4 --res ne4_ne4 \     --compset FC5AV1C-L --mach blues

  cd  /lcrc/project/ACME/iulian/CASES/SMALL4

  ./xmlchange --id NTASKS --val 4   ./xmlchange --id ROOTPE_CPL --val 2   ./xmlchange --id NTASKS_CPL --val 2    ./xmlchange --id NTASKS_ATM --val 2   ./xmlchange --id STOP_N --val 1   ./xmlchange --id NTASKS_ESP --val 1   ./xmlchange --id DEBUG --val TRUE

  ./case.setup

  ./case.build

  verify the launch: 

  ./preview_run

  [iulian@blogin4 SMALL4]$ ./preview_run    BATCH SUBMIT:    case.run -> qsub -l walltime=03:00:00 -A ACME case.run

   MPIRUN: mpiexec -n 4 /lcrc/project/ACME/iulian/acme_scratch/SMALL4/bld/acme.exe >> acme.log.$LID 2>&1

  verify that you can launch the case, with  ./case.submit ==============================    * Then you launch ~/allinea/forge/ddt and follow the web page to connect to   the remote ddt, etc.  I really hope it works, because otherwise I'll have   to launch from X11.

* Anyway, an ./xmlchange call is how you set stuff up to run in debug   mode.  There doesn't seem to be an interactive queue, but so far my unit   tests haven't taken long to start up, so there you go.

* The ddt client download page was annoyingly difficult to find - ultimate   it was:

  https://developer.arm.com/products/software-development-tools/hpc/downloads/download-arm-forge/older-versions-of-remote-client-for-arm-forge

  Note that "arm" is not my ARM, but allinea something whatever that leads   to ARM as an acronym for this SW development firm.  Whatever.

* I should look up totalview...huh, we *do* have totalview on blues.  I'm   going to have to do some googling tomorrow to figure out how to do all   this debugging stuff, but let's try it with ddt first and see if that   gets us where we need to go.

3/12/2019

* Okay, the initial model build, at least, fails because of improper mpas   building, I think - it can't find some mpasli files.  dev.master has no   problem there, I think, but dev does - and a straight up git submodule   update --init seems to fail in dev.  So:

  Try to clone and build dev again:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_dev2   cd E3SM_dev2   git checkout shippert/cpl/add-gcam   git submodule update --init

  ...well, that seems to have worked!  Okay, let's try and do unit testing   with dev2.

  NOPE - it did not work - we get this error at the end:

---------- fatal: reference is not a tree: acf0ccb6273a950022484bdb33dca1d406101bc6 fatal: reference is not a tree: c35922ee6d866c990f52588d195d9405563fdd80 fatal: reference is not a tree: 1d80ca5d7d434df3d51b1ca68177dd91440eecbd Unable to checkout 'acf0ccb6273a950022484bdb33dca1d406101bc6' in submodule path 'components/mpas-cice/model' Unable to checkout 'c35922ee6d866c990f52588d195d9405563fdd80' in submodule path 'components/mpas-o/model' Unable to checkout '1d80ca5d7d434df3d51b1ca68177dd91440eecbd' in submodule path 'components/mpasli/model' ----------

  I do not know what that might mean.

* So, take two: I'ma copy directly the dev.master mpas stuff over to dev,   and *then* submit a unit test, again.  Straight up:

  cd ~/E3SM_dev.master/components   cp -r mpas* ~/E3SM_dev/components/  

* New fails:

============================   E3sm_dev

    FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=12     FAIL ERS.f45_g37_rx1.DTEST.anvil_intel RUN time=23     FAIL ERS_IOP4c.f19_g16_rx1.A.anvil_intel RUN time=11     FAIL ERS_IOP4c.ne30_g16_rx1.A.anvil_intel RUN time=12     FAIL ERS_IOP4p.f19_g16_rx1.A.anvil_intel RUN time=12     FAIL ERS_IOP4p.ne30_g16_rx1.A.anvil_intel RUN time=10     FAIL ERS_IOP.f19_g16_rx1.A.anvil_intel RUN time=12     FAIL ERS_IOP.f45_g37_rx1.DTEST.anvil_intel RUN time=29     FAIL ERS_IOP.ne30_g16_rx1.A.anvil_intel RUN time=9     FAIL ERS_Ld5.T62_oQU120.CMPASO-NYF.anvil_intel RUN time=30 *   FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=74     FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=50     FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=12 *   FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=46 *   FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=48     FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=41     FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=13     FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=31     FAIL SMS.T62_oQU120_ais20.MPAS_LISIO_TEST.anvil_intel RUN time=52  
============================   E3SM_dev.master

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=42     FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=49     FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51 !   FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39 !   FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=44

============================

* Okay, well, that's fun - a couple dev.master fails now pass for dev (+   copied mpas).  I honestly have no idea what is going on there.  Should I   rerun dev.master?  Could there have been a system issue?

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=40     FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=48     FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51     FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39     FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=45

  Okay, a rerun fails the same way.  That's kind of amusing - so my dev   branch actually got a couple tests to pass where they failed before.

3/11/2019

* I believe the master commit that I branched off of was 'ab81d6e'.

  I get this from this:

  git config --global alias.hist "log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short"   git hist

  ...to setup a pretty printing format.  'ab81d6e' was the commit just   prior to my initial commit.

* So, to get E3SM_dev.master:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_dev.master   cd E3SM_dev.master   git checkout ab81d6e   git submodule update --init

* That seems to have worked!

* Here are the E3SM_dev.master and E3SM_dev fails for unit tests:

  $ ./cs.* | egrep -i 'fail '

====================================   E3SM_dev.master

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=42     FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=49     FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51     FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39     FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=44 ====================================   E3SM_dev

    FAIL ERS.f09_g16_g.MPASLISIA.anvil_intel MODEL_BUILD time=14     FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=53     FAIL ERS.f45_g37_rx1.DTEST.anvil_intel RUN time=29     FAIL ERS_IOP4c.f19_g16_rx1.A.anvil_intel RUN time=17     FAIL ERS_IOP4c.ne30_g16_rx1.A.anvil_intel RUN time=26     FAIL ERS_IOP4p.f19_g16_rx1.A.anvil_intel RUN time=20     FAIL ERS_IOP4p.ne30_g16_rx1.A.anvil_intel RUN time=20     FAIL ERS_IOP.f19_g16_rx1.A.anvil_intel RUN time=18     FAIL ERS_IOP.f45_g37_rx1.DTEST.anvil_intel RUN time=29     FAIL ERS_IOP.ne30_g16_rx1.A.anvil_intel RUN time=19     FAIL ERS_Ld5.T62_oQU120.CMPASO-NYF.anvil_intel MODEL_BUILD time=22 *   FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=53     FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=60     FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=20 *   FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=65 *   FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=83 *   FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel MODEL_BUILD time=756 *   FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=53     FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=18     FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=36     FAIL SMS.T62_oQU120_ais20.MPAS_LISIO_TEST.anvil_intel MODEL_BUILD time=21


====================================

* So, the fact that we fail at run time for five dev.master runs is not   great, but at least we have others we can try to debug.  There are 21   debug fails, out of 36 tests - if I had to guess, I'd say the other 15   were simple, non-coupled tests.  I should probably check the status log   of one of them vs. dev.master, too.

  But, ultimately, it might make sense to port up to current E3SM (merge   with ~/E3SM) before we get finished with unit testing.

* I'm curious about the model_build fails - there's probably something   interesting to learn there.  Note that one of the dev.master run fails   is a dev build fail, too.

* So:

1 check model_build fails 2 figure out how to use the new debugger 3 find a coupled test that we are failing, above, and debug that.

  Hopefully, these three steps will get us most of the way to fully debugged.

2/14/2019

* Man, I keep barking up the wrong tree - I thought I found where we used   the pftdyn file in BiogeophysRestMod.F90, but that's for restarts, not in   general.  (I'm sure I'll have to modify that, too, except clm in   ACME/E3SM doesn't have such a file...)

  Anyway, grepping for "surfdata" seems to be closer to what we want - but   the annoying thing is that the surface data file name is in the namelist,   so it's really hard to track down and verify I'm looking at the right   thing from the code.  I keep going back and forth with the sample run   Kate gave me, but it's like chasing your tail sometimes.  Or, more like   following three forks of a river at once.

* Anyway, I'm now reviewing main/surfrdMod.F90 to see if that may give me   some clues as to how the stuff that goes into   surfdata_360x720_mcrop_dyn.nc is actually used by clm.

2/11/2019

* Okay, that docker class didn't do me any favors.  Trying to recover and   get back to where I was.  Right now I'm trying to figure out what I need   to write back out for z->l - i.e. the iac -> lnd fields.  Those are in   the pftdyn file, which means I need to find the clm code in iESM that   reads that file so I can tell what is important.

* From there, I need to grab those values at the time we are writing them   to pft, and also then pass them back up to be stuffed into an MCT-style   avect. 

* So, first things first - 

1/22/2019

* Okay, so tracking down some details, it looks like the surface landuse   stuff is written out to the "pftdyn" file, which is called   surfdata_360x720_mcrop_dyn.nc in updateannuallanduse() (hardcoded).   It appears to take the surfdata_360x720_mcrop.nc landuse file, which   gives data only for 2005, and updates it to give yearly values in the   *_mcrop_dyn.nc version.

  My guess is that clm normally reads the *_mcrop.nc to get 2005 landuse   values, and then runs some simple extrapolation or parameterization to   take it to the current run year, and does that instead.  With gcam, we   instead run our more sophisticated landuse calculation and send that to   clm via this dyn file.

  So: original: surfdata_360x720_mcrop.nc         w/gcam: surfdata_360x720_mcrop_dyn.nc

* gcam2emissfile_run_mod() writes out co2flux_iESM_dyn.nc, again hardcoded,   which seems to contain monthly values of the co2_flux. From the ncdump:

	float CO2_flux(time, lat, lon) ; 		CO2_flux:long_name = "CO2 fossil fuel emission flux" ; 		CO2_flux:units = "1e3 g m-2 s-1" ; 		CO2_flux:_FillValue = -999.f ; 		CO2_flux:standard_name = "tendency_of_atmosphere_mass_content_of_carbon_dioxide_due_to_emission      So, this suggests that this is an extra impulse to co2flux, to be added   to that from the clm calculation, rather than folded into that.  At   least, at this point in the iac run it is - maybe iESM folds it in later,   but right here is the actual z2a_z flux I need to grab.

? (I really do need to see how this gets sent up the chain in iESM, though

* In principle, this is great, we have tracked down where this stuff is   getting written out, and how it's getting passed to the other mods, which   lets me simply grab the data at that point and pass it back up the chain   to iac_run_mod(), which can stuff it into the appropriate coupled   AVects. 

* Issues:

1 still need to write out files, because gcam/glm use them internally, at   the very least for its own calculations.  Both pftdyn and dynco2fluxfile   are read first by their functions, and then written out.

2 How to pass this information back through the calling chain.  At least   gcam2emiss is in fortran, so it can set module variables or something to   get back to a place where I build my AVects.  updateannuallanduse() is in   C, so I probably need to modify the fortran wrapper to get a C structure   containing the landuse info and reformat it into something I can use to   build the AVects.

3 how do AVects deal with different shapes of coordinate variables?   co2flux is on a strict lat,lon grid.  landuse appears to be indexed by   "column", with the column parameterized to lat,lon values.  I need to   figure out how MCT handles this kind of thing - at the very least, I'll   need to pass along the column index -> lat,lon information somehow (in   it's own AVect?  Are their such things as auxilliary or coordinate   attribute vectors?  Or does an Attribute Vector structure itself have the   coordinates and indexing info built in?  Review how clm sends information   to other components, and see how they handle their column-based indexing   there.

4 I'm pretty sure this the only way iac interacts with atm, but is there   any other feedback to lnd other than landuse?

-----------------

* Wrt (3), above, check decompMod.F90 in the clm/src/main directory - it   defines a way to convert "clumps" back to atmos physics chunks.  There's   a lot of info on number of clumps, gridcells, columns, pfts, etc, all   used to define the coordinates of lnd values.  A lot of this stuff has to   do with the processor decomposition, which is less of an issue for me,   but since I'm dealing with land style column variables, it seems like I   should be able to follow what they are doing there.

  So now consider lnd_import_export.F90, which is how we go from   lnd2atm_vars to l2x() arrays.  It really looks like it's just   gathering at the start of lnd_export() - from the proc-based 'g' gridcell   index to the global 'i' based index.

  So it really looks like it can handle a gridcell and/or column based   coordinate system - at least, the structure of bounds_type seems to argue   that. 

! Also, check out the sign convention comment:

       ! sign convention is positive downward with         ! hierarchy of atm/glc/lnd/rof/ice/ocn.  so water sent from land to rof is positive

?  ?  I'll have to think about that - so co2 from iac to atm is negative? ? 

1/18/2019

* Whew!  We are finally getting somewhere.  Here is the calling tree:

* clm calls iac_run_mod(), which calls, in order, given the appropriate   alarms:

1 iac2gcam_run_mod()             (clmC alarm) 2 gcam_run_mod()                 (gcam alarm) 3 gcam2emissfile_run_mod()       (as above, plus co2flux and emiss options) 4 gcam2glm_run_mod()             (glm alarm) 5 glm_run_mod()                  ("")

6 glm2iac_run_mod()              (""), calls: a   updateannuallanduse_main()         calls:        ... o      calcchurtt() o      writepftdynfile() 

* So, finally I see where the file is created that talks back to clm from   gcam, I believe.

? I don't know if that's *all* that gcam does for clm - update the landuse   vars - or if there is something else.  But these are the only primary   functions called by iac_run_mod() - everything else is logging or timing   or transforming arrays or something like that.  (I *think*).

* Okay, so my belief that the pftdynfile is important comes from a comment   inside of lnd_comp_mct.F90, line 568: see 1/17/2019, below, bullet 5.

* From updateannuallanduse.c, I see that the pftfile written out by   writepftdynfil is called: surfdata_360x720_mcrop_dyn.nc.  And we have one   of those in Kate's test directory:

  /pic/projects/iESM/iesm_model_dir/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001/run

* So, what is apparently happening is that surfdata_360x720_mcrop.nc (no   _dyn) is an exisiting netcdf file that has information about crop and ? landuse for the year 2005.  I believe that this is used in clm as a   jumping off point, and is probably the basis of some parameterization or   simple extrapolation for land and crop use as we go forward.  But iac   actually *models* land and crop use, so what it does instead is this:

1 take the single-sample surfdata_360x720_mcrop.nc file 2 copy to surfdata_360x720_mcrop_dyn.nc 3 each gcam model year, append new land and crop use information to this   file 

  So if you look at surfdata_360x720_mcrop_dyn.nc, it has the same fields   and dimensional shape and everything, except it goes from 2005 to 2092,   following the TIME coordinate variable (shame on them for capitalizing   it, but whatever.)  There *are* duplicate values for some TIMEs - for   example, there are four values with a TIME of 2071.  My guess is   restarts?  Or bug fix or system barf reruns?  Anyway, it probably appends   to this _dyn file if it can find it, so any kind of rerunning may lead to   duplicate days.  The file is huge with lots grid points to work through,   so it's hard to see if all four 2071 TIME samples are identical or not.   But that's not really important, so whatever.

* Okay, so I need to figure out what clm uses from this file, and if there   are any other files that iac modifies that it may use.  So, I need to   work through updateannuallanduse.c, which is 80 pages of code, with lots   and lots of comments and sections that says "we don't use this in iESM".

* updateannuallanduse.c is actually a C program, written for some other   purpose, crammed into iESM - it's actually called via a fortran wrapper   function by glm2iac_run_mod(), rather than by gcam itself.  But I need to   figure out what it's doing, and how it is doing it, before I can figure   out how to make my iac code provide the same information in E3SM.

* It looks like writepftdynfile() mostly copies the   outhurtpftval[inpft][outgrid] array values into the mcrop_dyn.nc file:   See c. line 5287.  

  calchurtt() creates outhurttpftval[][], so we need to follow that   calculation, and maybe some of the ones upstream of it, to extract where   this information comes from.  Or, not?  Maybe outhurttpftval[pft][grid]   are the exact values I need to stuff into an attribute vector - instead   of writepftdynfile() instead call a returnpftdynavect() or something.

* Okay, back to clm - it looks like there is a pftdynMod.F90 module, and   clm_driver.F90 (as well as other files) all hit on a grep for "pftdyn". @ So probably should work through clm_driver.F90, figure out how this   pftdyn file is read in, and that should give me some clues as to how to   use MCT to set the same information back from iac in E3SM.

1/17/2019

* From the phone call with Kate:

* As it turns out, we *only* use HR and NPP on input - the other   carbon-related vegetation fields ultimately were not used in this version   of gcam (I got the impression it didn't accomplish what they want, so it   probably won't in the future, either).

* Here is an iESM run, which will let me do things like examine the clm.h1   file etc:

  /pic/projects/iESM/iesm_model_dir/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001

   An example of the outputs, with history files, is available here:    /pic/projects/iESM/rcp85_results/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001 

* From this, we see that the dimensions are HR[time,column] and   NPP[time,pft].  This means the only place we get any kind of location   information is in the HR - column is a 1D parameter of a 2D location (a @ grid column, obviously).  So I gotta figure out how to make MCT work with   a parameterized location like this - it can't be all that hard, because   obviously translations between columns and grid coordinates have to   happen all the time in a climate model.

* I haven't yet been able to track down how (or what) iac sends back to clm   in iESM.  It look like the "clm pftdyn file" contains the output from   iac, as per line 568 in lnd_comp_mct.F90 (in iESM).  I'ma look for it in   the iESM run above, and see if I can backtrack where it is created - I   don't really see an nf90_create anywhere else in the whole clm tree in   iESM.  I might be missing something, or maybe instead it cracks open an   existing file and updates it?  Kate said something like that, so clm uses   standard tools to output a history file (thus avoiding naked nf90_create   calls), and then iac/clm uses nf90_open instead.

  I'm fishing.  My greps for nf90_create and nf90_open haven't been all   that illumniating on this issue, but maybe I'm just missing it.

* Co2 flux will be separate from lnd for E3SM, but in iESM it might have   been merged into what lnd sends back.  If so, finding how iESM folds iac   into clm is gonna be a little more subtle than if it dumped iac results   to a file or something.  Then again, maybe it's just merged right before   filling the output attribute vector.  I kind of need to see examples of   filling in output attvects, anyway, even if it's for iESM.

1/15/2019

* As I've previously encountered, teasing out exactly what information I'm   supposedt to pull out of clm/lnd and put into an attribute vector is   excruciating - there are zillions of little calls to nf90_inq_varid()   floating around.  Some are for writing output (at the end of   calc_clmC()), some for opening a "base" file, some obviously for cracking   a history file, etc.  I need to track down what each of these things are   for. 

  To do this, how about something like this:

  $ egrep 'nf90_open|nf90_inq_varid|subroutine' iac2gcam_mod.F90

  The output below is lightly formatted, to highlight different read   sections and different subroutines.


==============================


  subroutine iac2gcam_init_mod( EClock, cdata, iaco, gcami)     status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncidbase)     status = nf90_inq_varid(ncidbase, "abovg_c_mean_pft", base_abovg_c_mean_pftVarId)

    status= nf90_open(trim(clm2gcam_mapfile),nf90_nowrite,ncid)     status = nf90_inq_varid(ncid, "CCSM_ID", CCSM_IDVarId)     status = nf90_inq_varid(ncid, "Country_AEZ_ID", Country_AEZ_IDVarId)     status = nf90_inq_varid(ncid, "GCAM_ID", GCAM_IDVarId)     status = nf90_inq_varid(ncid, "Weight", WeightVarId)   end subroutine iac2gcam_init_mod

-----------------------

  subroutine iac2gcam_run_mod( EClock, cdata, iaco, gcami)     status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncidbase)

       status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncid)        status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)        status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)        status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)           status = nf90_inq_varid(ncid,'npp_mean_pft',varid)           status = nf90_inq_varid(ncid,'hr_mean_pft',varid)

    status = nf90_inq_varid(ncidbase, "area", areaVarId)     status = nf90_inq_varid(ncidbase, "landfrac", landfracVarId)     status = nf90_inq_varid(ncidbase, trim(var1name), base_var1_mean_pftVarId)     status = nf90_inq_varid(ncidbase, trim(var2name), base_var2_mean_pftVarId)     status = nf90_inq_varid(ncidbase, "pft_weight_mean_g", base_pft_weight_mean_gVarId)   end subroutine iac2gcam_run_mod

------------------

  subroutine calc_clmC(yy,mm,bfn,out_pft_weight,out_abovg_c,out_blowg_c,out_npp,out_hr,calc_avg)

        ! Read restart         status= nf90_open(filename,nf90_nowrite,ncid)

        status = nf90_inq_varid(ncid,'year',varid)         status = nf90_inq_varid(ncid,'month',varid)         status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)         status = nf90_inq_varid(ncid,'wcnt',varid)         status = nf90_inq_varid(ncid,'abovg_c_max_pft',varid)         status = nf90_inq_varid(ncid,'blowg_c_max_pft',varid)            status = nf90_inq_varid(ncid,'npp_max_pft',varid)            status = nf90_inq_varid(ncid,'hr_max_pft',varid)         status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)         status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)            status = nf90_inq_varid(ncid,'npp_mean_pft',varid)            status = nf90_inq_varid(ncid,'hr_mean_pft',varid)         status = nf90_inq_varid(ncid,'cnt3',varid)

     status= nf90_open(trim(filename),nf90_nowrite,ncid)

        ! If first call - so these are coordinates          status = nf90_inq_varid(ncid, "lon", varid)         status = nf90_inq_varid(ncid, "lat", varid)         status = nf90_inq_varid(ncid, "area", varid)         status = nf90_inq_varid(ncid, "landfrac", varid)         status = nf90_inq_varid(ncid, "pfts1d_ixy", varid)         status = nf90_inq_varid(ncid, "pfts1d_jxy", varid)         status = nf90_inq_varid(ncid, "pfts1d_itype_veg", varid)         status = nf90_inq_varid(ncid, "pfts1d_itype_lunit", varid)         status = nf90_inq_varid(ncid, "cols1d_ixy", varid)         status = nf90_inq_varid(ncid, "cols1d_jxy", varid)         status = nf90_inq_varid(ncid, "cols1d_itype_lunit", varid)

     ! Read in always, so these are our state variables      status = nf90_inq_varid(ncid, "CWDC", varid)      status = nf90_inq_varid(ncid, "TOTLITC", varid)      status = nf90_inq_varid(ncid, "TOTSOMC", varid)         status = nf90_inq_varid(ncid, "HR", varid)      status = nf90_inq_varid(ncid, "DEADCROOTC", varid)      status = nf90_inq_varid(ncid, "FROOTC", varid)      status = nf90_inq_varid(ncid, "LIVECROOTC", varid)      status = nf90_inq_varid(ncid, "TOTVEGC", varid)         status = nf90_inq_varid(ncid, "NPP", varid)      status = nf90_inq_varid(ncid, "pfts1d_wtgcell", varid)

     ! This is the iac_clmC_file, looks like a history file         status= nf90_create(filename,nf90_clobber,ncid)         status = nf90_inq_varid(ncid,'lon',varid)         status = nf90_inq_varid(ncid,'lat',varid)         status = nf90_inq_varid(ncid,'PFT',varid)         status = nf90_inq_varid(ncid,'area',varid)         status = nf90_inq_varid(ncid,'landfrac',varid)         status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)         status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)         status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid) 	   ! if calc_avg            status = nf90_inq_varid(ncid,'npp_mean_pft',varid)            status = nf90_inq_varid(ncid,'hr_mean_pft',varid)

        ! Restart file         status= nf90_create(filename,nf90_clobber,ncid)         status = nf90_inq_varid(ncid,'lon',varid)         status = nf90_inq_varid(ncid,'lat',varid)         status = nf90_inq_varid(ncid,'PFT',varid)         status = nf90_inq_varid(ncid,'area',varid)         status = nf90_inq_varid(ncid,'landfrac',varid)         status = nf90_inq_varid(ncid,'year',varid)         status = nf90_inq_varid(ncid,'month',varid)         status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)         status = nf90_inq_varid(ncid,'wcnt',varid)         status = nf90_inq_varid(ncid,'abovg_c_max_pft',varid)         status = nf90_inq_varid(ncid,'blowg_c_max_pft',varid)            status = nf90_inq_varid(ncid,'npp_max_pft',varid)            status = nf90_inq_varid(ncid,'hr_max_pft',varid)         status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)         status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)            status = nf90_inq_varid(ncid,'npp_mean_pft',varid)            status = nf90_inq_varid(ncid,'hr_mean_pft',varid)         status = nf90_inq_varid(ncid,'cnt3',varid)   end subroutine calc_clmC ==============================

* Okay, that at least clarifies which calls are for which function, and   from which file (at least, a way to track it back.)  The time and   lat/lon grid should be available in another way

1/14/2019

* Gotta transfer some of my scrawlings back to here again.

* Trying to setup the coupling indices, which have names like this:

  index_x2r_Flrl_foo or index_l2x_Sl_bar

  This lets us figure out which element of the attribute vector is which,   so if gcam is looking for _foo from some input it has the index set.   Most of this is automatic and internal, you just need to set up the links   between what field name you want to associate with which index. The way   that is done is via the seq_fields_x2r_fields stuff in   ./cime/src/drivers/mct/shr/seq_flds_mod.F90.

* I've looked that over at least twice since I started, and now it's time   to figure it out.

  It seems like I haven't modified it at all, even though it's a coupler   function.  Huh - I didn't need it for the stub, although, maybe I do, and   that's why we've seen run errors (modulo the debugger issues)?  Hmm.

  Anyway, it looks like I need to add in iac and the 'z' component   everywhere in that mod.

  No, I did mod seq_flds_mod.F90 in October - what did I do?  It looks   like I added some seq_flds_x2z, z2x _fluxes and _states vars, but did not   add the component names or anything further on?

! seq_flds_mod.F90

* Okay, working on this - I'm going to assume I have both states and fluxes   for now, but it's possible I only have states.  This is where I'm weak,   trying to figure out the actual science of the code and how it relates to   these variables and stuff, and unfortunately so far I haven't had much   luck inferring everything from iESM.

  Interestingly, they seem to allow customized coupler fields through the   namelist mechanism - if I'm reading this right, maybe you could decide   at run time to add some more info to the coupler?  Interesting, but   pretty advanced for my purposes....  Still, I'm going to add in the x2z,   z2x options.

* Next, we define domain coordinates, lat, lon, hgt, area, etc.

! Okay, this is a little hard to get my head around - it *seems* like, in   this file, seq_flds_mod.F90, we are building up lists of strings that   describe the various quantities that we need to toss around between   different components and and the coupler.  So, for example, here are all   the Surface Latent Heat Fluxes, that need to get sent into the atmosphere   and ocean components, *from* the ice and lnd components (line c. 1218 of   seq_flds_mod.F90).

     call seq_flds_add(l2x_fluxes,"Fall_lat")      call seq_flds_add(xao_fluxes,"Faox_lat")      call seq_flds_add(i2x_fluxes,"Faii_lat")      call seq_flds_add(x2a_fluxes,"Faxx_lat")      call seq_flds_add(x2o_fluxes,"Foxx_lat")

  ...followed by some calls to "metadata_set" to link the names above with   various attributes like units and longname and what not.

  So, looking at this, here's what I believe these mean: set Fall_lat to   the latent heat fluxes coming from the lnd component, Faii_lat to the   latent heat fluxes coming from the ice component, and Faxx_lat and   Foxx_lat for the latent heat fluxes sent to the atmosphere and ocean   components.   I think the xao stuff is a little more complicated, having   to do with some kind of joint calculation between atm and ocn   components (maybe at the boundaries?).  Anyway, all the Fa stuff will be   used by the atm component, the Fo will be used by the ocm component.

  Okay, at the top of seq_flds_mod.F90 we what the letters mean:  Faox_   means "flux between atmos and ocean computed by coupler".  So things like   Faxx_lat mean "latent heat flux, between atmos and coupler (!) calculated   by coupler".  I'm afraid I don't understand what flux between a component   and teh coupler might mean...

  Anyway, there's some merging and scaling rules, but they all seem to be   associated with atm component, which makes sense - that's global, and   everything else is not, so the lndfrac, icefrac, ocnfrac scaling issues   are important there.  For going between lnd and gcam, though, I don't   think scaling is an issue.

  (Does "scaling" mean something like - if over land, lndfrac = 1,   otherwise lndfrac=0, and then multiply by lndfrac everywhere?)

* Okay, clearly I need a list of fields that GCAM pulls out of the land   model and then map those to appropriate Sl_foo states and any fluxes that   might occur.  I'm thinking it's mostly state variables, since gcam is   generally only on land and the boundary overlaps aren't important.  But,   maybe the carbon dioxide output is a Fzaz_co2 variable or something?

* Also, searching for carbon gives some interesting hydrophilic and   hydrophobic deposition fluxes between ice and ocean, and atmos and   coupler (!) (Faxa_...).  So "deposition" suggest this is carbon that   comes out of the first component and into the second - dropping out of   the air, or melting from the ice.  I still don't understand what "flux   between atmos and coupler" means, though - does "coupler" mean *global*   coverage or something?  Or does the coupler do some actual   calculations with Fmxn_ style fluxes?

* There are also carbon concentrations - So_doc1, So_doc2, e.g. for ocean.

* Then, "sea ice dissolved organic carbon flux", Fioi_doc1 -  okay, I'm   getting those, that's carbon released into the ocean from sea ice   melting.

* Now, if flds_co2b is set, we do "Fall_fco2_lnd", "surface upward flux of Co2   from land".  The flux between the lnd component and the atmos component,   to represent carbon being released into the atmos from the lnd   component.  So, this suggest maybe "Fazz_co2" is how I send co2 fluxes   back from gcam, probably to merge with the values coming from lnd.

  Here is a tricky part, then, scientifically - the lnd component is source   of carbon up into atm.  Right now, that's just from the ground cover,   right - trees decaying, that kind of thing.  So gcam needs to account for   economic (human) development in the same way, but also may change the lnd   flux - if we cut down trees to make a city, that's less decaying co2 but   more anthropogenic co2.  This suggests a carbon "flux" between lnd and   iac, but I'm not sure it's a flux in the way E3SM uses the term.  Maybe   just a state change?  Does flux mean "go across geographic boundaries   between the physical domains of the components?"  But what about river   runoff and stuff like that - any Flrl_ or Flrr_ fluxes?  I'm sure there   are. 

* Okay, now more complications - we do different things with flds_co2a   vs. flds_co2b vs. flds_co2c, etc, which are gonna be namelist options.   These are obviously some kind of different options for different ways of   modelling co2, and there is some overlad (Fall_fco2_lnd is in both b and   c, as are the Sa_co2{diag,prog} state variables.

* I'm sure there are other vegetation values, but at least these carbon   fields give me a jumpoff point to trying to map the iESM fields pulled   from the history files to something we can get out of the coupler.

1/5/2019

* Making some progress - have a working iac_comp_mct.F90 file, although   some subfunctions and modules will need to be written.  Built almost   entirely by copying rof_comp_mct and lnd_comp_mct - I've kept all the   generic looking parts, but I still dont' completely grok all the   intricacies of MCT yet, so I might be including some stupid or foolish   stuff. 

* In particular, my understanding of "domain" calculations are how you   divvy things up amongst your processors.  GCAM currently is a single proc   component, but I've kept all that domain infrastructure in case (a) my   understanding is wrong and domain does something else; (b) we go to   multiprocessor versions of GCAM someday; and (c) so that the calling   structure is the same for iac - set up gsmap, setup domain, that kind of   thing.  I think it's likely that all the gsgrid stuff is required by MCT,   and there very well may need the dom_z indicator in how MCT calls iac.

* Okay, here's some stuff I need to write, either as part of the IAC   coupler code or elsewehre:

1 Modules: @ iac_mod module, element iac (re: RunoffMod) @ gcam_var module, with lat lon, log, startup, instance, active logicals   (re: RtmVar)

2 functions, either in above or other modules: @ gcam_cpl_indeces_set() (re: rtm_cpl_indeces_mod) @ gcam_mpi_init() (re: RtmSpmd_mod) @ gcam_var_set() (probably gcam_var module) @ gcam_init() @ iac_run_mod() - modification of existing function



12/28/2018

* Modules and functions I need to review, and see where they are in clm and   whether we need them in iac_comp_mct.F90, iac_run_mct():

* clm_instMod, clm2atm_vars, etc.   clm_driver, clm_drv   clm_time_manager   clm_varctl   clm_varorb

* seq_cdata_setptrs() gets the cdata from the coupler.  So, how do the   cdata_z (e.g.) structures get initialized in the first place?  There's a   legacy seq_cdata_init() function in seq_cdata_mod, but it is apparently   only used for the data models.  Does the (e.g.) lnd_init_mct() thing set   up cdata?

* Okay, the component_types, which are named just ike components directly   (e.g. 'iac', 'lnd') appear to be the overall structures that contain all   the information.  The accessor to get the "cdata" things is   component_get_cdata_cC(), which just looks for comp%cdata_cc.  So, for   example, we need to fill in iac%cdata_cc to find the cdata_z we use in   the iac_run_mct() and iac_init_mct() etc.  Geez, this goes way back up   the chain.

  Look at component_mod.F90 in ~/PIC/ACME/cime/src/drivers/mct/main.

  Well, it seems like there's an overall array of components, and the   cdata_cc stuff just contains communication ids, domain and gsmap   information.  I'm not sure where that stuff gets generated, though....   See line 138 of component_mod.F90.

! Sheesh, down the rabbit hole, and it's still slipping away, even after   all this time.  I guess I'll leave initialization to later, and we'll   puzzle it back together as we need it.

? My big question is what information cdata_z will have?  Is it just   mapping and communication, or is field info and what not?  AVects?  Is it   the same as the cdata structures we see in iESM, or similar, or   completely different?

12/17/2018 2018-12-17 12:58:47

* Notes from iac2gcam_mod.F90

* subroutine iac2gcam_run_mod() - function to run gcam

12/17/2018

* Quick update on all the unit tests over the weekend:

* ACME.master and ES3M *do* fail some unit tests, and they *seem* to be the   same ones, so there is something going on with Constance wrt to those   particular tests.  I need to verify they are failing the same tests, if   they both run them, and then exclude them from the cases I examine for my   ACME build.

  A quick examination of .../create_test.out tells you some of this - I   probably do need to do a quick dump of the TestStats using the cs.*   scripts to get a solid picture of which tests are telling me something   interesting about my ACME build.

* Quick review: ACME is my build, ACME.master is the master for my branch   (hopefully, assuming my git-fu is okay), and E3SM (in ~/test) is the most   recent grab and build of the main E3SM repository, as of last week.

12/14/2018

* Upon spending all night reflecting on this, I've decided a couple things:

1 I don't need to generate baselines normally, they should be correct from   the machine file (probably made as part of deriving the machine file in   the first place).

2 I will run baselines for ACME.master, since it has "(no branch)" and thus   can't figuring out which baselines to use.  I'm going to put those in   /pic/scratch/d3a230/acme_baseline.

3 I might then use those same baselines for my ACME branch runs, when I get   that far, just to compare directly with ACME.master.

* Additional notes:

* I think "baseline" are not actual runs but something to do with comparing   namelist files; if it fails the baseline it means you modified the   namelisting somehow.  That's why we see these NLCOMP fails when trying to   generate baselines into the /pic/project/climate area, where I don't have   permissions. 

* I need to clean up /pic/scratch/d3a230, so I can find things again.  I'm   moving everything into "old" directories, and will work with   /pic/scratch/d2a230/acme, .../acme.master, and .../e3sm subdirectories.

* Huh!  If you give baseline information but not a -g, you have to use -c   to get them to compare.  Which means, probably, that baselines are all   optional.  What a waste of time, then.  I'ma run with -c and the same   acme.master baselines with acme, just in case, but from now on just don't   use -c, -b, -g, or anything related to baselines, and see if everything   workds. 

! So, first up:

@ ACME.master: git submodule update --init @ Run the tests for acme.master, which should work.

  ./create_test -p iesm --test-root /pic/scratch/d3a230/acme.master -g \      --baseline-root /pic/scratch/d3a230/acme.master/acme_baseline \      --baseline-name acme_master \      acme_developer >& ~/ACME.master/create_test.out &

@ Run my acme branch again, using same baselines:

  ./create_test -p iesm --test-root /pic/scratch/d3a230/acme -c \      --baseline-root /pic/scratch/d3a230/acme.master/acme_baseline \      --baseline-name acme_master \      acme_developer >& ~/ACME/create_test.out &

@ Run the e3sm tests, without any baselining

  ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm \      e3sm_developer >& ~/test/E3SM/create_test.out &

? Well, when I got back from lunch my connection to the PIC had dumped out,   and now I can't tell if all these jobs that are pending on the Model   build are actually doing anything or not, or if the compile got   interrupted or something.  What a mess - this is what I get for starting   several of these jobs at once, I can't ps -ef and tell what is going   on...

* Anyway, jeez, the builds take forever to simply compile - my guess is we   don't do any kind of parallel build, so I guess it just has to go through   a lot of work to get there.

! Anyway, the most important thing is that acme.master is FAILING on some   runs!  I see Seg Faults and stuff!  If that's true, it means that my   code, built after that, may *also* be failing just for the same reason.   I'm also seeing build fails on mpas - my conjecture here is that the   submodule stuff is messed up somehow?  That grabbing the most recent   version of mpas makes the builds not work right.

* Anyway, everything is up in the air - I don't want to start new builds on   top of this, so for now I'm going to wait until this evening and check in   on how everything is progressing.  Assuming the E3SM code (which is very   new) passes all it's unit tests, then we'll check and see if any more   progress has been done on the others - if not, we'll have to resubmit   them, this time one at a time, and hope they can run over the weekend.

* Right now, create_test.out has mod dates:

  Constance[ACME]% ls -al create_test.out   -rw-r--r-- 1 d3a230 users 34398 Dec 14 12:25 create_test.out   Constance[ACME]% ls -al ~/ACME.master/create_test.out   -rw-r--r-- 1 d3a230 users 43782 Dec 14 12:34 /people/d3a230/ACME.master/create_test.out

----------------------------   Constance[ACME]% tail create_test.out   Finished SHAREDLIB_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel in 3984.677218 seconds (PASS)   Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5AV1C-L.constance_intel with 4 procs   Finished MODEL_BUILD for test ERS.f19_g16_rx1.A.constance_intel in 1730.722585 seconds (PASS)   Starting RUN for test ERS.f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes   Finished MODEL_BUILD for test SMS.ne30_f19_g16_rx1.A.constance_intel in 1732.463999 seconds (PASS)   Starting RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes   Finished RUN for test ERS.f19_g16_rx1.A.constance_intel in 111.358953 seconds (PEND). [COMPLETED 1 of 38]   Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel with 4 procs   Finished RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel in 107.099653 seconds (PEND). [COMPLETED 2 of 38]   Starting MODEL_BUILD for test ERS.f19_g16.I1850CLM45.constance_intel.clm-betr with 4 procs ===========================   Constance[ACME.master]% tail create_test.out   Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel with 4 procs   Finished MODEL_BUILD for test SMS.ne30_f19_g16_rx1.A.constance_intel in 1775.574343 seconds (PASS)   Starting RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes   Finished RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel in 109.920069 seconds (PEND). [COMPLETED 7 of 38]   Starting MODEL_BUILD for test SMS_Ln9.ne4_ne4.FC5AV1C-L.constance_intel.cam-outfrq9s with 4 procs   Finished MODEL_BUILD for test ERS_IOP4c.f19_g16_rx1.A.constance_intel in 1768.789043 seconds (PASS)   Starting RUN for test ERS_IOP4c.f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes   Finished RUN for test ERS_IOP4c.f19_g16_rx1.A.constance_intel in 116.711359 seconds (PEND). [COMPLETED 8 of 38]   Starting MODEL_BUILD for test ERS_Ln9.ne4_ne4.FC5AV1C-L.constance_intel with 4 procs   Finished MODEL_BUILD for test ERS.f19_g16.I1850CLM45.constance_intel.clm-betr in 4068.002696 seconds (PASS) ---------------------------

  ...so compare against that to see if any more progress on this has been   done.

* The reason why I'm worried about it is that my ps -fu d3a230 seems to   only bring up e3sm activity, but it's hard to tell for sure.  So, wait   for e3sm to finish running and then review.

* ...okay, a couple hours later, and I'm only seeing updates to the E3SM   run.  So that means the ACME and ACME.master runs did stall out, for   whatever reason.  Thus, we need to rerun them, and it takes *many* hours   to run one of these things, so maybe do one per day or something over the   weekend.  I'd like to set up a script to do this, but I am likely to use   the wrong options or whatever, so I'd better check in and do it by hand.   I may even have to come in to work to do this, since I'm not sure why it   hung before and what might happen if (e.g.) my home connection went down   or something.

12/13/2018

* So, I'm trying to revert back to my master branch, in order to run the   unit tests from that.  So I cloned into ~/ACME.master, because I didn't   want to mung anything up, and did a "git checkout master", which,   apparently, didn't do anything.

  Thinking that it's possible that's because my clone points back to ~/ACME   as my master, I then played around a bit and tried this:

  git checkout remotes/origin/master

  ...which seems to have at least grabbed a version of the code without my   iac changes.  So now the question is - was this the (old) version of the   master I originally cloned, or is this possibly the latest version of the   code?  

  It says I'm currently on "(no branch)", a "headless branch", whatever   that means.  Since I don't want to actually do any development over here,   that's fine, but I really wish I understood all this arcane git-fu going   on. 

* For now, I'm going to work under the assumption that it's the master   code that I'm working with.  Grepping around in cime/scripts/* for   "acme_developer" hits, while "e3sm_developer" does not.  That doesn't   prove anything, but I'm encouraged by the "acme"s floating around and   lack of "e3sm"s.

* So, I'm going to try running:

  cd ~/ACME.master/scripts   ./create_test -p iesm -g acme_developer >& ~/ACME.master/create_test.out &

  ... and see what happens.

* Ah, crap, it can't determine the baseline because my branch doesn't make   sense.  But because I don't know anything about what baselines are or how   they should be used, using the -b option like create_test says is   problematic.  

  Okay, run without baseline generation, and hope that somewhere in the   machine config it points to the right baselines.  So far, it appears to   be doing something when you run without -g, so that's good news.

* So, the runs that need fates are failing on setupt, because   /people/d3a230/ACME.master/components/clm/src/external_models/fates/main   doesn't exist - the fates subdir exists, but nothing else is there.  This   is consistent with what I found before - somehow fates (along with mpas,   I think) are checked out as part of setup or building?  Maybe as part of   baselining? 

? Maybe I should do a git checkout master from ~/ACME, and hope that works?   Then it will have the branch correct and we can run the baselines from   that?  I'm reluctant to mess with ~/ACME like that, but I'm running out   of things I know how to do.

* git log tells you the commit messages, so I'm hoping to roll back to a   reasonable branch name with a "git checkout <commit>".

  ...nope, that gets me back to (no branch) again.  Damn.  Maybe a git   clone of the repository, then roll back to this commit?  Sheesh, this is   a nightmare.

* Okay, try this:

  mkdir ~/test   cd ~/test   git clone git@github.com:E3SM-Project/E3SM.git   cd E3SM   git submodule update --init

  ...that should be the current build of e3sm.  From there, do the git log,   find the ACME stuff from way back when, and do that.  

  The submodule stuff is for MPAS and fates and sbetr and whatever, so   hopefully that will cause those kind of errors to stop happening?

  Anyway, maybe I should run the tests on this, before trying to checkout   an old commit?  I'm trying to avoid the (no branch) stuff, because taht   mungs the baselines.

* Okay, a list of things I need to consider:

1 fates,mpas,submodules 2 baseline generation? 3 (no branch), impact on running/generating baseline, using baseline? 4 git merge my branch with latest E3SM?

* I'm going to run the create_test -g command from E3SM, using a testroot   of /pic/scratch/d3a230/e3sm:

  mkdir -p /pic/scratch/d3a230/e3sm   cd ~/test/E3SM/cime/scripts   ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm -g   e3sm_developer 

* Failed with NLComp, which appears to be because   /pic/projects/climate/acme_baselines doesn't exist.  Maybe I need to give   a baseline directory to generate into?  I'm sure that's it, actually -   generate baselines using the -b directory, and use that for future tests!

  Right!  So use --baseline-root /pic/scratch/d3a230/es3m/baseline!

* Same thing with my tests using my branch and other stuff.

  Okay, once these runs finish, I'm going to do that:

  mkdir  /pic/scratch/d3a230/es3m/baseline   ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm -g \      --baseline-root /pic/scratch/d3a230/es3m/baseline \           e3sm_developer

* Do the same thing with my ACME.master:

  cd ~/ACME.master   git submodules update --init

  (hopefully, this won't be out of phase with my old old code...)

  mkdir -p /pic/scratch/d3a230/acme.master/baseline   cd cime/test   ./create_test -p iesm --test-root /pic/scratch/d3a230/acme.master -g \      --baseline-root /pic/scratch/d3a230/acme.master/baseline \           acme_developer   

=======================

* Anyway, next up is trying to launch the interactive shell via srun, and   seeing if that works.  Assuming it doesn't, then these are the things I   need to check to get debugging working:

0 interactive shell (srun) 0 read testing documentation on atlassian again and again   1 read totalview documentation for how to run 2 check version of ifort 3 check compiler logs and see if everything is compiled with -g  4 Get a clean install of the latest version of E3SM, and run the tests.

12/11/18

* I'm having trouble committing my changes to git.  I wanted to just commit   the rename from components/iac to components/gcam, but that's not   working, so I'm going to try and commit all my changes.

* Thus, here are the files I changed to get it to compile with the iac   stub:

        modified:   cime/config/acme/config_files.xml         modified:   cime/config/acme/machines/Makefile         modified:   cime/scripts/lib/CIME/case.py         modified:   cime/src/build_scripts/buildlib.csm_share         modified:   cime/src/drivers/mct/cime_config/config_component.xml         modified:   cime/src/drivers/mct/main/cesm_comp_mod.F90         modified:   cime/src/drivers/mct/main/prep_lnd_mod.F90         modified:   cime/src/drivers/mct/main/seq_frac_mct.F90         modified:   cime/src/drivers/mct/main/seq_hist_mod.F90         modified:   cime/src/drivers/mct/main/seq_rest_mod.F90         modified:   cime/src/drivers/mct/shr/seq_flds_mod.F90         modified:   cime/src/drivers/mct/shr/seq_timemgr_mod.F90         typechange: components/homme/cmake/machineFiles/sandia-srn-sems.cmake         deleted:    components/homme/utils/cime         modified:   components/mpas-cice/model (modified content)         modified:   components/mpas-o/model (modified content)         modified:   components/mpasli/model (modified content)

  I have no idea whats going on with the mpas or homme stuff here - it   looks like they get modified by the build process or something.  

* I put the actual diffs in a build.diff file - please don't erase that.

? Huh - well, I had to go into components/mpas*/model directories and do a   git commit -a -m'Submodule commit' to get the broader ACME commit to   finally go through.  But that begs the question - what exactly is going   on with any mpas repository changes?  Seriously, I never modified that at   all myself - the only thing I can think of (which is insane) is that the   *build* process, somehow, modified the code.  !  Also, these changes were   listed as a 'typechange', which, I don't know what that means, but it   seems to be something like changing a link to a file or something like   that. 

! Anyway, some of my build and testing issues seemed to be related to   'mpas', if I remember right.  A 'submodule' is a whole other repository   for code to be grabbed from, so could it be that 'mpas' got changed and   pulled as part of the build and test, so I have a new version in my   working area?  That's completely nuts!  That's just a recipe for code to   get changed out from under me, without my knowing it!  

  A lot of weird things that I don't understand going on here...

12/4/18

* From the slack, in how to debug:

--------   I tackled this issue previously as:   - Compile a single test/case with DEBUG=TRUE   - Start the job in the interactive queue   - cd <RUNDIR>   - Launch totalview and run $EXEROOT/e3sm.exe (edited) --------

* So, how to start interactive queue?

  https://confluence.pnnl.gov/confluence/display/RC/Using+Debuggers+-+TotalView   https://confluence.pnnl.gov/confluence/display/RC/Launching+Interactive+Jobs

========= To launch an interactive job, use the isub command:

    isub -A <''your-account''> -W ''mm'' -N ''nn'' -s <''your-shell''>

Arguments:

  -A project account(required): same as the value on your #SBATCH -A line in a batch job

  -W time in minutes (default 30)

  -N number of nodes (= processor core count/24) (default 2)

  -s shell to use (default your current shell)

  -p partition to use (optional, do not change unless you need to)

  -h print a help message

isub requires that you be running X windows (Xming on Windows), since it opens an X terminal.

For example: to run a 30 minute interactive job in csh on 2 nodes under project constancetest:

    isub -A constancetest -W 30 -N 2 -s csh Launching an Interactive Shell

You can start an interactive shell as in this example.

    srun -A constancetest -p short --time=45 -I60 --pty -N 2 --ntasks-per-node=24 -u /bin/tcsh

The -I60 means that if the job cannot run within 60 seconds, then the     system will stop trying to run it. In this example, a 45 minute time     limit is requested, therefore the short partition is picked, since it     is generally less busy. ==========

* So: isub -A gcam -W 45 -I60 -N 4

  I need to figure out how many nodes are used in my test case.  Or just   use four.

11/6/2018

* /pic/scratch/d3a230/ERP_Ld3.f45_f45.ICLM45ED.constance_intel.clm-fates.20181101_094300_i966hg

  ./case.build --clean   DEBUG=TRUE ./case.build

  Check to see if compiling with -DDEBUG and/or -g or whatever.  If not,   figure out how to compile that way.

  Okay, that failed, because I'm in the csh - so:

  setenv DEBUG TRUE   ./case.build

  ...and hope that takes.  If not, have to dig deeper...

* Okay, I'm pretty sure that didn't work the way I wanted it to - I'm   seeing -DNDEBUG and -O2 and no -g in the compile lines of:

  /pic/scratch/d3a230/csmruns/ERP_Ld3.f45_f45.ICLM45ED.constance_intel.clm-fates.20181101_094300_i966hg/bld/acme.bldlog.181106-145641

* I'm not certain where to go from here - look into the individual make   files?  There has to be something in case.setup or case.build that makes   DEBUG happen.

  It appears to be set inside of env_build.xml, and other .xml files, so,   um, how do those get set?  It seems like, from the timing, that   env_build.xml is created from case.build.  Jeepus, I'm going to have to   dig into the CASE tools, aren't I?

10/12/18

* Makefile in, I think,   /people/d3a230/ACME/cime/config/acme/machines/Makefile

  ...that we have to modify to link with iac.  

  Yup, that did it - add libiac.a in the ULIB thing.

* https://acme-climate.atlassian.net/wiki/spaces/Docs/pages/17006928/Installing+the+ACME+tests

  TL;DR - cd ACME/cime/scripts, ./create_test acme_developer -g.   (There's something about "wait_for_tests", which, I didn't really get.)

  Future runs of acme_developer tests can go without the -g, we just need a   baseline for the initial run.

  Should probably dump output to a .out file.

10/11/18

* Almost - but I'm not linking with the iac library I build, so we fail on   the linking part.  Look at:

  ~/ACME/cime/src/drivers/mct/cime_config/buildexe

  ...to see if you can figure out where I'm missing something.

10/10/18

* Finally tracked down all the .xml changes I need to make, created a siac   stub, and did other misc changes.  I need to document what these file   changes are - the .xml files, at least, are the configuration changes   needed to add a new component class.

@ Fractions - I need to go back to seq_frac_mct.F90 and update the   fractions infrastructure for IAC.  Right now, I've passed in the argument   and declared everything, because I'm working on compilation right now,   but eventually we need to understand what exactly is going on here.  It   seems like each component has it's own fractions_zx built, but I can't   quite figure out what it is doing and what would be appropriate for the   iac to use.  For now we simply do nothing, so fractions_zx will be   allocated but unused - I'll have to see later on if that mungs up the   interpolation step before we call iac.

* Fractions are the fraction of each grid cell that is lnd, ocn, ice, and   atm.  Frac of atm is always 1.0, which makes sense - you have an   atmosphere everywhere on the planet.  Fractions of lnd is static, but the   fractions of ice change over time, which is why this is important, I   think - you need a dynamic update of surface type fractions, so you have   to rebuild these fractions every Nth iteration and you have to use these   fractions instead of some kind of default mapping.

  But!  Does GCAM use them at all?  Do we need them?  I feel like any time   we use data on a grid we need to have the fractions available, even if   our couplings (lnd and atm) will never change.

? Something else - writing history and restart files.  We need to figure   out if that makes sense for iac and what should be written - it may be   that the GCAM outputs are fine for what we want and the total history   files aren't needed.  Restart is a little different question, if we have   a persistent state inside of gcam that we need to track.

* List of things I need to revisit as I integrate iac/gcam:

1 fractions_zx 2 seq_rest_read and seq_rest_write - figure out what to dump to a restart   file 3 seq_hist_write, seq_hist_writeavg

10/8/18

* Finally tracked down (I think) the way to add a brand new optional   component to the build system.  The issue is that the existing 7   components are pretty hardwired into seem, and we don't want to require   IAC in the case string if we aren't building with it (which would require   retrofactoring every test case string).  Apparently, the ESP component   was also added after with the same issues, so it was made optional,   giving me a path forward in IAC.

  There's probably more to this, but this is what I've done so far:

1 Modified ~/PIC/ACME/cime/scripts/lib/CIME/case.py c. L526.   The way ESP was optional is that we check the number of allowed   components vs. the number specified on the case string, and if we had   fewer than expected we added 'sesp' (the stub ESP) to the build.   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   So, do   the same thing - if we are short, look for ESP and, separately, IAC, and   add the default (stub) if they are missing.  (For now, I'm adding the   actual iac build, because I want to test and I haven't written a 'siac'   build yet.) XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

! I THINK THIS IS NOT RIGHT - I *believe* the order of things matters - we   need the fifth component class to match the fifth component, so we can   map one to the other.  That's insane - we should have a mapping in the   build, right?  Maybe that's what the case string is for, though, to say   "first use this for coupler, then this for atm, etc...

  ALSO, "component class" is 'iac', while "component" is 'gcam'.  Review rules   for case string and see if order matters!

  ...this means that we might have to require IAC to be either after or   before ESP in our case string.  Probably before, to make cases that work   for ESP correct.

* Okay, so some clarity - the order DOES matter, so we pretty much have to   tack IAC on to the end, and have to use the count of components to figure   out if we have to add ESP and IAC or not.  And, I'm pretty sure we will   have to eventually require explicitly an ESP component if we want to use   IAC. 

  So, if 2 less, use SESP and SIAC.  If 1 less, add SIAC.

* Also, compsets are defined in config_compsets.xml, but these are all over   the place - in the mct area, and in a lot of component areas.  This is   where the strings are built, so probably an IAC config_compsets.xml is   the way to build our gcam runs.

* Finally, the case strings *do* give the model - for example, in   .../components/cam/cime_config/config_compsets.xml, we define the F1850   compset with this string:

  1850_CAM4_CLM40%SP_CICE%PRES_DOCN%DOM_SROF_SGLC_SWAV

  So, use cam4, clm40, and cice models, then the data ocean model and stub   rof, glc, and wav.

* So, this explains why we have to have a 'siac' - it's in the place of a   real component, because we can't consider iac and gcam as the same thing   - we may eventually want to run another economic model as the iac   component (or another version of gcam).

* I know other versions of our models are used - cam4 and cam5, clm40 and   clm45.  So somewhere in the build system must be a way to map the "CAM4"   string into telling it to compile cam4 vs. cam5.

  My guess is it will show up in the buildlib script somewhere...

* Okay, this ALSO means that I need to modify the buildlib for   mct/coupler/whatever - that's the one that decides to build the   iac_foo.mod stuff I coded up.  That's the key one to get the default   tests up and running.

2 Modified .../cime/src/drivers/mct/cime_config/config_component.xml,   adding IAC to the COMP_CLASSES.  That will get my component_classes right   for the above check to work.

3 buildnml   buildlib for iac component (not yet finished)

  Hopefully, the above config changes will automatically send it into the   iac component directory and run these scripts to build and create the   namelists.  But we probably have some more work to do to force it to link   or install correctly - hopefully not.

  Update: yes, there is more work.  Only the coupler understands "iac" - we   have to somewhere tell it to build gcam when we want the iac component.   In this way, we can swap out 

* Everybody uses perl for buildnml; only clm uses python for buildlib.   That's almost certainly no longer true with recent branches, which is an   issue, because I should use python... (they are porting to python).

  

7/2/18

@Xseq_com_mct.F90 mods are done - mostly cut and paste and mod to iac, IAC,   and 'z'.  It mostly is just counting up and assigning indeces to the   procs and stuff, so it's all the same for each component.

? Next up is figuring out how to set an iac component type.

6/27/18

* Modifications to seq_infodata_mod.F90:

* mostly just search for "lnd", and wherever you see variable definitions   or function arguments that have that, you'll see similar sections for   every component, so add an "iac" version.

! But!  Down in the function seq_infodata_Exchange() there's a little more   to it - that function is used to broadcast some of infodata between pes   to exchange information (really, just make it global).  But I need to   analyze that function first to see what is involve - I could just mimic   rof or wav or somethign like that, but I'd rather get a sense of what   this function is doing wrt to what I think iac should be doing.

  ...okay, it looks like just an interface for broadcasting the simple   infodata info like "lnd_present" and whatever.  Good, I can just copy   it. 

* Okay, seq_infodata_mod.F90 all done!

* The final thing is seq_comm_mct.F90, I think, before I'm ready for the   initial commit.

  (okay, I committed without setting seq_comm_mct.F90.)

! Need to track down component type.

@Xcomponent_type_mod.F90      Trivial!  Just declare the iac(:) type and make sure num_inst_iac is   available, plus there was one  public thing.

@Xcomponent_mod.F90

  Also, pretty trivial - some "one letter" stuff to add in 'z' for iac, and   that's it.  But, this is where component_run() is, so it's worth looking   that function over (and adding a 'z' one-letterism to it).  Also,   component_exch() and component_diag(), which are things I need to build   hooks for, eventually.

? There is some stuff in component_mod.F90 about "aream", in domains where   appropriate.  That includes most of the domains, apparently, but since I   don't understand waht it does yet I'ma going to leave out any iac   additions there.  But if it's something abouut "area matching", which it   might be because there's a lot of "samegrid_xy" stuff, then maybe it   makes sense to keep iac on the lnd grid and this is the infrastructure   for doing that?

  I should review prep_lnd_get_mapper_Sa2l(), which suggests it's something   like mapping state variables from atmos to lnd or something.

6/25/18

* Some random notes on things I've been wondering about:

* xxx_prognostic means "does model xxx need input from the driver",   according to the description in seq_infodata_mod.F90.   I suspect this is   to allow the model to run several time steps while only interacting with   the driver some of them - once per day, etc.  This means xxx_prognostic   is probably set at the top of every driver loop.

* xxx_present means "does xxx component actually exist", so it's set at run   time. 

? My current question I'm pondering is what do we do when we are in the   coupler, the lnd (e.g.) model is running, so it does the standard   prep_lnd_calc_z2x_lx() call to set up getting the inputs from iac ('z')   to lnd ('l')...but it's not the one time a year in which the iac is run?   What does the z2x_lx(:) Avect hold when the model hasn't run this loop?   What does the lnd model do with that information?  Does   prep_lnd_calc_z2x_lx() have some kind of interface or alarm for   determining that iac has run now and now we need to apply its output this   run, or do we assume we are going to use the same iac outputs for every   sample of the upcoming year, or what?

* There's a lot of stuff in seq_infodata_mod.F90 that comes from the   namelist or is set in some other way, so it's going to take me a bit to   track down all the various ways adding iac to this mod will matter.

5/23/18

* I'm way far behind, because of personal stuff, but we want to be ready   with some kind of initial checkin for the code review on July 1.  And we   don't want to dump a whole garbage can full of code modifications all at   once and have them tell me I'm doing it wrong.

  So, my initial checkin will be modifications mostly to the coupler code   to have IAC sections in there - stubbing out the actual calls to setup   and run GCAM, but getting all the infrastructure together, possibly with   an initial version of the GCAM code at least in its directory.

  This will let code reviewers see what I'm planning on doing, and also   might allow somebody to look it over and help me figure out some of the   problems I've been having.

1/5/18

* Here is my checklist and working notes as I continue the port.  I'm   losing track of all the new variables and mods and files I need to change   for each thing I add, so I need a place to scratch them down and mark   them off.

* cesm_comp_mod.F90:

@ Create iac_comp_mct:   iac_init_mct   iac_run_mct   iac_final_mct

@ modify seq_comm_mct:   IACID   ALLIACID   CPLALLIACID   CPLIACID   num_inst_iac

@ modify seq_timemgr_mod:   seq_timemgr_alarm_iacrun  

? seq_diag_mct:   seq_diag_iac_mct, maybe more - review what diagnostics we might need

@ seq_flds_mod: (Iac one digit is 'z', because the next best thing to being   right is to be very wrong):

  seq_flds_z2x_fluxes    seq_flds_x2z_fluxes

  Actually, I'm not sure if we are using fluxes or scalars or whatever.   I'll have to figure that out.

@ component_type_mod:

  iac - component

@ prep_iac_mod:

  Whole thing, to prep the iac component.

? At this point, I'm not sure if we need the prep routines as listed in   line 197-204.  Revisit.

? Don't know what fractions_?x(:) arrays do, but I better stick one in on   line 223:  fractions_zx(:)

@ c. Line 272: Figure out how to set and use iacrun_alarm and EClock_z

? c. Line 365: iac_prognostic - "iac component expects input", which, okay,   but I'm not sure what this logical means.  Does it get set somewhere as   part of the alarm mechanism?

? L418: Don't know what "iac_gnam" means.

* L431: samegrid_zl - my belief is that gcam is set up to take data on the   land grid and convert to regions from there; hence, I'm adding a   samegrid_zl logcial, because there's a bunch of those around.  But I   don't really know when it is used or how it is set...

? It appears to be used for SCM, so the question is, do we need to care   about IAC and SCM?

? L445 et al - do we want to create history files wiht iac?  For now, say   no.

? L536+ - I guess I'll add in mpicomp ids and "iamin" for IAC, but who   knows how they are really used.

? L588: "component instance counters"?  Okay, ezi.

* cesm_pre_init1():

! L662: Finally, some coding.  I need to follow suit with these, as they   set up all the communication stuff.  Make sure we have defined all these   approporiate seq_comm_name and seq_comm_iamin arrays.

* cesm_pre_init2():

* L930: call to seq_infodata_GetData() to find iac_present, etc.  So I have   to make sure the right stuff is in the infodata structure, too.

* L1128: iac_phase in seq_infodata_putData().  Hurm.

? L1146: we now get to some specials about single_column modelling on a   non-aqua planet.  Leave them alone for now.

* cesm_init():

  Awesome - we have two pre-init functions before we finally init!

@ L1215+   component_init_pre()   component_init_cc()   component_init_cx()  

  ...all called with iac, but probably generically.  Check, though.

@ L1342+   component_get_iamin_compid()   component_get_name()

@ L1369: Review seq_infodata_exchange()

  Again, lots of these are called generically and set up to allow   components to talk to each other.  But it's still pretty opaque.

@ Coupling flags: L1508.  I'm just faking it here, and maybe I need to   check on prognostic...

  I *think* we may need to do prognostic here - the comments at the top    sketchily suggest that "prognostic" means "expect input", which is what   lnd and atm need from iac.  The question is - do we need an   iac_prognostic to to couple lnd_c2_iac?

? L1691: prognostic instances, and making sure num_inst_xxx =   num_inst_max.  I really don't understand that, but I'll need to figure   out the prognostic stuff, first.

* L1719: prep_iac_init() - this is obviously something I have to write.

? L1765: seq_domain_check() - I suspect this won't be needed for iac, but   check into it nonetheless.  I'm not sure what "domains" are in this   section, and there are some component-based elements in the call to   seq_domain_check(), but they seem linked to the "samegrid_xx" variables. 

? L1813: compnent_init_areacor(): some kind of area corrections that I   don't understand.  In this case, they do seem to call for every   component, so add it in and figure it out later.

? L1860: component_diag(), something about "recv IC xxx", which I don't   understand either.  This probably means I need to develop diagnostics for   iac. 

* L1890: more about fractions, which I still don't get.

@ L1894: seq_frac_init() - modify for iac and fractions_zx.

? L1980+ Okay, we are starting to get into initialization and prep for   model runs, with a lot of specific component related elements.   Look for   lnd_ and atm_ prep for coupling with other components...

* L2043: this component_exch(atm, flow='x2c',...) - This is an init, so   we haven't looped over timesteps yet, so maybe it doesn't need to get   anything out of the iac output yet.  But this is the kind of thing we are   looking at.

* seq_flds_x2a_fluxes - this is the array (I think) of fields that we need   to grab from the coupler into the atm model.  So I will have to modify   this somewhere to include feedback from iac.

? L2109+ - once again, figure out what kind of thing is happening with   these calls to prep_lnd_calc_r2x_lx() et al.  They describe it as mapping   initial r2x_rx and g2x_gx to _ox, _ix, and _lx, which I guess is some   kind of grid mapping from the glc and rof components to ocean, ice, and   lnd.  I'm hoping this kind of thing doesn't matter for iac - right now we   expect the lnd mapping on input to gcam, and hopefully the lnd and atm   mapping are the same.  Otherwise, maybe see how we do land to atm   mapping, and make sure the gcam outputs go along with that?

? seq_hist_write() - add in iac and fractions_zx.

! Finally, cesm_run(), which is I think something I know a little about.   Let's find out how wrong I am!

@ L2167: seq_comm_mct.mod.F90, iac_layout

* L2171: hashcnt - I have notes on this somewhere, but I still don't know   if it applies to iac or not.  We'll have to revisit this.

? L2193: Do I need to set iac_phase=1?  We don't have wav or rof, and iac   is less connected than anything, so why don't I skip this for now.

@ L2252: define iacrun_alarm somewhere, seq_timmgr_alarmIsOn(),   seq_timemgr_alarm_iacrun.  This hopefully will make it clearer how alarms   go and when to run things.  Also, maybe review glcrun_avg_alarm and   ocnnext_alarm and see why they need extra alarms for those comps.

@ L2542: prep_lnd_calc_z2x_lx() - need to write this function, to do the   iac preparations for updating the land model.  I need to do something   like this for atm prep, as well.

?  ? The Big Quesiton I have so far is how this works with the fact that we ? run IAC yearly or five yearly, not on the same time grid as the lnd and   atm models.  Presumably, prep_lnd_calc_z2x_lx() and   prep_atm_calc_z2x_lx() will simply do nothing except on the right time   scale, but it's not clear to me how that works.  Do any other components   have this delayed and/or long term application?

! OKay, as I understand it, we want to run IAC at the start of a given   year (or 5-year block), which finally uses the averaged inputs from the   previous year, and then use that input to modify atm and lnd.  So, if my   understanding is right, that means we RUN iac before we SETUP lnd and   atm.  Is that right?  Do we do a full setup/run/post on IAC before doing   anything else?  That suggests we should put the IAC stuff right at the   top.

  Let me review other modules and see if I can see that - right now it   seems like we do setup (all comps), run (all comps), post (all comps),   but that could be just because they are all on the same time scale.

  Alternatively, I guess, we could run iac at the end, and that post will   then be available to the next one.  But don't do this - let's run right   at the top.

  I'm pretty sure it's okay - we apparently have a lot of options for when   ocn/atm models are run and how they are set up, and they happen spread ! throughout this function.  So, my guess is that I will have to couple in   iac in a couple different places - stick with the lnd stuff; whereever we   see lnd_c2_atm then activate an iac_c2_atm as well.

!  ! Okay, I think I'm gonna revise my plan, and just implement iac just like ! all the other components - serial in setup, then use the same parallel   mechanism and barriers as other components.  I believe what this means is   that we'll have the previous one year of lnd as input to iac, and then   the resulting output of the iac will apply on the *second* timestep of   the year for lnd and cam.  I don't think this is a gross violation of the   methodolgy, and allows GCAM to be run fully in parallel like other   components. 

  It's probably not a big deal either way, as GCAM is not computationally   intensive and runs once a year anyway, but if I ran GCAM serially in the   coupler before running everybody else's setup (which is essentially how   it runs in iESM) it would make this component different than the others,   which is harder to maintain and, crucially, more likely for me to get   wrong.  Also, who is to say we won't have a CPU intensive IAC module to   hook in some point in the future?

! l. 2349 - iac_prognostic - I'm not sure what this means, but I think it   suggests that if we are providing input to IAC this is the section where   we build those inputs.  When would iac_prognostic NOT be on?  I need to   ask Kate about this, but for now I'm keeping the form that the other   modules use.  (I'm following ROF as a template).

? Could iac_prognostic be variable, so it's set only on time steps where we   need to do stuff?  Hmm.

* prep_iac_accum_avg() - this function should take the average of the accum   vars.  I know this one.

* prep_iac_calc_l2r_rx() - I still don't know what fractions_Xx suggest,   but thi sis obviously the function that grabs the lnd vars out of the   coupler and makes them availalbe (or ready?) for iac to use.

! l. 2361 - prep_iac_mrg() - review what it means to "merge" in this   context.  Is this just prep work for the diagnostic in the next line, or @ is this more setup for iac to use?  Review what ROF merge does.

! l. 2376 - figure out how component_exch() works, what it does and what   all the arguments are.  I'm not sure the timer/barrier format string, but maybe   it doesn't matter that much.

* l.2772 - the run call is straightforward, but I still am not sure whether   "_fluxes" is what we are sending or not.  I need to know the difference   between a flux and state variable, since they seem to be treated   differently.  

* l.2849 - Just to be cheeky, I'm putting the IAC RECV before everybody   else.  I'm 78% sure it doesn't really matter, as everything is in   parallel, but just in case it does I want to run at the top so the lnd   can use it later.  Also, this WILL matter if at some point we decide to   simply run in the coupler before doing anything else, or something like   that (although I think at that point we'll move all these things together   in one block).

@ l.2878+ - prep_lnd_calc_z2x_lx() and prep_atm_calc_z2x_ax() - functions   in the land and atm components that I have to write to pull the important   outputs from iac.  This might be the toughest thing, because I'm not at   all conversent in those other models and do not know how they use these   inputs.  I hope the iESM code helps with regard to this, but since it   couples differently I'm not at all sure that will be the case.

  (Also, make sure these functions are named correctly - I'm still a little   shaky on the naming arcana).

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXX See below, after seq_hist_write() XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX ! l.2895_ - not sure what do_hist_z2x might imply - writing out history   files?  Just follow the template and figure it out later.  It may mean   figuring out seq_hist_writeaux() to add in the var names or something.

  (especially with regard to nx, ny, the write_now argument?  I set   write_now to the t1yr_alarm, because we run yearly, but, yes, well, hmm.)  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

@ l.3779 - restart file - seq_rest_write() - another in the endless string   of functions that I hope are easy to modify to deal with an iac   component.  Add in fractions_zx and the iac component, and hope for the   best.  

@ l.3803 - seq_hist_write() - same thing.   l.3812 - seq_hist_write_avg()

? Huh, down here is where some do_hist_a2x stuff happens.  Also the 1 year   lnd writes; maybe move iac down here?  Do we even need iac hist writes?

! You know what? Screw the do_hist_z2x stuff.  We'll add it in later if we   want it,  maybe right here, maybe after iac post like rof does.

* Restart -   l.3953 - seq_rest_read()

* l.4032 - just follow the pattern - see if root, whatever.

! Whoo-hoo, done with cesm_run - now cesm_final():

* l. 4129 - component_final() - review.  It might be part of what you do to   create a component in the first place.

* Hey, that's it. For cesm_comp_mod.F90.

6/27/17

  Script to run the model, from Balwinder:

  /people/sing201/runscr/int/acme/acme_def_cime5_03022017/acme_def_cime5_03022017
