!> @file libpiovdc.F90
!> @author  Yannick Polius <ypolius@ucar.edu>
!> @version 1.0
!> @date 02/08/2012
!> @brief The piovdc library for writing Vapor Data Collection (VDC) 2 data files
!> 
!> <br>
!> @details The piovdc library is used to write VDC2 data files in a	 
!> parallel manner using PIO. After the prerequisite library functions are 
!> used, a call to pio_writedarray is made, writing the passed 
!> data to an on disk VDC2 collection.<br>
!> PRE-REQUISITES: <br>
!> 	VDF meta-file must be generated, using either rawtovdf or vdfcreate
!> 	VDF file requires VDC version to be 2, and requires the Waveletname,
!>	WaveletBoundaryMode, CompressionRations, and NumTransforms to be set.<br>
!> POST-EFFECTS: <br>
!>	After a successful write, VDC2 data will be in a directory located in
!>	the same directory as the vdf file, using the vdf name, appended with _data
!> 	(ex. ghost.vdf generates VDC2 data in the dir ghost_data in the vdf dir)
!>	If no compression is enabled, a single, uncompressed .nc file will be 
!>	generated using PIO instead of a VDC
module piovdc
	use pio
	implicit none
contains

!> @brief subroutine checks MPI status and prepares a VDC optimized IO
!> decomposition for PIO to utilize in rearrange computational data 
!> 
!> PRE-REQUISITES : <br>valid vdf metadata file must be created for the job
!> 
!> POST-EFFECTS: <br>
!>	Used to initialize a VDC optimized IO decomposition for a block of data. After
!>	calling this function, the output nioprocs is used for the job size
!>	in PIO_init, iostart and iocount are used in PIO_initdecomp by IO tasks for setting up
!>	the data decomposition. Rank is provided in the event of MPI not being
!> 	initialized by the user, as it is required by PIO_init. The library will automatically 
!> 	start MPI if it has not been already. DO NOT use this function if your program is already using it's own unique IO decomposition,
!>	as VDC optimization might not be compatible with the decomp. Instead, skip this function and plug in your own values for nioprocs, iostart, and iocount into the appropriate PIO functions.
!> @param[in] vdf_path char location of vdf meta file MUST have null char appended
!> @param[inout] nioprocs int used as the upper limit for the # of tasks used for IO, returns the actual # used
!> @param[out] iostart int(3) calculated IO start position for current task
!> @param[out] iocount int(3) calculated block count for current task
!> @param[inout] rank int rank of the current task 
!> @param[inout] ierr int error handle
subroutine piovdc_init(vdf_path, nioprocs, iostart, iocount, rank, ierr)
#ifndef NO_MPIMOD
	use mpi			!_EXTERNAL
#endif
	implicit none
#ifdef NO_MPIMOD
	include 'mpif.h'	!_EXTERNAL
#endif
	integer (kind=PIO_OFFSET), intent(out)  :: iostart(3), iocount(3)
	character(LEN=*), intent(in)		   :: vdf_path
	integer (i4), intent(inout)	:: ierr, rank
	integer (i4), intent(inout)	:: nioprocs
	!Locals
	integer(i4)    :: bsize(3), dims(3)
	integer (i4)	:: nprocs, ts

	logical		:: flag

	!Check to see if MPI has been initialized
	call MPI_INITIALIZED(flag, ierr)
	if (.NOT. flag) then
	    call MPI_INIT(ierr)
	    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
 	    call MPI_COMM_SIZE(MPI_COMM_WORLD, nprocs, ierr)
#ifdef DEBUG
	    if(rank .eq. 0 ) then
		print *, 'MPI initialized by library. COMM SIZE: ', nprocs, ' nioprocs: ', nioprocs
	    endif
#endif
	else
	    call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
 	    call MPI_COMM_SIZE(MPI_COMM_WORLD, nprocs, ierr)
	endif

#ifdef DEBUG
	if (rank .eq. 0 ) then
		print *, 'Retrieving VDF info...', vdf_path
		print *, 'MPI COMM SIZE: ' , nprocs
	endif
#endif

	call getvdfinfo(bsize, dims, ts, rank, TRIM(vdf_path))

#ifdef DEBUG
	if (rank .eq. 0) then
		print *, 'VDF info retrieved'
		print *, 'Calling init_vdc2...'
	endif
#endif	

	call init_vdc2(rank, dims, bsize, iostart, iocount, nioprocs)
	
#ifdef DEBUG
	if(rank .eq. 0 ) then
	    print *, 'init_vdc2 calculated vdc2 block params'
	endif
#endif
	return 
endsubroutine

!> @brief subroutine that generates PIO handles that are VDC2 compatible
!> 
!> PRE-REQUISITES: <br>
!>	piovdc_init must have been called to calculate nioprocs needed by PIO_init
!>	PIO_init must have been called to generate the iosystem information <br>
!> POST-EFFECTS: <br>
!>	file_handle can be immediately used with PIO, which will not write to a single
!>	NetCDF file but to a VDC 2 data collection. var_handle MUST be opened with 
!>	piovdc_open_var before being used to write data for a variable.
!
!> @param[out] file_handle File_desc_t holds PIO-specific information concerning the file to be outputted
!> @param[out] var_handle Var_desc_t holds general VDF information to pass through PIO to the VDF library
!> @param[in] iosystem IOsystem_desc_t holds the io information generated by PIO_init
!> @param[in] iocount integer array containing the size of the data for an IO task
!> @param[in] vdf_path char the path to the vdf meta file
!> @param[inout] ierr int error handle
subroutine piovdc_gen_vdc2_handles(file_handle, var_handle, iosystem, iocount, vdf_path, ierr)
#ifndef NO_MPIMOD
	use mpi			!_EXTERNAL
#endif
	implicit none
#ifdef NO_MPIMOD
	include 'mpif.h'	!_EXTERNAL
#endif

	type (Var_desc_t),intent(out) :: var_handle
	type (file_desc_t), intent(out) :: file_handle
	character(LEN=*),intent(in)	::	vdf_path
	type (iosystem_desc_t), target, intent(in) :: iosystem
	integer(kind=PIO_OFFSET), dimension(:), intent(in) :: iocount
	integer(i4), intent(inout)	:: ierr
	!LOCALS
	integer(i4)	:: bsize(3), dims(3)
	integer(i4) 	::	ts !total num of time steps
	integer(i4)	::	rank

	file_handle%iotype = pio_iotype_vdc2
	file_handle%iosystem => iosystem

	call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)
	call GetVDFInfo(bsize, dims, ts, rank, TRIM(vdf_path))	
	var_handle%bs = bsize
	var_handle%dims = dims
	var_handle%num_ts = ts
	var_handle%vdf = vdf_path
endsubroutine

!> @brief subroutine that packs var data into pio's var_desc_t to pass through PIO to the VDC library
!
!> PRE-REQUISITES:<br>
!>	piovdc_gen_vdc2_handles MUST have been called to generate the VDC2 compatible
!>	var_handle that is to be passed into this function
!> POST-EFFECTS:
!>	<br>var_handle can now be used with PIO to output data corresponding to a single
!>	variable and timestep to the VDC
!> @param[in] ts int timestep to write
!> @param[in] varname char variable to write MUST have null char appended
!> @param[in] lod int level of detail
!> @param[inout] vardesc Var_desc_t the variable description structure
!> @param[out] ierr error return handle
!> @return var_desc is now an opened variable that can be used with pio_write_darray
subroutine piovdc_open_var(ts, varname, lod, reflevel, var_desc, ierr)
	integer(i4), intent(in)	:: ts, lod
	character(LEN=*), intent(in)	:: varname
	integer(i4), intent(out) :: ierr
	type (Var_desc_t), intent(inout)	:: var_desc

	var_desc%name = varname
	var_desc%cur_ts = ts
	var_desc%lod = lod	
	var_desc%reflevel = reflevel
	RETURN
endsubroutine


!> @brief subroutine checks start/count for out of bounds, adjusts if the start/count is too high, negates start if it is invalid
!> POST-EFFECTS:
!>	<br>all start/counts are now legal, non-IO tasks have negated start counts
!> @param[in] global_dims int(3) global grid dimensions
!> @param[in] rank int rank of current MPI task
!> @param[inout] start int(3) current MPI task global start
!> @param[inout] count int(3) current MPI task global count
subroutine adjust_bounds(global_dims, start, count, rank)
	real (r4), dimension(:), intent(in) :: global_dims
	integer(i4), intent(in) :: rank
	integer (kind=PIO_OFFSET), dimension(:), intent(inout) :: start, count

	!first check to ensure the start is legal

	if (start(1) .GT. global_dims(1) .OR. start(2) .GT. global_dims(2) .OR. start(3) .GT. global_dims(3)) then !outside of global bounds!

	   !negate everything, they're useless
#ifdef DEBUG
	   print *, ' rank: ' , rank, ' start: ' , start, ' count: ' , count , ' negated'
#endif
	   start = (/ 0, 0, 0/)
	   count = (/ 0, 0, 0/)
	else 
	   !start is legit but count might not be, check & adjust to the boundaries
	   if(count(1) + start(1) - 1 .GT. global_dims(1)) then
	      count(1) = global_dims(1) - start(1) + 1

	   endif
	   if(count(2) + start(2) - 1 .GT. global_dims(2)) then
	      count(2) = global_dims(2) - start(2) + 1

	   endif
	   if(count(3) + start(3) - 1 .GT. global_dims(3)) then
	      count(3) = global_dims(3) - start(3) + 1

	   endif
	end if
end subroutine

!> @brief subroutine that, given a global grid, VDC blocksize, and max # of nioprocs, will
!> automatically create an VDC optimized IO decomposition that uses the most possible IO tasks
!
!> POST-EFFECTS:
!>	<br>Each MPI Task is now either and IO task or a computational task. IO tasks have valid start/counts
!> @param[in] rank int rank of the current MPI task
!> @param[inout] nioprocs int represents the max possible # of IO procs, algorithm will try to get as close as possible to this # and return it in nioprocs
!> @param[in] blockdims int(3) global grid dimensions represented as VDC blocks
!> @param[out] start int(3) iostart for the current MPI task
!> @param[out] count int(3) iocount for the current MPI task
!> @param[in] bsize int(3) VDC block size
subroutine auto_get_start_count(rank, nioprocs, block_dims, start, count, bsize)
  use pio_kinds
  integer (kind=PIO_OFFSET), intent(out):: start(3), count(3)
  integer(i4), dimension(:), intent(in) :: bsize
  integer (i4), intent(in) :: rank, 
  real (r4), dimension(:), intent(in) 	:: block_dims

  integer (i4), intent(inout) :: nioprocs
  !locals
  real (r4)             :: proc_count
  integer (i4)             :: lpp, spp0, spp1, counter, slab_counter, calc_procs, nslabs, nlinesPslab
  logical		:: found

  found = .FALSE.
  nlinesPslab = CEILING(block_dims(2)) !max # of possible lines per slab PER TASK
  nslabs = CEILING(block_dims(3))
  calc_procs = -1
  if (nioprocs .EQ. 1) then
	nioprocs = 1
	start = (/0, 0, 0/)
	count = block_dims * bsize
  else
	do slab_counter=1, nslabs
	  do counter=1, nlinesPslab 
		proc_count =  CEILING(nlinesPslab / REAL(counter)) * CEILING(nslabs / REAL(slab_counter))
		!test to see if counter # of lines per processor per slab is possible
		if (nioprocs >= proc_count) then
			if (proc_count .gt. calc_procs) then
				calc_procs = proc_count ! return the actual # of io procs used
				count = (/ INT(block_dims(1) * bsize(1)), counter * bsize(2), slab_counter *bsize(3) /)
				start = (/ 0, mod(rank, INT(CEILING(nlinesPslab / REAL(counter)))) * counter * bsize(2), INT(rank / CEILING(nlinesPslab / REAL(counter))) * slab_counter * bsize(3)/) + 1
				call adjust_bounds(block_dims * bsize, start, count, rank)

				if(proc_count .eq. nioprocs) then !using max #of procs, suitable solution found (for now)
					found = .TRUE.
					exit				
				end if
				  if (found) then
					exit
				  end if
			end if
		end if
#ifdef DEBUG

#endif
	  end do
	  if (found) then
		exit
	  end if
	end do
  end if
 nioprocs = calc_procs
end subroutine

!> @brief subroutine that prepares the global grid to be split by the auto_start_count routine
!
!> POST-EFFECTS:
!>	<br> A valid IO decomposition is created that can be used with PIO
!> @param[in] rank int rank of the current
!> @param[in] data_dims int(3) size of the global grid
!> @param[in] vdc_bsize int(3) VDC block size
!> @param[out] iostart int(3) IO start for the current MPI task
!> @param[out] iocount int(3) IO count for the current MPI task
!> @param[inout] ioprocs int max # of IO procs, gets returned as the actual # used
subroutine init_vdc2(rank, data_dims, vdc_bsize, iostart, iocount, ioprocs)
  integer (kind=PIO_OFFSET), intent(out)  :: iostart(3), iocount(3)
  integer (i4), intent(in) :: rank
  integer(i4), dimension(:), intent(in) :: data_dims, vdc_bsize
  integer (i4), intent(inout):: ioprocs
  !locals
  real(r4) :: vdc_blocks(3)   
  integer (i4)	:: ierr

#ifdef DEBUG
  if(rank .eq. 0) then
	print *, 'Calling get start count...block_dims: ', data_dims/real(vdc_bsize), ' bsize: ' , vdc_bsize, ' ioprocs: ', ioprocs, ' dims: ', data_dims
  endif
#endif
  vdc_blocks = data_dims/real(vdc_bsize)
  
  call auto_get_start_count (rank, ioprocs, vdc_blocks, iostart, iocount, vdc_bsize)
  
#ifdef DEBUG 
		print *, 'Retrieved VDF start count', iostart, '-', iocount, 'rank: ' , rank
#endif

endsubroutine

end module piovdc
