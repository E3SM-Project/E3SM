<chapter id="creating_a_case ">
<title>Creating and Setting Up A Case</title>

<para>The first step in creating a &cesm; experiment is to use
&create_newcase;. </para>

<!-- ======================================================================= -->
<sect1 id="how_to_create_case">
<title>How to create a new case</title>

<para>
CESM supports out of the box <ulink
url="../modelnl/compsets.html">component sets</ulink>, <ulink
url="../modelnl/grid.html">model grids</ulink> and <ulink
url="../modelnl/machines.html">hardware platforms</ulink>.  Component
sets (usually referred to as compsets) define both the specific model
components that will be used in a given &cesm; configuration,
<emphasis>and</emphasis> any component-specific namelist or
configuration settings that are specific to this configuration.  In
the CESM1.2 release series (starting with CESM1.2.0) component sets
and resolutions have been significantly changed to permit addressing
the growing model complexity. Both compsets and models grids are now
associated with three names: a new longname, a new short name (that is
backwards compatible with the older CESM1.1 release series long name)
and a new alias name (that is backwards compatible with the older
CESM1.1 release series short name).
</para>

<sect2 id="compset_names">
<title>New Component Set Naming Convention</title>
<para> The new component set (compset) longname has the form 
<screen>
TIME_ATM[%phys]_LND[%phys]_ICE[%phys]_OCN[%phys]_ROF[%phys]_GLC[%phys]_WAV[%phys][_BGC%phys]

TIME = model time period (e.g. 2000, 20TR, RCP8...)   
ATM  = [CAM4, CAM5, DATM, SATM, XATM]	   
LND  = [CLM40, CLM45, DLND, SLND, XLND]	   
ICE  = [CICE, DICE, SICE, SICE]		   
OCN  = [POP2, DOCN, SOCN, XOCN,AQUAP]	   
ROF  = [RTM, DROF, SROF, XROF]		   
GLC  = [CISM1, SGLC, XGLC]			   
WAV  = [SWAV, XWAV]				   
BGC  = optional BGC scenario                    

The OPTIONAL %phys attributes specify sub-modes of the given system
For example DOCN%DOM is the  DOCN data ocean (rather than slab-ocean) mode.
ALL the possible %phys choices for each component are listed 
by the calling create_newcase with the -list compsets argument.
ALL data models now have a %phys option that corresponds to the data
model mode 
</screen>
</para><para>
As an example, the new longname,
20TR_CAM4_CLM40%CN_CICE_POP2_RTM_SGLC_SWAV, refers to running the
prognostic components CAM, CLM, RTM, CICE, POP2 with stubs SGLC and
SWAV components.  The particular configuration will be a transient 1850 to 2000
run using cam5 physics, clm4.0 physics with clm4.0 cn,
prognostic cice (default) and pop2 (default).  The shortname and alias
for this compset are now B_1850-2000_CAM5_CN and B20TRC5CN, which
correspond to the CESM1.1 series longname and shortname, respectively.
<emphasis>Any one of the three compset names </emphasis>(longname, shortname or
alias) can be used as input to &create_newcase;.  It is now also
much easier to create your own custom compset (see <link
linkend="faq_createowncompset">How do I create my own
compset?</link>). All the out-of-the-box CESM1.2 release series
compsets are listed in <ulink url="../modelnl/compsets.html">component
sets</ulink>. Upon clicking on any of the long names a pop up box will
appear that provides more details of the component configuration.
</para>
</sect2>

<sect2 id="grid_names"> 
<title>New Overall Model Grid Naming Convention</title>
<para> The new model grid longname has the form
<screen>
a%name_l%name_oi%name_r%name_m%mask_g%name_w%name

a%  = atmosphere grid 
l%  = land grid 
oi% = ocean/sea-ice grid (must be the same) 
r%  = river grid 
m%  = land mask grid 
g%  = internal land-ice (CISM) grid
w%  = wave component grid (not relevant in CESM1.2 series) 

From the point of view of model coupling - the glc (CISM) grid is assumed to
be identical to the land grid. However, the internal CISM grid can be different, 
and is specified by the g% value.
</screen>
</para>

<para>
As an example, the new longname,
a%ne30np4_l%ne30np4_oi%gx1v6_r%r05_m%gx1v6_g%null_w%null, refers to
running an ne30np4 spectral element 1-degree atmosphere and land grids
a gx1v6 Greenland pole 1-degree ocean and sea-ice grids a 1/2 degree
river routing grid, null wave and internal cism grids and an
ocean/land mask that is determined by the gx1v6 ocean mask.  The
shortname and alias for this grid are now ne30np4_gx1v6 and ne30_g16,
which correspond to the CESM1.1 series longname nad shortname
respectively.  <emphasis>Any one of the three grid names </emphasis>
(longname, shortname or alias) can be used as input to
&create_newcase;.  It is now simpler for you to introduce new user
defined grids (see <link linkend="newgrid">Adding a new user-defined
grid</link>).  All the out-of-the-box CESM1.2 release series model
grids are listed in <ulink url="../modelnl/grid.html">grids
</ulink>. Upon clicking on any of the long names a pop up box will
appear that provides more details of the model grid.
</para>

<para>
Component grids (such as the atmosphere grid or ocean grid above) are
denoted by the following naming convention:
<itemizedlist>

<listitem><para>"[dlat]x[dlon]" are regular lon/lat finite volume
grids where dlat and dlon are the approximate grid spacing.  The
shorthand convention is "fnn" where nn is generally a pair of numbers
indicating the resolution.  An example is 1.9x2.5 or f19 for the
approximately "2-degree" finite volume grid.  Note that CAM uses an
[nlat]x[nlon] naming convection internally for this
grid. </para></listitem>

<listitem><para>"Tnn" are spectral lon/lat grids where nn is the
spectral truncation value for the resolution.  The shorthand name is
identical. An example is T85.</para></listitem>

<listitem><para>"ne[X]np[Y]" are cubed sphere resolutions where X and
Y are integers.  The short name is generally ne[X].  An example is
ne30np4 or ne30.</para></listitem>
<listitem><para>"pt1" is a single grid point.</para></listitem>

<listitem><para>"gx[D]v[n]" is a displaced pole grid where D is the
approximate resolution in degrees and n is the grid version.  The
short name is generally g[D][n].  An example is gx1v6 or g16 for a
grid of approximately 1-degree resolution.</para></listitem>

<listitem><para>"tx[D]v[n]" is a tripole grid where D is the approximate resolution
in degrees and n is the grid version. </para></listitem>

</itemizedlist>
</para>

</sect2>

<sect2 id="using_create_newcase"> 
<title>Using create_newcase</title>
<para> 
You should first use the -h option in calling &create_newcase; to
document its input options.  In addition, the &create_newcase; -list
[compsets,grids,machine] option can also be used to see which
component sets, model grids, and machines are supported. The links
above, however, provide a much more complete determination.
&create_newcase; can be called with the following arguments:
</para>
<screen>
create_newcase \     
     -case <emphasis>case-name</emphasis> \        
     -compset <emphasis>component-set</emphasis> \      
     -res <emphasis>resolution</emphasis> \         
     -mach <emphasis>machine-name</emphasis> \
     [-compiler <emphasis>compiler-name></emphasis> \
     [-mpilib <emphasis>mpi-library-name</emphasis>] \
     [-mach_dir <emphasis>alternative pathname for Machines directory</emphasis>] \
     [-confopts [_AOA],[AOE],[_D],[_E],[_N],[_P],[_R]] \
     [-pecount [S,M,L,X1,X2]] \
     [-pes_file <emphasis>full-pathname</emphasis>] \
     [-user_compset <emphasis>new user compset long name</emphasis>] \
     [-user_grid_file <emphasis>full-pathname of user xml grid file</emphasis>] \
     [-help [or -h]] |
     [-list <emphasis>[compsets,grids,machines]</emphasis>  \         
     [-silent [or -s]] \
     [-verbose [or -v]] \
     [-xmlmode [normal, expert]] \
     [-nowarning]      
</screen>

<para>Required arguments to &create_newcase; are -case, -mach, -compset and -res.
If you want to use your own pes setup file, specify the full
pathname of that file for the optional -pes_file argument. The
required pes_file format is provided at
<filename>$CIMEROOT/scripts/sample_pes_file.xml</filename>. 
</para>

<para> 
Following is a simple example of using &create_newcase;: In what
follows, $&CIMEROOT; is the full pathname of the root directory of the
CESM distribution. 
<screen> 
> cd $&CIMEROOT;/scripts 
> create_newcase -case ~/cesm/example1 \ 
  -compset B_1850_CAM5_CN \ 
  -res ne30np4_gx1v6 \
  -mach yellowstone
</screen>
</para>

<para>
This example creates a $&CASEROOT; directory
<filename>~/cesm1/example1</filename> where $&CASE; is "example1" with
a model resolution of 0.9x1.25_gx1v6 (a 1-degree atmosphere/land grid
with a nominal 1-degree ocean/ice grid using the gx1v6 ocean
mask). The component set B_1850_CN uses fully active components
configured to produce a present-day simulation.  The complete example
appears in the <link linkend="use_case_basic">basic
example</link>. $&CASE; can include letters, numbers, ".", and "_".
Note that &create_newcase; creates the $&CASEROOT; directory. If the
directory already exists, it prints a warning and aborts. 
</para>

<para>
As a more general description, &create_newcase; creates the directory
$&CASEROOT;, which is specified by the -case option. In $&CASEROOT;,
&create_newcase; installs files to build and run the model and perform
long term archiving of the case on the target platform.
&create_newcase; also creates the directory
<filename>$CASEROOT/Buildconf/</filename>, that in turn contains
scripts to generate component namelist and build component libraries.
The table below outlines the files and directories created by
&create_newcase;
</para>

<table>
<title> Result of invoking &create_newcase; </title>
<tgroup cols="2">

<thead>
<row>
<entry>Directory or Filename</entry>
<entry>Description </entry>
</row>
</thead>
<tbody>

<row>
<entry>README.case</entry> <entry>File detailing your &create_newcase;
usage.  This is a good place for you to keep track of runtime problems
and changes.</entry>
</row>

<row>
<entry>CaseStatus</entry>
<entry>File containing a list of operations done in the current case.</entry> 
</row>

<row>
<entry>Buildconf/</entry>
<entry>Directory containing scripts to generate component namelists
and component and utility libraries (e.g., PIO, MCT). You should never
have to edit the contents of this directory (unlike in CESM1.0.5)
</entry>
</row>

<row>
<entry>SourceMods/</entry>
<entry>Directory where you can place modified source code.</entry>
</row>

<row>
<entry>LockedFiles/</entry>
<entry>Directory that holds copies of files that should not be
changed. The xml files are "locked" after its variables have been used
by other parts of the system and cannot be changed.  The scripts do
this by "locking" a file and not permitting you to modify that file
unless a 'clean' operation is performend.  See <xref linkend="faq_lockedfiles"/>.
You should never edit the contents of this directory.
</entry>
</row>

<row>
<entry>Tools/</entry> 
<entry>Directory containing support utility scripts. You should never  
need to edit the contents of this directory.</entry>
</row>

<row>
<entry>env_mach_specific</entry> 
<entry>File used to set a number of machine-specific environment
variables for building and/or running. Although you can edit this at
any time, build environment variables should not be edited after a
build is invoked.
</entry>
</row>

<row>
<entry>env_case.xml</entry> 
<entry> Sets case specific variables (e.g. model components, model and
case root directories) and <emphasis>cannot</emphasis> be modified
after a case has been created.  To make changes, your should re-run
&create_newcase; with different options.</entry>
</row>

<row>
<entry>env_build.xml</entry>
<entry> Sets model build settings, including component resolutions and
component configuration options (e.g. CAM_CONFIG_OTPS) where
applicable (see <ulink url="../modelnl/env_build.html">env_build.xml
variables</ulink>).</entry>
</row>

<row>
<entry>env_mach_pes.xml</entry> 
<entry> Sets component machine-specific processor layout (see <xref
linkend="case_conf_setting_pes"/>). The settings in this are
critical to a well-load-balanced simulation (see <link 
linkend="running_ccsm_loadbalance">loadbalancing a run</link>).
</entry>
</row>

<row>
<entry>env_run.xml</entry>
<entry>Sets run-time settings such as length of run, frequency of
restarts, output of coupler diagnostics, and short-term and long-term
archiving. See 
<ulink url="../modelnl/env_run.html#run_start">run initialization variables</ulink>,
<ulink url="../modelnl/env_run.html#run_stop">run stop variables</ulink>,
<ulink url="../modelnl/env_run.html#run_restart">run restart control variables</ulink>,
for a more complete discussion of general run control settings.
</entry>
</row>

<row>
<entry><emphasis>cesm_setup</emphasis></entry>
<entry> Script used to set up the case (create the $CASE.run script and
user_nl_xxx files)</entry>
</row>

<row>
<entry><emphasis>$&CASE;.$&MACH;<filename>.build</filename></emphasis> </entry> 
<entry>Script to build component and utility libraries and model executable.</entry>
</row>

<row>
<entry><emphasis>$&CASE;.$&MACH;<filename>.clean_build</filename></emphasis></entry> 
<entry> Script to remove all object files and libraries and unlocks
<filename>Macros</filename> and
<filename>env_build.xml</filename>. This step is required before a
clean build of the system.</entry>
</row>

<row>
<entry><emphasis>$&CASE;.$&MACH;<filename>.l_archive</filename></emphasis></entry>
<entry>Script to performs long-term archiving of output data (see <link
linkend="running_ccsm_env_output">long-term archiving</link>).  This
script will only be created if long-term archiving is available on the
target machine.</entry>
</row>

<row>
<entry><emphasis>xmlchange</emphasis></entry>
<entry>Utility for modifying values in the xml files.</entry>
</row>

<row>
<entry><emphasis>preview_namelists</emphasis></entry>
<entry>Utility to enable users to see their component namelists in
$&CASEROOT;<filename>/CaseDocs</filename> before running the
model. NOTE: the namelists generated in
$&CASEROOT;/<filename>CaseDocs</filename> should not be edited by the
user - they are only their to document model behavior.</entry>
</row>

<row>
<entry><emphasis>check_input_data</emphasis></entry>
<entry>Utility that checks for various input datasets and moves them
into place.</entry>
</row>

<row>
<entry><emphasis>create_production_test</emphasis></entry> 
<entry>Utility used to create a test of your case.</entry>
</row>

</tbody>
</tgroup>
</table>

<para>
For more complete information about the files in the case directory, see
<xref linkend="faq_casestuff"/>
</para>

<para>
The xml variables in the <filename>env_*.xml</filename> files are
translated into csh environment variables with the same name by the
script <command>$&CASEROOT;/Tools/ccsm_getenv</command>. Conversion of xml
variables to environment variables is used by numerous script
utilities as part of building, and running a given
case. It is important to note that you do not explicitly see this
conversion.
</para>
<note><para>Users can only modify the xml variables. Users cannot
modify the csh environment variables directly.</para></note>

<para>Complete lists of &cesm; environment variables in the xml files
that appear in $&CASEROOT; are provided in
<ulink url="../modelnl/env_case.html">case variables </ulink>,
<ulink url="../modelnl/env_mach_pes.html">pe layout variables </ulink>,
<ulink url="../modelnl/env_build.html">build-time variables </ulink>, and 
<ulink url="../modelnl/env_run.html">run-time variables</ulink>.
</para>

</sect2>
</sect1>

<!-- ======================================================================= -->

<sect1 id="how_to_setup_case">
<title> How to set up a case and customize the PE layout</title>

<sect2>
<title>Calling <command>cesm_setup</command></title>

<para> The <command>cesm_setup</command> command does the following:

<itemizedlist>
<listitem><para>
Creates the Macros file if it does not exist. Calling <command>cesm_setup -clean</command>
does not remove this file. 
</para></listitem>

<listitem><para> 
Creates the files, <filename>user_nl_xxx</filename>, (where xxx
denotes the set of components targeted for the specific case).  As an
example, for a B_ compset, xxx would denote
[cam,clm,rtm,cice,pop2,cpl].  <emphasis>In CESM1.2, these files are
where all user component namelist modifications are now
made. </emphasis> <command>cesm_setup -clean</command> does not remove
these files
</para></listitem>

<listitem><para> Creates the file
<filename>$CASEROOT/$CASE.run</filename> which runs the &cesm;
model and performs short-term archiving of output data (see <link
linkend="running_ccsm">running &cesm;</link>).
<command>cesm_setup</command> also contains the necessary batch directives
to run the model on the required machine for the requested PE layout.
Any user modifications to &env_mach_pes.xml; must be done
<emphasis>before</emphasis> &cesm_setup; is invoked. In the simplest case,
&cesm_setup; can be run without modifying this file and default settings
will be then be used.  The &cesm_setup; command must be run in the
$&CASEROOT; directory. 
</para></listitem>
</itemizedlist>

<command>cesm_setup -clean</command> moves 
<filename>$CASEROOT/$CASE.run</filename> and a copy of 
<filename>env_mach_pes.xml</filename> to a time-stamped directory in
<filename>MachinesHist</filename>. The $&CASEROOT; directory will now
appear as if &create_newcase; had just been run with the exception
that already created Macros and user_nl_xxx files will not be touched
and local modifications to the <filename>env_*.xml</filename> files
will be preserved. After further modifications are made to
&env_mach_pes.xml;,
<command>cesm_setup</command> must be rerun before you can build and run the
model.
</para>

<para>
If &env_mach_pes.xml; variables need to be changed after &cesm_setup; has been
called, then <command>cesm_setup -clean</command> must be run first,
followed by <command>cesm_setup</command>.
</para>

<para>
The following summarizes the new directories and files that are
created by <command>cesm_setup</command>. For more information
about the files in the case directory, see
<xref linkend="faq_casestuff"/>.
</para>

<table><title> Result of calling &cesm_setup; </title>
<tgroup cols="2">
<thead>
<row>
<entry>File or Directory</entry>
<entry>Description</entry>
</row>
</thead>
<tbody>

<row>
<entry>Macros </entry>
<entry>File containing machine-specific makefile directives for your
target platform/compiler. This is only created the
<emphasis>first</emphasis> time that <command>cesm_setup</command> is
called. Calling <command>cesm_setup -clean</command> will not remove the
Macros file once it has been created.
</entry>
</row>

<row>
<entry>user_nl_xxx[_NNNN] files </entry>
<entry>Files where all user modifications to component namelists are
made. xxx denotes the set of components targeted for the specific
case. NNNN goes from 0001 to the number of instances of that
component (see the <link linkend="use_case_intro">multiple
instance</link> discussion below).  For example, for a B_ compset, xxx
would denote [cam,clm,rtm,cice,pop2,cpl]. For a case where there is only 1
instance of each component (default) NNNN will not appear in the
user_nl file names.  A user_nl file of a given name will only be
created once. Calling <command>cesm_setup -clean</command> will not remove
any user_nl files. Changing the number of instances in the
<filename>env_mach_pes.xml</filename> will only cause new user_nl files
to be added to $&CASEROOT;.
</entry>
</row>

<row>
<entry>$CASE.run</entry> <entry>File containing the
necessary batch directives to run the model on the required machine
for the requested PE layout.  Runs the &cesm; model and performs
short-term archiving of output data (see <link
linkend="running_ccsm">running &cesm;</link>).
</entry>
</row>

<row>
<entry>CaseDocs/</entry> <entry>Directory that
contains all the component namelists for the run. This is for reference
only and files in this directory SHOULD NOT BE EDITED since they will
be overwritten at build time and run time.
</entry>
</row>

<row>
<entry>env_derived</entry>
<entry>File containing environmental variables derived from other settings.
Should <emphasis>not</emphasis> be modified by the user.</entry>
</row>

</tbody>
</tgroup>
</table>

</sect2>

<!-- ======================================================================= -->
<sect2 id="case_conf_setting_pes">
<title>Changing the PE layout </title>

<para> 
<ulink url="../modelnl/env_mach_pes.html">&env_mach_pes.xml</ulink>
variables determine the number of processors for each component, the
number of instances of each component and the layout of the components
across the hardware processors.  Optimizing the throughput and
efficiency of a &cesm; experiment often involves customizing the
processor (PE) layout for <link
linkend="running_ccsm_loadbalance">load balancing</link>.  CESM has
significant flexibility with respect to the layout of components
across different hardware processors. In general, the &cesm;
components -- atm, lnd, ocn, ice, glc, rof, and cpl -- can run on
overlapping or mutually unique processors. Whereas Each component is
associated with a unique MPI communicator, the driver runs on the
union of all processors and controls the sequencing and hardware
partitioning. The component processor layout is via three settings:
the number of MPI tasks, the number of OpenMP threads per task, and
the root MPI processor number from the global set.</para>

<para>For example, the following &env_mach_pes.xml; settings</para>

<screen>
&lt;entry id="NTASKS_OCN" value="128" /&gt;
&lt;entry id="NTHRDS_OCN" value="1" /&gt;
&lt;entry id="ROOTPE_OCN" value="0" /&gt;
</screen>
<para>
would cause the ocean component to run on
128 hardware processors with 128 MPI tasks using one
thread per task starting from global MPI task 0 (zero).
</para>
<para>In this next example: </para>
<screen>
&lt;entry id="NTASKS_ATM" value="16" /&gt;
&lt;entry id="NTHRDS_ATM" value="4"  /&gt;
&lt;entry id="ROOTPE_ATM" value="32" /&gt;
</screen>
<para>
the atmosphere component will run on 64 hardware processors using 16
MPI tasks and 4 threads per task starting at global MPI task 32.
There are NTASKS, NTHRDS, and ROOTPE input variables for every
component in &env_mach_pes.xml;.  There are some important things
to note.</para>

<itemizedlist>
<listitem><para>
NTASKS must be greater or equal to 1 (one) even for inactive (stub) components.
</para></listitem>

<listitem><para>
NTHRDS must be greater or equal to 1 (one).  If NTHRDS is set to 1, this
generally means threading parallelization will be off for that component.
NTHRDS should never be set to zero.
</para></listitem>

<listitem><para>
The total number of hardware processors allocated to a component is
NTASKS * NTHRDS.
</para></listitem>

<listitem><para>
The coupler processor inputs specify the pes used by coupler 
computation such as mapping, merging, diagnostics, and flux
calculation.  This is distinct from the driver which always 
automatically runs on the union
of all processors to manage model concurrency and sequencing.
</para></listitem>

<listitem><para>
The root processor is set relative to the MPI global communicator, not the
hardware processors counts.  An example of this is below.
</para></listitem>

<listitem><para>
The layout of components on processors has no impact on the science.
The scientific sequencing is hardwired into the driver.  Changing 
processor layouts does not change intrinsic coupling lags or coupling
sequencing.  ONE IMPORTANT POINT is that for a fully 
active configuration, the atmosphere component is hardwired in the driver to 
never run concurrently with the land or ice component.  Performance 
improvements associated with processor layout concurrency is therefore
constrained in this case such that there is never a performance reason
not to overlap the atmosphere component with the land and ice components.
Beyond that constraint, the land, ice, coupler and ocean models can
run concurrently, and the ocean model can also run concurrently with the 
atmosphere model.  
</para></listitem>

<listitem><para>
If all components have identical NTASKS, NTHRDS, and ROOTPE
set, all components will run sequentially on the same hardware
processors.
</para></listitem>

</itemizedlist>

<para>The root processor is set relative to the MPI global
communicator, not the hardware processor counts.
For instance, in the following example:</para>

<screen>
&lt;entry id="NTASKS_ATM" value="16" /&gt;
&lt;entry id="NTHRDS_ATM" value="4"  /&gt;
&lt;entry id="ROOTPE_ATM" value="0"  /&gt;
&lt;entry id="NTASKS_OCN" value="64" /&gt;
&lt;entry id="NTHRDS_OCN" value="1"  /&gt;
&lt;entry id="ROOTPE_OCN" value="16" /&gt;
</screen>

<para>
the atmosphere and ocean are running concurrently, each on 64
processors with the atmosphere running on MPI tasks 0-15 and the ocean
running on MPI tasks 16-79. The first 16 tasks are each threaded 4
ways for the atmosphere. The batch submission script ($CASE.run)
should automatically request 128 hardware processors, and the first 16
MPI tasks will be laid out on the first 64 hardware processors with a
stride of 4. The next 64 MPI tasks will be laid out on the second set
of 64 hardware processors.
</para>

<para>
If you set ROOTPE_OCN=64 in the preceding example, then a total of
176 processors would have been requested and the atmosphere would
have been laid out on the first 64 hardware processors in 16x4
fashion, and the ocean model would have been laid out
on hardware processors 113-176. Hardware processors 65-112 would
have been allocated but completely idle.  
</para>

<note><para>&env_mach_pes.xml; <emphasis>cannot</emphasis> be modified
after "./cesm_setup" has been invoked without first invoking
"cesm_setup -clean". For an example of changing pes, see <xref
linkend="use_case_pelayout"/>  </para></note>

</sect2>

</sect1>

<!-- ======================================================================= -->

<sect1 id="multiinst">
<title>Multi-instance component functionality</title>

<para> Like the CESM1.1 series, the CESM1.2 series also has the
new capability to run multiple component instances under one model
executable.  The only caveat to this usage is that if N multiple
instances of any one active component is used, then N multiple
instances of ALL active components are required. More details are
discussed below. The primary motivation for this development was to be
able to run an ensemble Kalman-Filter for data assimilation and
parameter estimation (e.g. UQ).  However, it also provides you with
the ability to run a set of experiments within a single CESM
executable where each instance can have a different namelist, and have
all the output go to one directory.</para>

<para> In the following an F compset will be used as an
illustration. Utilizing the multiple instance code involves the
following steps: </para>
<orderedlist>

<listitem><para>
create the case
<screen>
> create_newcase -case Fmulti -compset F  -res ne30_g16 -mach hopper
> cd Fmulti
</screen>
</para></listitem>
<listitem><para>
Lets assume the following out of the box pe-layout for hopper:
<screen>
&lt;entry id="NTASKS_ATM"   value="128"  /&gt;    
&lt;entry id="NTHRDS_ATM"   value="1"  /&gt;    
&lt;entry id="ROOTPE_ATM"   value="0"  /&gt;    
&lt;entry id="NINST_ATM"   value="1"  /&gt;    
&lt;entry id="NINST_ATM_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_LND"   value="128"  /&gt;    
&lt;entry id="NTHRDS_LND"   value="1"  /&gt;    
&lt;entry id="ROOTPE_LND"   value="0"  /&gt;    
&lt;entry id="NINST_LND"   value="1"  /&gt;    
&lt;entry id="NINST_LND_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_ICE"   value="128"  /&gt;    
&lt;entry id="NTHRDS_ICE"   value="1"  /&gt;    
&lt;entry id="ROOTPE_ICE"   value="0"  /&gt;    
&lt;entry id="NINST_ICE"   value="1"  /&gt;    
&lt;entry id="NINST_ICE_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_OCN"   value="128"  /&gt;    
&lt;entry id="NTHRDS_OCN"   value="1"  /&gt;    
&lt;entry id="ROOTPE_OCN"   value="0"  /&gt;    
&lt;entry id="NINST_OCN"   value="1"  /&gt;    
&lt;entry id="NINST_OCN_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_GLC"   value="128"  /&gt;    
&lt;entry id="NTHRDS_GLC"   value="1"  /&gt;    
&lt;entry id="ROOTPE_GLC"   value="0"  /&gt;    
&lt;entry id="NINST_GLC"   value="1"  /&gt;    
&lt;entry id="NINST_GLC_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_WAV"   value="128"  /&gt;    
&lt;entry id="NTHRDS_WAV"   value="1"  /&gt;    
&lt;entry id="ROOTPE_WAV"   value="0"  /&gt;    
&lt;entry id="NINST_WAV"   value="1"  /&gt;    
&lt;entry id="NINST_WAV_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_CPL"   value="128"  /&gt;    
&lt;entry id="NTHRDS_CPL"   value="1"  /&gt;    
&lt;entry id="ROOTPE_CPL"   value="0"  /&gt;    
</screen>
</para>

<para>
For an F compset, only atm, lnd, rof are full prognostic components.
The ocn is a prescribed data component, cice is a mixed
prescribed/prognostic component (ice-coverage is prescribed) and glc
and wav are stub components.
Lets say we want to run 2 instances of CAM in this experiment. The
current implementation of multi-instances will also require you to run
2 instances of CLM, CICE and RTM. However, you have the flexibility to run either 1
or 2 instances of DOCN (we can ignore glc and wav since they do not do
anything in this compset). To run 2 instances of CAM, CLM, CICE, RTM and DOCN, all you
need to do is to change NINST_ATM, NINST_LND, NINST_ICE, NINST_ROF and NINST_DOCN above from 1 to 2.
This will result in the following &env_mach_pes.xml; file:
</para><para>
<screen>
&lt;entry id="NTASKS_ATM"   value="128"  /&gt;    
&lt;entry id="NTHRDS_ATM"   value="1"  /&gt;    
&lt;entry id="ROOTPE_ATM"   value="0"  /&gt;    
&lt;entry id="NINST_ATM"   value="2"  /&gt;    
&lt;entry id="NINST_ATM_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_LND"   value="128"  /&gt;    
&lt;entry id="NTHRDS_LND"   value="1"  /&gt;    
&lt;entry id="ROOTPE_LND"   value="0"  /&gt;    
&lt;entry id="NINST_LND"   value="2"  /&gt;    
&lt;entry id="NINST_LND_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_ICE"   value="128"  /&gt;    
&lt;entry id="NTHRDS_ICE"   value="1"  /&gt;    
&lt;entry id="ROOTPE_ICE"   value="0"  /&gt;    
&lt;entry id="NINST_ICE"   value="2"  /&gt;    
&lt;entry id="NINST_ICE_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_ROF"   value="128"  /&gt;    
&lt;entry id="NTHRDS_ROF"   value="1"  /&gt;    
&lt;entry id="ROOTPE_ROF"   value="0"  /&gt;    
&lt;entry id="NINST_ROF"   value="2"  /&gt;    
&lt;entry id="NINST_ROF_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_OCN"   value="128"  /&gt;    
&lt;entry id="NTHRDS_OCN"   value="1"  /&gt;    
&lt;entry id="ROOTPE_OCN"   value="0"  /&gt;    
&lt;entry id="NINST_OCN"   value="2"  /&gt;    
&lt;entry id="NINST_OCN_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_GLC"   value="128"  /&gt;    
&lt;entry id="NTHRDS_GLC"   value="1"  /&gt;    
&lt;entry id="ROOTPE_GLC"   value="0"  /&gt;    
&lt;entry id="NINST_GLC"   value="1"  /&gt;    
&lt;entry id="NINST_GLC_LAYOUT"   value="concurrent"  /&gt;    

&lt;entry id="NTASKS_CPL"   value="128"  /&gt;    
&lt;entry id="NTHRDS_CPL"   value="1"  /&gt;    
&lt;entry id="ROOTPE_CPL"   value="0"  /&gt;    
</screen>
</para><para>
As a result of this, you will have 2 instances of CAM, CLM and CICE (prescribed) and RTM, each running concurrently on
64 MPI tasks - and only 1 instance of DOCN.
</para></listitem>

<listitem><para> A separate user_nl_xxx_NNNN file (where NNNN is the
number of the component instances) will be generated
when <command>cesm_setup</command> is called. In particular,
calling <command>cesm_setup</command> with the above &env_mach_pes.xml; file
will result in the following <filename>user_nl_*</filename> files in
$&CASEROOT;
<screen>
user_nl_cam_0001
user_nl_cam_0002
user_nl_cice_0001
user_nl_cice_0002
user_nl_clm_0001
user_nl_clm_0002
user_nl_cpl
user_nl_docn_0001
user_nl_docn_0002
user_nl_rtm_0001
user_nl_rtm_0002
</screen>
</para><para> and the following <filename>*_in_*</filename> files
and <filename>*txt*</filename> files in $CASEROOT/CaseDocs:
</para><para>
<screen>
atm_in_0001
atm_in_0002
docn.streams.txt.prescribed_0001
docn.streams.txt.prescribed_0002
docn_in_0001
docn_in_0002
docn_ocn_in_0001
docn_ocn_in_0002
drv_flds_in
drv_in
ice_in_0001
ice_in_0002
lnd_in_0001
lnd_in_0002
rof_in_0001
rof_in_0002
</screen>
</para><para> The namelist for each component instance can be modified
by changing the corresponding user_nl_xxx_NNNN file for that component
instance. Modifying the user_nl_cam_0002 will result in the namelist
changes you put in to be active ONLY for instance 2 of CAM. To change the
DOCN stream txt file instance 0002, you should place a copy of 
<filename>docn.streams.txt.prescribed_0002</filename> in $&CASEROOT;
with the name <filename>user_docn.streams.txt.prescribed_0002</filename>
and modify it accordlingly.
</para></listitem>

</orderedlist>

<para> 
It is also important to stress the following points:
</para>

<orderedlist>

<listitem><para> <command>Different component instances can ONLY
differ by differences in namelist settings - they are ALL using the
same model executable.</command> </para></listitem>

<listitem><para> Only 1 coupler component is supported in the CESM1.2 series
multiple instance implementation.  </para></listitem>

<listitem><para> <filename>user_nl_*</filename> files once they are
created by <command>cesm_setup</command> <emphasis>ARE NOT</emphasis> removed by
calling <command>cesm_setup -clean</command>. The same is true for
<filename>Macros</filename> files.  </para></listitem>

<listitem><para> In general, you should run multiple instances
concurrently (the default setting in &env_mach_pes.xml;).  The serial
setting is only for EXPERT USERS in upcoming development code
implementations.</para></listitem>

</orderedlist>

</sect1>

<!-- ======================================================================= -->

<sect1 id="modifying_xml">
<title>Modifying an xml file</title>

<para>
You can edit the xml files directly to change the variable
values. However, modification of the xml variables is best
done using <command>xmlchange</command> in the $&CASEROOT;
directory since it performs variable error checking as part of
changing values in the xml files. <command>xmlchange</command>
also records changes to the CaseStatus file for future reference
and reproducibility. To invoke <command> xmlchange</command>:
</para>

<screen>
xmlchange &lt;entry id&gt;=&lt;value&gt;
-- OR --
xmlchange -id &lt;entry id&gt; -val &lt;name&gt; -file &lt;filename&gt;
          [-help] [-silent] [-verbose] [-warn] [-append] [-file]
</screen>

<variablelist>

<varlistentry><term><option>-id</option></term>
<listitem>
<para>
The xml variable name to be changed. (required)
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-val</option></term>
<listitem>
<para>
The intended value of the variable associated with the -id argument. (required)
</para>
<note><para>If you want a single quotation mark ("'", also called an
apostrophe) to appear in the string provided by the -val option,
you must specify it as "&amp;apos;".
</para></note>
</listitem>
</varlistentry>

<varlistentry><term><option>-file</option></term>
<listitem>
<para>
The xml file to be edited. (optional)
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-silent</option></term>
<listitem>
<para>
Turns on silent mode. Only fatal messages will be issued. (optional)
</para>
</listitem>
</varlistentry>
<varlistentry><term><option>-verbose</option></term>

<listitem>
<para>
Echoes all settings made by <command>create_newcase</command> and
<command>cesm_setup</command>. (optional)
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-help</option></term>
<listitem>
<para>
Print usage info to STDOUT. (optional)
</para>
</listitem>
</varlistentry>
</variablelist>

</sect1>

<!-- ======================================================================= -->
<sect1 id="cloning_case">
<title>Cloning a case (Experts only)</title>

<para>
This is an advanced feature provided for expert users. If you are
a new user, skip this section.</para>

<para> If you have access to the run you want to clone, the
&create_clone; command will create a new case while also preserving
local modifications to the case that you want to clone. You can run the
utility &create_clone; either from $&CIMEROOT; or from the directory
where you want the new case to be created. It has the following
arguments:</para>

<variablelist>
<varlistentry><term><option>-case</option></term>
<listitem>
<para>
The name or path of the new case.
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-clone</option></term>
<listitem>
<para>
The full pathname of the case to be cloned.
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-silent</option></term>
<listitem>
<para>
Enables silent mode. Only fatal messages will be issued.
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-verbose</option></term>
<listitem>
<para>
Echoes all settings.
</para>
</listitem>
</varlistentry>

<varlistentry><term><option>-help</option></term>
<listitem>
<para>
Prints usage instructions.
</para>
</listitem>
</varlistentry>
</variablelist>

<para>
Here is the simplest example of using &create_clone;:
</para>

<screen>
> cd $&CIMEROOT;/scripts
> create_clone -case $&CASEROOT; -clone $&CLONEROOT; 
</screen>

<para>
&create_clone; will preserve any local namelist modifications made in the user_nl_xxxx files as well as any source code modifications in the SourceMods tree.   Note that the new case directory will be
identical to the cloned case directory 
<emphasis>except</emphasis> for the original cloned scripts
$&CASEROOT;.$&MACH;.build, $&CASEROOT;.$&MACH;.clean_build,
$&CASEROOT;.$&MACH;.run, and $&CASEROOT;.$&MACH;.l_archive,
which will have new names in the new case.
</para>

<note><title>Important:</title>
<para>Do not change anything in the &env_case.xml;
file.  The $&CASEROOT;/ directory will now
appear as if &create_newcase; had just been run -- with the exception
that local modifications to the env_* files are preserved. 
</para></note>

<para>
Another approach to duplicating a case is to use the information in
that case's README.case file to create a new case. Note that this
approach will <emphasis>not</emphasis> preserve any local
modifications that were made to the original case, such as source-code
or build-script modifications; you will need to import those
changes manually.</para>

</sect1>

<!-- ======================================================================= -->

</chapter>

