#!/usr/bin/env python

"""
Jenkins runs this script to perform a test of an acme
test suite.
"""

import acme_util
acme_util.check_minimum_python_version(2, 7)
import wait_for_tests

import argparse, sys, os, getpass, socket, shutil, glob, doctest

from acme_util import expect, warning, verbose_print

SENTINEL_FILE = "ONGOING_TEST"

###############################################################################
def parse_command_line(args, description):
###############################################################################
    parser = argparse.ArgumentParser(
usage="""\n%s [-g] [-d] [--verbose]
OR
%s --help
OR
%s --test

\033[1mEXAMPLES:\033[0m
    \033[1;32m# Run the tests and compare baselines \033[0m
    > %s
    \033[1;32m# Run the tests, compare baselines, and update dashboard \033[0m
    > %s -d
    \033[1;32m# Run the tests, generating a full set of baselines (useful for first run on a machine) \033[0m
    > %s -g
    \033[1;32m# From most recent run, bless any namelist changes \033[0m
    > %s -b -n
    \033[1;32m# From most recent run, bless all changes \033[0m
    > %s -b
    \033[1;32m# From most recent run, bless changes to test foo and bar only \033[0m
    > %s -b foo bar
    \033[1;32m# From most recent run, bless only namelist changes to test foo and bar only \033[0m
    > %s -n -b foo bar
""" % ((os.path.basename(args[0]), ) * 10),

description=description,

formatter_class=argparse.ArgumentDefaultsHelpFormatter
)

    parser.add_argument("-v", "--verbose", action="store_true", dest="verbose", default=False,
                        help="Print extra information")

    parser.add_argument("-g", "--generate-baselines", action="store_true", dest="generate_baselines", default=False,
                        help="Generate baselines")

    parser.add_argument("-d", "--submit-to-dashboard", action="store_true", dest="submit_to_dashboard", default=False,
                        help="Send results to Cdash")

    parser.add_argument("-n", "--namelists-only", action="store_true", dest="namelists_only", default=False,
                        help="Only compare/generate namelists. Useful for quickly blessing namelist changes")

    parser.add_argument("--branch", action="store", dest="branch", default=None,
                        help="Force baseline actions (compare/generate) to use baselines for a specific branch instead of the current branch. Also impacts dashboard job name. Useful for testing a branch other than next or master")

    parser.add_argument("-b", "--bless", action="store_true", dest="bless", default=False,
                        help="Update baselines by copying diffing files from most recent nightly run directly into baselines")

    parser.add_argument("-t", "--test-suite", action="store", dest="test_suite", default=None,
                        help="Override default acme test suite that will be run")

    parser.add_argument("-p", "--cdash-project", action="store", dest="cdash_project", default=wait_for_tests.ACME_MAIN_CDASH,
                        help="The name of the CDash project where results should be uploaded")

    parser.add_argument("bless_tests", nargs="*",
                        help="When blessing, limit the bless to tests matching these regex")

    args = parser.parse_args(args[1:])

    acme_util.set_verbosity(args.verbose)

    expect(not (args.generate_baselines and args.bless),
           "Does not make sense to use -g and -r together")
    expect(not (args.submit_to_dashboard and args.generate_baselines),
           "Does not make sense to use -g and -d together")
    expect(not (args.submit_to_dashboard and args.bless),
           "Does not make sense to use -r and -d together")
    expect(not (len(args.bless_tests) > 0 and not args.bless),
           "Providing specific test names only makes sense with -r")

    return args.generate_baselines, args.submit_to_dashboard, args.branch, args.namelists_only, args.bless, args.test_suite, args.cdash_project, args.bless_tests

###############################################################################
def cleanup_queue(set_of_jobs_we_created, batch_system):
###############################################################################
    """
    Delete all jobs left in the queue
    """
    current_jobs = set(acme_util.get_my_queued_jobs(batch_system))
    jobs_to_delete = set_of_jobs_we_created & current_jobs

    for job_to_delete in jobs_to_delete:
        warning("Found leftover job: %s" % job_to_delete)
        del_cmd = "%s %s" % (acme_util.get_batch_system_info(batch_system)[1], job_to_delete)
        stat = acme_util.run_cmd(del_cmd, verbose=True, ok_to_fail=True)[0]
        if (stat != 0):
            warning("FAILED to clean up leftover job: %s" % job_to_delete)
        else:
            warning("Deleted leftover job: %s" % job_to_delete)

###############################################################################
def bless_test_results(casearea, baseline_root, git_branch, namelists_only, bless_tests):
###############################################################################
    test_results = wait_for_tests.get_test_results(glob.glob("%s/*/TestStatus" % casearea),
                                                   True, # no wait
                                                   False, # don't check throughput
                                                   False) # don't ignore namelist diffs

    baseline_area = os.path.join(baseline_root, git_branch)

    for test_name, test_data in test_results.iteritems():
        if (bless_tests in [[], None] or acme_util.match_any(test_name, bless_tests)):
            test_result = test_data[1]

            if (test_result == wait_for_tests.TEST_PASSED_STATUS):
                if (bless_tests not in [[], None]):
                    warning("Asked to bless test %s, but it passed" % test_name)
            else:

                print "Blessing results for test:", test_name, "most recent result:", test_result

                # Get baseline dir for this test
                baseline_dir_for_test = os.path.join(baseline_area, test_name)
                expect(os.path.isdir(baseline_dir_for_test),
                       "Problem, baseline dir '%s' does not exist" % baseline_dir_for_test)

                # Get testcase dir for this test
                globs = glob.glob("%s*" % os.path.join(casearea, test_name))
                expect(len(globs) == 1, "Expected exactly one match for testcase area for test '%s', found '%s'" % (test_name, globs))
                testcase_dir_for_test = globs[0]

                # Find files of various types
                namelist_files = []
                other_files = []
                for root, _, files in os.walk(baseline_dir_for_test):
                    if (root == baseline_dir_for_test):
                        rel_root = ""
                    else:
                        rel_root = root.replace("%s/" % baseline_dir_for_test, "")

                    for file_ in files:
                        rel_file = os.path.join(rel_root, file_)

                        if (rel_root == "CaseDocs" or file_.startswith("user_nl")):
                            # TODO: Do a namelist diff and only copy if different?
                            namelist_files.append(rel_file)
                        else:
                            other_files.append(rel_file)

                # Update namelist files
                acme_util.safe_copy(testcase_dir_for_test, baseline_dir_for_test, namelist_files)

                # Update other files
                if (test_result != wait_for_tests.NAMELIST_FAIL_STATUS and not namelists_only):
                    acme_util.safe_copy(testcase_dir_for_test, baseline_dir_for_test, other_files)

###############################################################################
def jenkins_generic_job(generate_baselines, submit_to_dashboard,
                        baseline_branch=None, namelists_only=False, bless=False,
                        arg_test_suite=None, cdash_project=None, bless_tests=None):
###############################################################################
    acme_machine = acme_util.probe_machine_name()
    expect(acme_machine is not None,
           "Did not recognize current machine '%s'" % socket.gethostname())

    acme_repo = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    expect(os.path.isdir(os.path.join(acme_repo, ".git")),
           "Expected '%s' to be a git repository" % acme_repo)

    compiler, default_test_suite, use_batch, project, testroot, baseline_root, proxy = \
        acme_util.get_machine_info(acme_machine)
    test_suite = default_test_suite if arg_test_suite is None else arg_test_suite
    testroot = testroot.replace("<USER>", getpass.getuser()).replace("<PROJECT>", project)
    casearea = os.path.join(testroot, "jenkins")
    git_branch = acme_util.get_current_branch(repo=acme_repo) if baseline_branch is None else baseline_branch

    if (bless):
        bless_test_results(casearea, baseline_root, git_branch, namelists_only, bless_tests)
        return True

    if (use_batch):
        batch_system = acme_util.probe_batch_system()
        expect(batch_system is not None, "Failed to probe batch system")

    #
    # Env changes
    #

    if (submit_to_dashboard and proxy is not None):
        os.environ["http_proxy"] = proxy

    #
    # Cleanup previous test leftovers. Code beyond here assumes that
    # we have the testroot and casearea to ourselves. No other ACME testing
    # should be happening for the current user. A sentinel file helps
    # enforce this.
    #

    sentinel_path = os.path.join(testroot, SENTINEL_FILE)
    expect(not os.path.isfile(sentinel_path),
           "Tests were already in progress, cannot start more!")

    # Very tiny race window here, not going to sweat it
    open(sentinel_path, 'w').close()

    try:

        if (os.path.isdir("Testing")):
            shutil.rmtree("Testing")

        for old_dir in glob.glob("%s/*%s*" % (testroot, acme_machine)):
            shutil.rmtree(old_dir)

        if (os.path.isdir(casearea)):
            shutil.rmtree(casearea)

        #
        # Make note of things already in the queue so we know not to delete
        # them if we timeout
        #

        if (use_batch):
            preexisting_queued_jobs = acme_util.get_my_queued_jobs(batch_system)

        #
        # Set up create_test command and run it
        #

        os.chdir(os.path.join(acme_repo, "scripts"))
        git_branch = acme_util.get_current_branch() if baseline_branch is None else baseline_branch
        baseline_action = "-generate" if generate_baselines else "-compare"
        create_test_cmd = "./create_test -xml_mach %s -xml_compiler %s -xml_category %s -testroot %s -project %s %s %s" % \
                          (acme_machine, compiler, test_suite, casearea, project, baseline_action, git_branch)

        if (namelists_only):
            create_test_cmd += " -nlcompareonly"

        if (not use_batch):
            create_test_cmd += " -autosubmit off -nobatch on"

        acme_util.run_cmd(create_test_cmd, verbose=True, arg_stdout=None, arg_stderr=None)

        os.chdir("../..")

        # TODO: the testing scripts should produce all PASS when only generating namelists
        if (namelists_only and generate_baselines):
            return True

        if (use_batch):
            # This is not fullproof. Any jobs that happened to be
            # submitted by this user while create_test was running will be
            # potentially deleted. This is still a big improvement over the
            # previous implementation which just assumed all queued jobs for this
            # user came from create_test.
            # TODO: change this to probe casearea for jobs ids
            our_jobs = set(acme_util.get_my_queued_jobs(batch_system)) - set(preexisting_queued_jobs)

        #
        # Wait for tests
        #

        cdash_build_name = "_".join([test_suite, git_branch, compiler]) if submit_to_dashboard else None
        tests_passed = wait_for_tests.wait_for_tests(glob.glob("%s/*/TestStatus" % casearea),
                                                     not use_batch, # wait if using queue
                                                     False, # don't check throughput
                                                     False, # don't ignore namelist diffs
                                                     cdash_build_name,
                                                     cdash_project)
        if (not tests_passed and use_batch):
            # Cleanup
            cleanup_queue(our_jobs, batch_system)

        return tests_passed
    finally:
        expect(os.path.isfile(sentinel_path), "Missing sentinel file")
        os.remove(sentinel_path)

###############################################################################
def _main_func(description):
###############################################################################
    if ("--test" in sys.argv):
        test_results = doctest.testmod(verbose=True)
        sys.exit(1 if test_results.failed > 0 else 0)

    acme_util.stop_buffering_output()

    generate_baselines, submit_to_dashboard, baseline_branch, namelists_only, bless, test_suite, cdash_project, bless_tests = \
        parse_command_line(sys.argv, description)

    sys.exit(0 if jenkins_generic_job(generate_baselines, submit_to_dashboard, baseline_branch, namelists_only, bless, test_suite, cdash_project, bless_tests) else 1)

###############################################################################

if (__name__ == "__main__"):
    _main_func(__doc__)
