<?xml version="1.0"?>

<config_machines version="2.0">

  <!--

   ===============================================================
   COMPILER and COMPILERS
   ===============================================================
   If a machine supports multiple compilers - then
    - the settings for COMPILERS should reflect the supported compilers
      as a comma separated string
    - the setting for COMPILER should be the default compiler
      (which is one of the values in COMPILERS)

   ===============================================================
   MPILIB and MPILIBS
   ===============================================================
   If a machine supports only one MPILIB is supported - then
   the setting for  MPILIB and MPILIBS should be blank ("")
   If a machine supports multiple mpi libraries (e.g. mpich and openmpi)
    - the settings for MPILIBS should reflect the supported mpi libraries
      as a comma separated string

   The default settings for COMPILERS and MPILIBS is blank (in config_machines.xml)

   Normally variable substitutions are not made until the case scripts are run, however variables
   of the form $ENV{VARIABLE_NAME} are substituted in create_newcase from the environment
   variable of the same name if it exists.

   ===============================================================
   PROJECT_REQUIRED
   ===============================================================
   A machine may need the PROJECT xml variable to be defined either because it is
   used in some paths, or because it is used to give an account number in the job
   submission script. If either of these are the case, then PROJECT_REQUIRED
   should be set to TRUE for the given machine.


   walltimes:
   Denotes the walltimes that can be used for a particular machine.
   walltime: as before, if default="true" is defined, this walltime will be used
   by default.
   Alternatively, ccsm_estcost must be used to choose the queue based on the estimated cost of the run.

   mpirun: the mpirun command that will be used to actually launch the model.
   The attributes used to choose the mpirun command are:

   mpilib: can either be 'default' the name of an mpi library, or a compiler name so one can choose the mpirun
           based on the mpi library in use.

     the 'executable' tag must have arguments required for the chosen mpirun, as well as the executable name.

   unit_testing: can be 'true' or 'false'.
     This allows using a different mpirun command to launch unit tests

  -->

  <machine MACH="miller">
    <DESC> ORNL AF cluster 800-node AMD Epyc 2-sockets 64-cores per node</DESC>
    <OS>CNL</OS>
    <COMPILERS>gnu,cray,intel</COMPILERS>
    <MPILIBS>mpt</MPILIBS>
    <PROJECT>nwp501</PROJECT>
    <SAVE_TIMING_DIR>/lustre/storm/nwp501/proj-shared/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,nwp501</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lustre/storm/nwp501/proj-shared/e3sm/e3sm_scratch/</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lustre/storm/nwp501/proj-shared/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/storm/nwp501/proj-shared/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/storm/nwp501/proj-shared/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lustre/storm/nwp501/proj-shared/e3sm/tools/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>miller_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit </arg>
        <arg name="thread_count">-c $SHELL{echo 128/ {{ tasks_per_node }} |bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/cray/pe/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/cray/pe/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/cray/pe/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/cray/pe/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/cray/pe/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/cray/pe/modules/default/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="unload">craype</command>
        <command name="unload">cray-mpich</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="load">craype/2.7.15</command>
      </modules>
      <modules compiler="gnu">
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="load">PrgEnv-gnu/8.3.3</command>
        <command name="swap">gcc/12.1.0</command>
      </modules>
      <modules compiler="cray">
        <command name="unload">PrgEnv-gnu</command>
        <command name="load">gcc/9.3.0</command>
        <command name="load">PrgEnv-cray/8.3.3</command>
        <command name="rm">darshan</command>
      </modules>
      <modules>
        <command name="load">cray-mpich/8.1.16</command>
        <command name="load">cray-hdf5-parallel/1.12.1.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.8.1.3</command>
        <command name="load">cray-parallel-netcdf/1.12.2.3</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>1000</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="PERL5LIB">/usr/lib/perl5/5.26.2</env>
      <env name="NETCDF_C_PATH">/opt/cray/pe/netcdf-hdf5parallel/4.8.1.3/gnu/9.1/</env>
      <env name="NETCDF_FORTRAN_PATH">/opt/cray/pe/netcdf-hdf5parallel/4.8.1.3/gnu/9.1/</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="pm-cpu">
    <DESC>Perlmutter CPU-only nodes at NERSC.  Phase2 only: Each node has 2 AMD EPYC 7713 64-Core (Milan) 512GB</DESC>
    <NODENAME_REGEX>$ENV{NERSC_HOST}:perlmutter</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>intel,gnu,nvidia,amdclang</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/global/cfs/cdirs/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,m3411,m3412</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{PSCRATCH}/e3sm_scratch/pm-cpu</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/global/cfs/cdirs/e3sm/www/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>http://portal.nersc.gov/project/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>nersc_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 256/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/8.3.1/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cpe</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/12.3</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.5.0</command>
        <command name="load">intel/2023.2.0</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/24.5</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="amdclang">
        <command name="load">PrgEnv-aocc</command>
        <command name="load">aocc/4.1.0</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.30</command>
        <command name="load">cray-mpich/8.1.28</command>
        <command name="load">cray-hdf5-parallel/1.12.2.9</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.9</command>
        <command name="load">cray-parallel-netcdf/1.12.3.9</command>
        <command name="load">cmake/3.24.3</command>
        <command name="load">evp-patch</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_MPIIO_DVS_MAXNODES">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_MR_CACHE_MONITOR">kdreg2</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
    </environment_variables>
    <environment_variables compiler="intel" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/intel-2023.1.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/gcc-11.2.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="Albany_ROOT">$SHELL{if [ -z "$Albany_ROOT" ]; then echo /global/common/software/e3sm/albany/2024.03.26/gcc/11.2.0; else echo "$Albany_ROOT"; fi}</env>
      <env name="Trilinos_ROOT">$SHELL{if [ -z "$Trilinos_ROOT" ]; then echo /global/common/software/e3sm/trilinos/15.1.1/gcc/11.2.0; else echo "$Trilinos_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/nvidia-22.7; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia">
      <env name="BLAS_ROOT">$SHELL{if [ -z "$BLAS_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$BLAS_ROOT"; fi}</env>
      <env name="LAPACK_ROOT">$SHELL{if [ -z "$LAPACK_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$LAPACK_ROOT"; fi}</env>
      <env name="BLA_VENDOR">NVHPC</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="BLA_VENDOR">Intel10_64_dyn</env>
    </environment_variables>
    <environment_variables compiler="amdclang" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/aocc-4.0.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/intel; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/gnu; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="pm-gpu">
    <DESC>Perlmutter GPU nodes at NERSC.  Phase1 only: Each GPU node has single AMD EPYC 7713 64-Core (Milan) (256GB) and 4 nvidia A100's.</DESC>
    <NODENAME_REGEX>$ENV{NERSC_HOST}:perlmutter</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnugpu,gnu,nvidiagpu,nvidia</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>e3sm_g</PROJECT>
    <SAVE_TIMING_DIR>/global/cfs/cdirs/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,m3411,m3412</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{PSCRATCH}/e3sm_scratch/pm-gpu</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/global/cfs/cdirs/e3sm/www/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>http://portal.nersc.gov/project/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>nersc_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="gnu">256</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="nvidia">256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnu">64</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="nvidia">64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 128/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 64 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/8.3.1/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cpe</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu.*">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/12.3</command>
      </modules>

      <modules compiler="nvidia.*">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/24.5</command>
      </modules>

      <modules compiler="gnugpu">
        <command name="load">cudatoolkit/12.4</command>
        <command name="load">craype-accel-nvidia80</command>
      </modules>

      <modules compiler="nvidiagpu">
        <command name="load">cudatoolkit/12.4</command>
        <command name="load">craype-accel-nvidia80</command>
        <command name="load">gcc-native-mixed/12.3</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules>
        <command name="load">cray-libsci/23.12.5</command>
        <command name="load">craype/2.7.30</command>
        <command name="load">cray-mpich/8.1.28</command>
        <command name="load">cray-hdf5-parallel/1.12.2.9</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.9</command>
        <command name="load">cray-parallel-netcdf/1.12.3.9</command>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_MPIIO_DVS_MAXNODES">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_MR_CACHE_MONITOR">kdreg2</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
    </environment_variables>
    <environment_variables compiler="nvidiagpu">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/gnugpu ; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu.*" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/gcc-11.2.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia.*" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/nvidia-22.7; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="muller-cpu">
    <DESC>Muller CPU-only nodes on internal NERSC machine, similar to pm-cpu (very small)</DESC>
    <NODENAME_REGEX>$ENV{NERSC_HOST}:muller</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>intel,gnu,nvidia,amdclang</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/global/cfs/cdirs/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,m3411,m3412</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}/e3sm_scratch/muller-cpu</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/global/cfs/cdirs/e3sm/www/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>http://portal.nersc.gov/project/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>nersc_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 256/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/8.3.1/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cpe</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/12.3</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.5.0</command>
        <command name="load">intel/2024.1.0</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/24.5</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="amdclang">
        <command name="load">PrgEnv-aocc</command>
        <command name="load">aocc/4.1.0</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.30</command>
        <command name="load">cray-mpich/8.1.28</command>
        <command name="load">cray-hdf5-parallel/1.12.2.9</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.9</command>
        <command name="load">cray-parallel-netcdf/1.12.3.9</command>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_MPIIO_DVS_MAXNODES">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_MR_CACHE_MONITOR">kdreg2</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
    </environment_variables>
    <environment_variables compiler="intel" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/intel-2023.1.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/gcc-11.2.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="Albany_ROOT">$SHELL{if [ -z "$Albany_ROOT" ]; then echo /global/common/software/e3sm/albany/2024.03.26/gcc/11.2.0; else echo "$Albany_ROOT"; fi}</env>
      <env name="Trilinos_ROOT">$SHELL{if [ -z "$Trilinos_ROOT" ]; then echo /global/common/software/e3sm/trilinos/15.1.1/gcc/11.2.0; else echo "$Trilinos_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/nvidia-22.7; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia">
      <env name="BLAS_ROOT">$SHELL{if [ -z "$BLAS_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$BLAS_ROOT"; fi}</env>
      <env name="LAPACK_ROOT">$SHELL{if [ -z "$LAPACK_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$LAPACK_ROOT"; fi}</env>
      <env name="BLA_VENDOR">NVHPC</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="BLA_VENDOR">Intel10_64_dyn</env>
    </environment_variables>
    <environment_variables compiler="amdclang" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/aocc-4.0.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/intel; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/gnu; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="muller-gpu">
    <DESC>Muller GPU nodes on internal machine at NERSC. similar to pm-gpu</DESC>
    <NODENAME_REGEX>$ENV{NERSC_HOST}:muller</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnugpu,gnu,nvidiagpu,nvidia</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>e3sm_g</PROJECT>
    <SAVE_TIMING_DIR>/global/cfs/cdirs/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,m3411,m3412</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}/e3sm_scratch/muller-gpu</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/global/cfs/cdirs/e3sm/www/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>http://portal.nersc.gov/project/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>nersc_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="gnu">256</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="nvidia">256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnu">64</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="nvidia">64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 128/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 64 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/8.3.1/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cpe</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu.*">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/12.3</command>
      </modules>

      <modules compiler="nvidia.*">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/24.5</command>
      </modules>

      <modules compiler="gnugpu">
        <command name="load">cudatoolkit/12.2</command>
        <command name="load">craype-accel-nvidia80</command>
      </modules>

      <modules compiler="nvidiagpu">
        <command name="load">cudatoolkit/12.2</command>
        <command name="load">craype-accel-nvidia80</command>
        <command name="load">gcc-native-mixed/12.3</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules>
        <command name="load">cray-libsci/23.12.5</command>
        <command name="load">craype/2.7.30</command>
        <command name="load">cray-mpich/8.1.28</command>
        <command name="load">cray-hdf5-parallel/1.12.2.9</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.9</command>
        <command name="load">cray-parallel-netcdf/1.12.3.9</command>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_MPIIO_DVS_MAXNODES">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_MR_CACHE_MONITOR">kdreg2</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
    </environment_variables>
    <environment_variables compiler="nvidiagpu">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/gnugpu ; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu.*" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/gcc-11.2.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia.*" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/nvidia-22.7; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="alvarez">
    <DESC>test machine at NERSC, very similar to pm-cpu. each node has 2 AMD EPYC 7713 64-Core (Milan) 512GB</DESC>
    <NODENAME_REGEX>$ENV{NERSC_HOST}:alvarez</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>intel,gnu,nvidia,amdclang</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/global/cfs/cdirs/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>e3sm,m3411,m3412</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}/e3sm_scratch/alvarez</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/global/cfs/cdirs/e3sm/www/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>http://portal.nersc.gov/project/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/global/cfs/cdirs/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/cfs/cdirs/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/global/cfs/cdirs/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/global/cfs/cdirs/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>nersc_slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 256/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/opt/cray/pe/lmod/8.7.19/init/perl</init_path>
      <init_path lang="python">/opt/cray/pe/lmod/8.7.19/init/python</init_path>
      <init_path lang="sh">/opt/cray/pe/lmod/8.7.19/init/sh</init_path>
      <init_path lang="csh">/opt/cray/pe/lmod/8.7.19/init/csh</init_path>
      <cmd_path lang="perl">/opt/cray/pe/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/cray/pe/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cpe</command>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">matlab</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/13.2</command>
        <command name="load">cray-libsci/24.03.0</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.5.0</command>
        <command name="load">intel/2024.1.0</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">PrgEnv-nvidia</command>
        <command name="load">nvidia/24.5</command>
        <command name="load">cray-libsci/24.03.0</command>
      </modules>

      <modules compiler="amdclang">
        <command name="load">PrgEnv-aocc</command>
        <command name="load">aocc/4.1.0</command>
        <command name="load">cray-libsci/24.03.0</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.31.11</command>
        <command name="load">cray-mpich/8.1.29</command>
        <command name="load">cray-hdf5-parallel/1.12.2.11</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.11</command>
        <command name="load">cray-parallel-netcdf/1.12.3.11</command>
        <command name="load">cmake/3.24.3</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_MPIIO_DVS_MAXNODES">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/global/cfs/cdirs/e3sm/perl/lib/perl5-only-switch</env>
      <env name="FI_MR_CACHE_MONITOR">kdreg2</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
    </environment_variables>
    <environment_variables compiler="intel" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/intel-2023.1.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/gcc-11.2.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="Albany_ROOT">$SHELL{if [ -z "$Albany_ROOT" ]; then echo /global/common/software/e3sm/albany/2024.03.26/gcc/11.2.0; else echo "$Albany_ROOT"; fi}</env>
      <env name="Trilinos_ROOT">$SHELL{if [ -z "$Trilinos_ROOT" ]; then echo /global/common/software/e3sm/trilinos/15.1.1/gcc/11.2.0; else echo "$Trilinos_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/nvidia-22.7; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="nvidia">
      <env name="BLAS_ROOT">$SHELL{if [ -z "$BLAS_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$BLAS_ROOT"; fi}</env>
      <env name="LAPACK_ROOT">$SHELL{if [ -z "$LAPACK_ROOT" ]; then echo $NVIDIA_PATH/compilers; else echo "$LAPACK_ROOT"; fi}</env>
      <env name="BLA_VENDOR">NVHPC</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="BLA_VENDOR">Intel10_64_dyn</env>
    </environment_variables>
    <environment_variables compiler="amdclang" mpilib="mpich">
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /global/cfs/cdirs/e3sm/3rdparty/adios2/2.9.1/cray-mpich-8.1.25/aocc-4.0.0; else echo "$ADIOS2_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/intel; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /global/cfs/cdirs/e3sm/software/moab/gnu; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>

    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>


  <machine MACH="spock">
    <DESC>Spock. NCCS moderate-security system that contains similar hardware and software as the upcoming Frontier system at ORNL.</DESC>
    <NODENAME_REGEX>.*spock.*</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnu,cray</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>cli133</PROJECT>
    <CIME_OUTPUT_ROOT>/gpfs/alpine/$PROJECT/proj-shared/$ENV{USER}/e3sm_scratch/spock</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/gpfs/alpine/cli115/world-shared/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/gpfs/alpine/cli115/world-shared/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <CCSM_CPRNC>/gpfs/alpine/cli133/world-shared/grnydawn/e3sm/tools/cprnc_spock/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <NTEST_PARALLEL_JOBS>1</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>

    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -l -K -n {{ total_tasks }} -N {{ num_nodes }} </arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>

    <module_system type="module" allow_error="true">
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>

      <modules>
        <command name="purge"/>
        <command name="load">DefApps</command>
        <command name="load">cray-python/3.8.5.1</command>
        <command name="load">subversion/1.14.0</command>
        <command name="load">git/2.31.1</command>
        <command name="load">cmake/3.20.2</command>
        <command name="load">zlib/1.2.11</command>
        <command name="load">cray-libsci/21.06.1.1</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.0.0</command>
        <command name="load">cray-mpich/8.1.7</command>
        <command name="load">cray-hdf5-parallel/1.12.0.6</command>
        <command name="load">cray-netcdf-hdf5parallel/4.7.4.6</command>
        <command name="load">cray-parallel-netcdf/1.12.1.5</command>
      </modules>
      <modules compiler="cray">
        <command name="load">PrgEnv-cray/8.0.0</command>
        <command name="load">cray-mpich/8.1.7</command>
        <command name="load">cray-hdf5-parallel/1.12.0.6</command>
        <command name="load">cray-netcdf-hdf5parallel/4.7.4.6</command>
        <command name="load">cray-parallel-netcdf/1.12.1.5</command>
      </modules>

    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="NETCDF_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="frontier">
    <DESC>Frontier. AMD EPYC 7A53 64C nodes, 128 hwthreads, 512GB DDR4, 4 MI250X GPUs.</DESC>
    <NODENAME_REGEX>.*frontier.*</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>craygnu-hipcc,craygnu-mphipcc,craycray-mphipcc,crayamd-mphipcc,craygnu,craycray,crayamd</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>cli115</PROJECT>
    <SAVE_TIMING_DIR>/lustre/orion/proj-shared/cli115</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lustre/orion/cli115/proj-shared/$ENV{USER}/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lustre/orion/cli115/world-shared/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/orion/cli115/world-shared/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/orion/cli115/world-shared/e3sm/baselines/frontier/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lustre/orion/cli115/world-shared/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <NTEST_PARALLEL_JOBS>1</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>56</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>

    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks"> -l -K -n {{ total_tasks }} -N {{ num_nodes }} </arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="ntasks_per_gpu">$ENV{NTASKS_PER_GPU}</arg>
        <arg name="gpu_bind">$ENV{GPU_BIND_ARGS}</arg>
      </arguments>
    </mpirun>

    <module_system type="module" allow_error="true">
      <init_path lang="sh">/opt/cray/pe/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/cray/pe/lmod/lmod/init/csh</init_path>
      <init_path lang="perl">/opt/cray/pe/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/cray/pe/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="perl">/opt/cray/pe/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="python">/opt/cray/pe/lmod/lmod/libexec/lmod python</cmd_path>
      <modules compiler="craygnu.*">
        <command name="reset"></command>
        <!-- PrgEnv-gnu before cpe, or cray-mpich module doesn't update correctly -->
        <command name="load">PrgEnv-gnu</command>
        <command name="load">cpe/24.11</command>
        <command name="load">libunwind/1.8.1</command>
        <command name="load">cray-python/3.11.7</command>
        <command name="load">subversion/1.14.2</command>
        <command name="load">git/2.45.1</command>
        <command name="load">cmake/3.27.9</command>
        <command name="load">cray-hdf5-parallel/1.14.3.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.15</command>
        <command name="load">cray-parallel-netcdf/1.12.3.15</command>
        <command name="unload">darshan-runtime</command>
      </modules>
      <modules compiler="craygnu-hipcc">
        <command name="load">craype-accel-amd-gfx90a</command>
        <command name="load">rocm/6.2.4</command>
      </modules>
      <modules compiler="craygnu-mphipcc">
        <command name="load">craype-accel-amd-gfx90a</command>
        <command name="load">rocm/6.2.4</command>
      </modules>
      <modules compiler="craycray.*">
        <command name="reset"></command>
        <command name="load">Core/24.00</command>
        <command name="load">cpe/22.12</command>
        <command name="load">cce/15.0.1</command>
        <command name="load">craype/2.7.20</command>
      </modules>
      <modules compiler="craycray-mphipcc">
        <command name="load">craype-accel-amd-gfx90a</command>
        <command name="load">rocm/5.4.0</command>
      </modules>
      <modules compiler="crayamd.*">
        <command name="reset"></command>
        <command name="switch">Core Core/24.00</command>
        <command name="switch">PrgEnv-cray PrgEnv-amd/8.3.3</command>
        <command name="switch">amd amd/5.4.0</command>
        <command name="load">cray-libsci/22.12.1.1</command>

      </modules>
      <modules compiler="crayamd-mphipcc">
        <command name="load">craype-accel-amd-gfx90a</command>
        <command name="load">rocm/5.4.0</command>
      </modules>

      <modules compiler="^(?!craygnu).*">
        <command name="load">libunwind/1.5.0</command>
        <command name="load">libfabric/1.20.1</command>
        <command name="load">cray-mpich/8.1.26</command>
        <command name="load">cray-python/3.9.13.1</command>
        <command name="load">subversion/1.14.1</command>
        <command name="load">git/2.36.1</command>
        <command name="load">cmake/3.21.3</command>
        <command name="load">cray-hdf5-parallel/1.12.2.1</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.1</command>
        <command name="load">cray-parallel-netcdf/1.12.3.1</command>
        <command name="unload">darshan-runtime</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env>
      <env name="PNETCDF_PATH">$ENV{PNETCDF_DIR}</env>
      <env name="HDF5_ROOT"/> <!-- frontier cmake cannot find_package(HDF5) for some reason, so disable it -->
      <env name="MPICH_GPU_SUPPORT_ENABLED">0</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="MPICH_OFI_CXI_COUNTER_REPORT">2</env>
      <env name="LD_LIBRARY_PATH">$ENV{CRAY_LD_LIBRARY_PATH}:$ENV{LD_LIBRARY_PATH}</env>
      <env name="SKIP_BLAS">True</env> <!-- find_package(blas) doesn't work well with Cray LibSci-->
      <env name="GPUS_PER_NODE"> </env>
      <env name="NTASKS_PER_GPU"> </env>
      <env name="GPU_BIND_ARGS"> </env>
    </environment_variables>
    <environment_variables compiler=".*hipcc">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
      <env name="MPICH_CXX">$SHELL{which hipcc}</env>
      <env name="GPUS_PER_NODE">--gpus-per-node=8</env>
      <env name="NTASKS_PER_GPU">--ntasks-per-gpu=$SHELL{echo "`./xmlquery --value MAX_MPITASKS_PER_NODE`/8"|bc}</env>
      <env name="GPU_BIND_ARGS">--gpu-bind=closest</env>
    </environment_variables>

    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>

    <environment_variables compiler="craygnu.*" mpilib="mpich">
      <env name="PKG_CONFIG_PATH">/lustre/orion/cli115/world-shared/frontier/3rdparty/protobuf/21.6/gcc-native-13.2/lib/pkgconfig:$ENV{PKG_CONFIG_PATH}</env>
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/adios2/2.10.2/cray-mpich-8.1.30/craygnuamdgpu/cpe-24.07; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLOSC2_ROOT">$SHELL{if [ -z "$BLOSC2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/c-blosc2/2.15.2/gcc-native-13.2; else echo "$BLOSC2_ROOT"; fi}</env>
      <env name="MGARD_ROOT">$SHELL{if [ -z "$MGARD_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/mgard/1.5.2/gcc-native-13.2; else echo "$MGARD_ROOT"; fi}</env>
      <env name="SZ_ROOT">$SHELL{if [ -z "$SZ_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/sz/2.1.12.5/gcc-native-13.2; else echo "$SZ_ROOT"; fi}</env>
      <env name="ZFP_ROOT">$SHELL{if [ -z "$ZFP_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/zfp/1.0.1/gcc-native-13.2; else echo "$ZFP_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="craycray.*" mpilib="mpich">
      <env name="PKG_CONFIG_PATH">/lustre/orion/cli115/world-shared/frontier/3rdparty/protobuf/21.6/crayclang-15.0.1/lib/pkgconfig:$ENV{PKG_CONFIG_PATH}</env>
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/adios2/2.10.2/cray-mpich-8.1.26/crayclang-scream/cpe-22.12; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLOSC2_ROOT">$SHELL{if [ -z "$BLOSC2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/c-blosc2/2.15.2/crayclang-15.0.1; else echo "$BLOSC2_ROOT"; fi}</env>
      <env name="MGARD_ROOT">$SHELL{if [ -z "$MGARD_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/mgard/1.5.2/crayclang-15.0.1; else echo "$MGARD_ROOT"; fi}</env>
      <env name="SZ_ROOT">$SHELL{if [ -z "$SZ_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/sz/2.1.12.5/crayclang-15.0.1; else echo "$SZ_ROOT"; fi}</env>
      <env name="ZFP_ROOT">$SHELL{if [ -z "$ZFP_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/zfp/1.0.1/crayclang-15.0.1; else echo "$ZFP_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="crayamd.*" mpilib="mpich">
      <env name="PKG_CONFIG_PATH">/lustre/orion/cli115/world-shared/frontier/3rdparty/protobuf/21.6/amdclang-15.0.0/lib/pkgconfig:$ENV{CRAY_LIBSCI_PREFIX_DIR}/lib/pkgconfig:$ENV{PKG_CONFIG_PATH}</env>
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/adios2/2.10.2/cray-mpich-8.1.28/amdclang-15.0.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLOSC2_ROOT">$SHELL{if [ -z "$BLOSC2_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/c-blosc2/2.15.2/amdclang-15.0.0; else echo "$BLOSC2_ROOT"; fi}</env>
      <env name="MGARD_ROOT">$SHELL{if [ -z "$MGARD_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/mgard/1.5.2/amdclang-15.0.0; else echo "$MGARD_ROOT"; fi}</env>
      <env name="SZ_ROOT">$SHELL{if [ -z "$SZ_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/sz/2.1.12.5/amdclang-15.0.0; else echo "$SZ_ROOT"; fi}</env>
      <env name="ZFP_ROOT">$SHELL{if [ -z "$ZFP_ROOT" ]; then echo /lustre/orion/cli115/world-shared/frontier/3rdparty/zfp/1.0.1/amdclang-15.0.0; else echo "$ZFP_ROOT"; fi}</env>
    </environment_variables>
  </machine>

  <!-- Skylake nodes of Stampede2 at TACC -->
  <machine MACH="stampede2">
    <DESC>Stampede2. Intel skylake nodes at TACC. 48 cores per node, batch system is SLURM</DESC>
    <NODENAME_REGEX>.*stampede2.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <SAVE_TIMING_DIR>$ENV{SCRATCH}</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>acme</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{SCRATCH}/acme_scratch/stampede2</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{SCRATCH}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{SCRATCH}/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{SCRATCH}/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{SCRATCH}/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>ibrun</executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/apps/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/opt/apps/lmod/lmod/init/python</init_path>
      <init_path lang="sh">/opt/apps/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/opt/apps/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/opt/apps/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/opt/apps/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module -q</cmd_path>
      <cmd_path lang="csh">module -q</cmd_path>

      <modules>
        <command name="purge"/>
      </modules>

      <modules compiler="intel">
        <command name="load">intel/18.0.0</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">gcc/6.3.0</command>
      </modules>

      <modules mpilib="impi">
        <command name="load">impi/18.0.0</command>
      </modules>

      <modules mpilib="mpi-serial">
        <command name="load">hdf5/1.8.16</command>
        <command name="load">netcdf/4.3.3.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">phdf5/1.8.16</command>
        <command name="load">parallel-netcdf/4.3.3.1</command>
        <command name="load">pnetcdf/1.8.1</command>
      </modules>
      <modules>
        <command name="load">git</command>
        <command name="load">cmake</command>
        <command name="load">autotools</command>
        <command name="load">xalt</command>
        <!--command name="load">TACC</command-->
        <!--command name="load">python/2.7.13</command-->
      </modules>

    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>

      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="I_MPI_PIN">1</env>
      <env name="MY_MPIRUN_OPTIONS">-l</env>
      <env name="NETCDF_PATH">$ENV{TACC_NETCDF_DIR}</env>
      <env name="PNETCDF_PATH">$ENV{TACC_PNETCDF_DIR}</env>
    </environment_variables>
  </machine>

  <machine MACH="mac">
    <DESC>Mac OS/X workstation or laptop</DESC>
    <NODENAME_REGEX/>
    <OS>Darwin</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/acme/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/acme/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/acme/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jnjohnson at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>2</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments/>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$ENV{HOME}/projects/acme/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/acme/scratch/$CASE/bld</EXEROOT>
    <!-- cmake -DCMAKE_Fortran_COMPILER=/opt/local/bin/mpif90-mpich-gcc48 -DHDF5_DIR=/opt/local -DNetcdf_INCLUDE_DIR=/opt/local/include .. -->
    <!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
  </machine>

  <machine MACH="linux-generic">
    <DESC>Linux workstation or laptop</DESC>
    <NODENAME_REGEX>none</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/acme/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/acme/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/acme/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jayesh at mcs dot anl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>2</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$ENV{HOME}/projects/acme/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/acme/scratch/$CASE/bld</EXEROOT>
    <!-- cmake -DCMAKE_Fortran_COMPILER=/opt/local/bin/mpif90-mpich-gcc48 -DHDF5_DIR=/opt/local -DNetcdf_INCLUDE_DIR=/opt/local/include .. -->
    <!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
  </machine>

  <machine MACH="WSL2">
    <DESC>Windows Subsystem for Linux v2, using Ubuntu distribution</DESC>
    <NODENAME_REGEX>none</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/pt-e3sm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/pt-e3sm-inputdata</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/e3sm_scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/e3sm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>thorntonpe at ornl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$ENV{HOME}/e3sm_scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/e3sm_scratch/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="LAPACK_ROOT">$ENV{BLASLAPACK_LIBDIR}</env>
    </environment_variables>
  </machine>

  <machine MACH="singularity">
    <DESC>Singularity container</DESC>
    <NODENAME_REGEX>singularity</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/e3sm/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/e3sm/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/e3sm/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>lukasz at uchicago dot edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -launcher fork -hosts localhost -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$ENV{HOME}/projects/e3sm/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/e3sm/scratch/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="E3SM_SRCROOT">$SRCROOT</env>
    </environment_variables>
    <environment_variables mpilib="mpi-serial">
      <env name="NETCDF_PATH">/usr/local/packages/netcdf-serial</env>
      <env name="PATH">/usr/local/packages/cmake/bin:/usr/local/packages/hdf5-serial/bin:/usr/local/packages/netcdf-serial/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/usr/local/packages/szip/lib:/usr/local/packages/hdf5-serial/lib:/usr/local/packages/netcdf-serial/lib</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="NETCDF_PATH">/usr/local/packages/netcdf-parallel</env>
      <env name="PNETCDF_PATH">/usr/local/packages/pnetcdf</env>
      <env name="HDF5_ROOT">/usr/local/packages/hdf5-parallel</env>
      <env name="PATH">/usr/local/packages/cmake/bin:/usr/local/packages/mpich/bin:/usr/local/packages/hdf5-parallel/bin:/usr/local/packages/netcdf-parallel/bin:/usr/local/packages/pnetcdf/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/usr/local/packages/mpich/lib:/usr/local/packages/szip/lib:/usr/local/packages/hdf5-parallel/lib:/usr/local/packages/netcdf-parallel/lib:/usr/local/packages/pnetcdf/lib</env>
    </environment_variables>
  </machine>

  <machine MACH="ghci-oci">
    <DESC>OCI-based container</DESC>
    <NODENAME_REGEX>ghci-oci</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/projects/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/e3sm/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/projects/e3sm/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/usr/local/packages/bin/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm-team</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -launcher fork -hosts localhost -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>/projects/e3sm/scratch/$CASE/run</RUNDIR>
    <EXEROOT>/projects/e3sm/scratch/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="E3SM_SRCROOT">$SRCROOT</env>
    </environment_variables>
    <environment_variables mpilib="mpi-serial">
      <env name="NETCDF_PATH">/usr/local/packages</env>
      <env name="PATH">/usr/local/packages/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/usr/local/packages/lib</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="NETCDF_PATH">/usr/local/packages</env>
      <env name="PNETCDF_PATH">/usr/local/packages</env>
      <env name="HDF5_ROOT">/usr/local/packages</env>
      <env name="PATH">/usr/local/packages/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/usr/local/packages/lib</env>
    </environment_variables>
  </machine>

  <machine MACH="melvin">
    <DESC>Linux workstation for Jenkins testing</DESC>
    <NODENAME_REGEX>(melvin|watson|s999964|climate|penn|sems)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>gnu,intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/sems-data-store/ACME/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/sems-data-store/ACME/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/sems-data-store/ACME/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/sems-data-store/ACME/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/sems-data-store/ACME/cprnc/build.new/cprnc</CCSM_CPRNC>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to hwthread:overload-allowed</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="false">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">sems-env</command>
        <command name="load">acme-env</command>
        <command name="load">sems-git</command>
        <command name="load">acme-binutils</command>
        <command name="load">sems-python/3.5.2</command>
        <command name="load">sems-cmake/3.12.2</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">sems-gcc/7.3.0</command>
      </modules>
      <modules compiler="intel">
        <command name="load">sems-intel/16.0.3</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">sems-netcdf/4.4.1/exo</command>
        <command name="load">acme-pfunit/3.2.8/base</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">acme-openmpi/2.1.5</command>
        <command name="load">acme-netcdf/4.7.4/acme</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>1000</MAX_GB_OLD_TEST_DATA>
    <!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <environment_variables>
      <env name="NETCDFROOT">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="mappy">
    <DESC>Huge Linux workstation for Sandia climate scientists</DESC>
    <NODENAME_REGEX>mappy</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/sems-data-store/ACME/mappy/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/sems-data-store/ACME/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/sems-data-store/ACME/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/sems-data-store/ACME/baselines/mappy/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/sems-data-store/ACME/mappy/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>64</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>slurm_single_node</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="binding"> --cpu_bind=threads</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="false">
      <init_path lang="python">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="perl">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/init/perl</init_path>
      <init_path lang="sh">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/projects/sems/install/rhel9-x86_64/sems/lmod/lmod/8.7.24/gcc/11.4.1/base/lnirq74/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">sems-git/2.42.0</command>
        <command name="load">sems-cmake/3.27.9</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">sems-gcc/11.4.0</command>
        <command name="load">sems-openblas</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">sems-netcdf-c-serial/4.9.2</command>
        <command name="load">sems-netcdf-fortran-serial/4.6.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">sems-openmpi-no-cuda/4.1.6</command>
        <command name="load">sems-netcdf-c/4.9.2</command>
        <command name="load">sems-netcdf-cxx/4.2</command>
        <command name="load">sems-netcdf-fortran/4.6.1</command>
        <command name="load">sems-parallel-netcdf/1.12.3</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <environment_variables>
      <env name="NETCDF_C_PATH">$ENV{NETCDF_C_ROOT}</env>
      <env name="NETCDF_FORTRAN_PATH">$ENV{NETCDF_FORTRAN_ROOT}</env>
      <env name="BLAS_ROOT">$ENV{OPENBLAS_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="BLA_VENDOR">OpenBLAS</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$ENV{PARALLEL_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="ghci-snl-cpu">
    <DESC>Huge Linux workstation for Sandia climate scientists</DESC>
    <NODENAME_REGEX>^[a-fA-F0-9]{12}$</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/projects/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/e3sm/baselines/ghci-snl-cpu/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/e3sm/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>lbertag at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="binding"> --bind-to core</arg>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_C_PATH">$ENV{NETCDF_C_ROOT}</env>
      <env name="NETCDF_FORTRAN_PATH">$ENV{NETCDF_FORTRAN_ROOT}</env>
      <env name="PNETCDF_PATH">$ENV{PARALLEL_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
    </environment_variables>
  </machine>

  <machine MACH="weaver">
    <DESC>Sandia GPU testbed</DESC>
    <NODENAME_REGEX>weaver</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnugpu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/home/projects/e3sm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/projects/e3sm/scream/data</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/projects/e3sm/scream/data/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/projects/e3sm/baselines/weaver/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/home/projects/e3sm/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="false">
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">cuda/10.1.105</command>
        <command name="load">ucx/1.6.0</command>
        <command name="load">git/2.10.1</command>
        <command name="load">python/3.7.3</command>
        <command name="load">cmake/3.18.0</command>
        <command name="load">perl/5.22.1</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <environment_variables>
      <env name="LD_LIBRARY_PATH">/ascldap/users/projects/e3sm/scream/libs/mpfr/install/weaver/lib:/ascldap/users/projects/e3sm/scream/libs/gcc/install/weaver/gcc/8.5.0/lib64:/ascldap/users/projects/e3sm/scream/libs/gcc/install/weaver/gcc/8.5.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/ascldap/users/projects/e3sm/scream/libs/gcc/install/weaver/gcc/8.5.0/bin:/ascldap/users/projects/e3sm/scream/libs/gcc/install/weaver/gcc/8.5.0/libexec/gcc/powerpc64le-unknown-linux-gnu/8.5.0:/ascldap/users/projects/e3sm/scream/libs/openmpi/install/weaver/gcc/8.5.0/cuda/10.1.105/bin:/ascldap/users/projects/e3sm/scream/libs/pnetcdf/install/weaver/gcc/8.5.0/cuda/10.1.105/bin:/ascldap/users/projects/e3sm/scream/libs/netcdf-c/install/weaver/gcc/8.5.0/cuda/10.1.105/bin:/ascldap/users/projects/e3sm/scream/libs/netcdf-fortran/install/weaver/gcc/8.5.0/cuda/10.1.105/bin:/ascldap/users/projects/e3sm/scream/libs/wget/bin:/ascldap/users/jgfouca/perl5/bin:$ENV{PATH}</env>
      <env name="PERL5LIB">/ascldap/users/jgfouca/perl5/lib/perl5</env>
      <env name="PERL_LOCAL_LIB_ROOT">/ascldap/users/jgfouca/perl5</env>
      <env name="NETCDF_C_PATH">/ascldap/users/projects/e3sm/scream/libs/netcdf-c/install/weaver/gcc/8.5.0/cuda/10.1.105</env>
      <env name="NETCDF_FORTRAN_PATH">/ascldap/users/projects/e3sm/scream/libs/netcdf-fortran/install/weaver/gcc/8.5.0/cuda/10.1.105</env>
      <env name="PNETCDF_PATH">/ascldap/users/projects/e3sm/scream/libs/pnetcdf/install/weaver/gcc/8.5.0/cuda/10.1.105</env>
    </environment_variables>
  </machine>

  <machine MACH="snl-white">
    <DESC>IBM Power 8 Testbed machine</DESC>
    <NODENAME_REGEX>white</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/e3sm/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/e3sm/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/e3sm/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>mdeakin at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>1</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments/>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <modules>
        <command name="load">devpack/20181011/openmpi/2.1.2/gcc/7.2.0/cuda/9.2.88</command>
      </modules>
    </module_system>
    <RUNDIR>$ENV{HOME}/projects/e3sm/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/e3sm/scratch/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="NETCDF_C_PATH">$ENV{NETCDF_ROOT}</env>
      <env name="NETCDF_FORTRAN_PATH">/ascldap/users/jgfouca/packages/netcdf-fortran-4.4.4-white</env>
      <env name="E3SM_SRCROOT">$SRCROOT</env>
    </environment_variables>
  </machine>

  <machine MACH="snl-blake">
    <DESC>Skylake Testbed machine</DESC>
    <NODENAME_REGEX>blake</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel18</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/projects/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/e3sm/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/e3sm/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/e3sm/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/projects/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE_J>48</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>mdeakin at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments/>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="python">module</cmd_path>
      <modules>
        <command name="load">zlib/1.2.11</command>
        <command name="load">intel/compilers/18.1.163</command>
        <command name="load">openmpi/2.1.2/intel/18.1.163</command>
        <command name="load">hdf5/1.10.1/openmpi/2.1.2/intel/18.1.163</command>
        <command name="load">netcdf-exo/4.4.1.1/openmpi/2.1.2/intel/18.1.163</command>
      </modules>
    </module_system>
    <RUNDIR>$ENV{HOME}/projects/e3sm/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/e3sm/scratch/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="NETCDF_C_PATH">$ENV{NETCDF_ROOT}</env>
      <env name="NETCDF_FORTRAN_PATH">$ENV{NETCDFF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="anlworkstation">
    <DESC>Linux workstation for ANL</DESC>
    <NODENAME_REGEX>compute.*mcs.anl.gov</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/acme/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/climate1/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/climate1/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/climate1/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/home/climate1/acme/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -l -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="soft">
      <init_path lang="csh">/software/common/adm/packages/softenv-1.6.2/etc/softenv-load.csh</init_path>
      <init_path lang="sh">/software/common/adm/packages/softenv-1.6.2/etc/softenv-load.sh</init_path>
      <cmd_path lang="csh">source /software/common/adm/packages/softenv-1.6.2/etc/softenv-aliases.csh ; soft</cmd_path>
      <cmd_path lang="sh">source /software/common/adm/packages/softenv-1.6.2/etc/softenv-aliases.sh ; soft</cmd_path>
      <modules compiler="gnu">
        <command name="add">+gcc-8.2.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables mpilib="mpi-serial">
      <env name="PATH">/soft/apps/packages/climate/cmake/3.18.4/bin:/soft/apps/packages/climate/gmake/bin:$ENV{PATH}</env>
      <!-- We currently don't have a soft env for serial hdf5 and szip built with gcc 8.2.0 -->
      <env name="LD_LIBRARY_PATH">/soft/apps/packages/climate/hdf5/1.8.16-serial/gcc-8.2.0/lib:/soft/apps/packages/climate/szip/2.1/gcc-8.2.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <!-- We currently don't have a soft env for netcdf serial built with gcc 8.2.0 -->
      <env name="NETCDF_PATH">/soft/apps/packages/climate/netcdf/4.4.1c-4.2cxx-4.4.4f-serial/gcc-8.2.0</env>
    </environment_variables>
    <environment_variables mpilib="mpich">
      <!-- We currently don't have a soft env for parallel hdf5 and szip built with gcc 8.2.0 -->
      <env name="LD_LIBRARY_PATH">/soft/apps/packages/climate/hdf5/1.8.16-parallel/mpich-3.3.2/gcc-8.2.0/lib:/soft/apps/packages/climate/szip/2.1/gcc-8.2.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <!-- We currently don't have a soft env for mpich 3.3.2 built with gcc 8.2.0 -->
      <env name="PATH">/soft/apps/packages/climate/mpich/3.3.2/gcc-8.2.0/bin:/soft/apps/packages/climate/cmake/3.18.4/bin:/soft/apps/packages/climate/gmake/bin:$ENV{PATH}</env>
      <!-- We currently don't have a soft env for parallel hdf5 built with mpich 3.3.2 and gcc 8.2.0 -->
      <env name="HDF5_ROOT">/soft/apps/packages/climate/hdf5/1.8.16-parallel/mpich-3.3.2/gcc-8.2.0</env>
      <!-- We currently don't have a soft env for netcdf parallel built with mpich 3.3.2 and gcc 8.2.0 -->
      <env name="NETCDF_PATH">/soft/apps/packages/climate/netcdf/4.4.1c-4.2cxx-4.4.4f-parallel/mpich-3.3.2/gcc-8.2.0</env>
      <!-- We currently don't have a soft env for pnetcdf built with mpich 3.3.2 and gcc 8.2.0 -->
      <env name="PNETCDF_PATH">/soft/apps/packages/climate/pnetcdf/1.12.0/mpich-3.3.2/gcc-8.2.0</env>
    </environment_variables>
    <environment_variables mpilib="openmpi">
      <!-- We currently don't have a soft env for openmpi 2.1.5, zlib, szip, hdf5, NetCDF and PnetCDF libraries -->
      <env name="PATH">/soft/apps/packages/climate/openmpi/2.1.5/gcc-8.2.0/bin:/soft/apps/packages/climate/cmake/3.18.4/bin:/soft/apps/packages/climate/gmake/bin:$ENV{PATH}</env>
      <env name="ZLIB_ROOT">/soft/apps/packages/climate/zlib/1.2.11/gcc-8.2.0-static</env>
      <env name="SZIP_ROOT">/soft/apps/packages/climate/szip/2.1/gcc-8.2.0-static</env>
      <env name="HDF5_ROOT">/soft/apps/packages/climate/hdf5/1.8.12-parallel/openmpi-2.1.5/gcc-8.2.0-static</env>
      <env name="NETCDF_PATH">/soft/apps/packages/climate/netcdf/4.7.4c-4.3.1cxx-4.4.4f-parallel/openmpi-2.1.5/gcc-8.2.0-static-hdf5-1.8.12-pnetcdf-1.12.0</env>
      <env name="PNETCDF_PATH">/soft/apps/packages/climate/pnetcdf/1.12.0/openmpi-2.1.5/gcc-8.2.0</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables>
      <env name="PERL5LIB">/soft/apps/packages/climate/perl5/lib/perl5</env>
    </environment_variables>
  </machine>

  <machine MACH="anlgce-ub22">
    <DESC>ANL CELS General Computing Environment (Linux) workstation (Ubuntu 22.04)</DESC>
    <NODENAME_REGEX>compute-386-(01|02|03|05|07|08)|compute-240-(15)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich,openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/scratch/$ENV{USER}/e3sm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/scratch/$ENV{USER}/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nfs/gce/projects/climate/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nfs/gce/projects/climate/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/nfs/gce/projects/climate/e3sm/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jayesh at mcs dot anl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -l -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> --oversubscribe -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/8.5.6/init/env_modules_python.py</init_path>
      <init_path lang="perl">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/lmod/init/perl</init_path>
      <init_path lang="bash">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/lmod/init/bash</init_path>
      <init_path lang="sh">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/nfs/gce/software/custom/linux-ubuntu22.04-x86_64/spack/opt/spack/linux-ubuntu22.04-x86_64/gcc-11.2.0/lmod-8.5.6-hkjjxhp/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">module</cmd_path>
      <cmd_path lang="bash">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">gcc/12.1.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables mpilib="mpi-serial">
      <!-- We currently don't have modules for serial NetCDF -->
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-serial/gcc-12.1.0</env>
    </environment_variables>
    <environment_variables mpilib="mpich">
      <!-- We currently don't have modules for HDF5, NetCDF & PnetCDF -->
      <env name="LD_LIBRARY_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/mpich/4.1.2/gcc-12.1.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/mpich/4.1.2/gcc-12.1.0/bin:$ENV{PATH}</env>
      <env name="ZLIB_ROOT">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/zlib-1.2.11-p7dmb5p</env>
      <env name="HDF5_ROOT">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/hdf5/1.12.2/mpich-4.1.2/gcc-12.1.0</env>
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-parallel/mpich-4.1.2/gcc-12.1.0</env>
      <env name="PNETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/pnetcdf/1.12.3/mpich-4.1.2/gcc-12.1.0</env>
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /nfs/gce/projects/climate/software/moab/devel/mpich-4.1.2/gcc-12.1.0; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables mpilib="openmpi">
      <!-- We currently don't have modules for HDF5, NetCDF & PnetCDF -->
      <env name="LD_LIBRARY_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/openmpi/4.1.6/gcc-12.1.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/openmpi/4.1.6/gcc-12.1.0/bin:$ENV{PATH}</env>
      <env name="ZLIB_ROOT">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/zlib-1.2.11-p7dmb5p</env>
      <env name="HDF5_ROOT">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/hdf5/1.12.2/openmpi-4.1.6/gcc-12.1.0</env>
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-parallel/openmpi-4.1.6/gcc-12.1.0</env>
      <env name="PNETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/pnetcdf/1.12.3/openmpi-4.1.6/gcc-12.1.0</env>
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /nfs/gce/projects/climate/software/moab/devel/openmpi-4.1.6/gcc-12.1.0; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables>
      <env name="PERL5LIB">/nfs/gce/projects/climate/software/perl5/lib/perl5</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="mpich">
      <env name="PKG_CONFIG_PATH">$SHELL{if [ -z "$PKG_CONFIG_PATH" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/protobuf/21.6/gcc-12.1.0/lib/pkgconfig; else echo "/nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/protobuf/21.6/gcc-12.1.0/lib/pkgconfig:$PKG_CONFIG_PATH"; fi}</env>
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/adios2/2.10.2/mpich-4.1.2/gcc-12.1.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLOSC2_ROOT">$SHELL{if [ -z "$BLOSC2_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/c-blosc2/2.15.2/gcc-12.1.0; else echo "$BLOSC2_ROOT"; fi}</env>
      <env name="MGARD_ROOT">$SHELL{if [ -z "$MGARD_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/mgard/1.5.2/gcc-12.1.0; else echo "$MGARD_ROOT"; fi}</env>
      <env name="SZ_ROOT">$SHELL{if [ -z "$SZ_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/sz/2.1.12.5/gcc-12.1.0; else echo "$SZ_ROOT"; fi}</env>
      <env name="ZFP_ROOT">$SHELL{if [ -z "$ZFP_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu22.04-x86_64/zfp/1.0.1/gcc-12.1.0; else echo "$ZFP_ROOT"; fi}</env>
    </environment_variables>
  </machine>

  <machine MACH="anlgce">
    <DESC>ANL CELS General Computing Environment (Linux) workstation</DESC>
    <NODENAME_REGEX>compute-(240|386)-[0-9][0-9]</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich,openmpi</MPILIBS>
    <SAVE_TIMING_DIR>/scratch/$ENV{USER}/e3sm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/scratch/$ENV{USER}/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/nfs/gce/projects/climate/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/nfs/gce/projects/climate/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/nfs/gce/projects/climate/e3sm/cprnc/build/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>jayesh at mcs dot anl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -l -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> --oversubscribe -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="perl">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/init/perl</init_path>
      <init_path lang="bash">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/init/bash</init_path>
      <init_path lang="sh">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-6fjdtku/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">module</cmd_path>
      <cmd_path lang="bash">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">autoconf/2.69-bmnwajj</command>
        <command name="load">automake/1.16.3-r7w24o4</command>
        <command name="load">libtool/2.4.6-uh3mpsu</command>
        <command name="load">m4/1.4.19-7fztfyz</command>
        <command name="load">cmake/3.20.5-zyz2eld</command>
        <command name="load">gcc/11.1.0-qsjmpcg</command>
        <command name="load">zlib/1.2.11-p7dmb5p</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables mpilib="mpi-serial">
      <!-- We currently don't have modules for serial NetCDF -->
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-serial/gcc-11.1.0</env>
    </environment_variables>
    <environment_variables mpilib="mpich">
      <!-- We currently don't have modules for HDF5, NetCDF & PnetCDF -->
      <env name="LD_LIBRARY_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/mpich/4.0/gcc-11.1.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/mpich/4.0/gcc-11.1.0/bin:$ENV{PATH}</env>
      <env name="ZLIB_ROOT">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/zlib-1.2.11-p7dmb5p</env>
      <env name="HDF5_ROOT">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/hdf5/1.12.1/mpich-4.0/gcc-11.1.0</env>
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-parallel/mpich-4.0/gcc-11.1.0</env>
      <env name="PNETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/pnetcdf/1.12.2/mpich-4.0/gcc-11.1.0</env>
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /nfs/gce/projects/climate/software/moab/devel/mpich-4.0/gcc-11.1.0; else echo "$MOAB_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables mpilib="openmpi">
      <!-- We currently don't have modules for HDF5, NetCDF & PnetCDF -->
      <env name="LD_LIBRARY_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/openmpi/4.1.3/gcc-11.1.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/openmpi/4.1.3/gcc-11.1.0/bin:$ENV{PATH}</env>
      <env name="ZLIB_ROOT">/nfs/gce/software/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/zlib-1.2.11-p7dmb5p</env>
      <env name="HDF5_ROOT">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/hdf5/1.12.1/openmpi-4.1.3/gcc-11.1.0</env>
      <env name="NETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/netcdf/4.8.0c-4.3.1cxx-4.5.3f-parallel/openmpi-4.1.3/gcc-11.1.0</env>
      <env name="PNETCDF_PATH">/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/pnetcdf/1.12.2/openmpi-4.1.3/gcc-11.1.0</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables>
      <env name="PERL5LIB">/nfs/gce/projects/climate/software/perl5/lib/perl5</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="mpich">
      <env name="PKG_CONFIG_PATH">$SHELL{if [ -z "$PKG_CONFIG_PATH" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/protobuf/21.6/gcc-11.1.0/lib/pkgconfig; else echo "/nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/protobuf/21.6/gcc-11.1.0/lib/pkgconfig:$PKG_CONFIG_PATH"; fi}</env>
      <env name="ADIOS2_ROOT">$SHELL{if [ -z "$ADIOS2_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/adios2/2.10.2/mpich-4.0/gcc-11.1.0; else echo "$ADIOS2_ROOT"; fi}</env>
      <env name="BLOSC2_ROOT">$SHELL{if [ -z "$BLOSC2_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/c-blosc2/2.15.2/gcc-11.1.0; else echo "$BLOSC2_ROOT"; fi}</env>
      <env name="MGARD_ROOT">$SHELL{if [ -z "$MGARD_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/mgard/1.5.2/gcc-11.1.0; else echo "$MGARD_ROOT"; fi}</env>
      <env name="SZ_ROOT">$SHELL{if [ -z "$SZ_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/sz/2.1.12.5/gcc-11.1.0; else echo "$SZ_ROOT"; fi}</env>
      <env name="ZFP_ROOT">$SHELL{if [ -z "$ZFP_ROOT" ]; then echo /nfs/gce/projects/climate/software/linux-ubuntu20.04-x86_64/zfp/1.0.1/gcc-11.1.0; else echo "$ZFP_ROOT"; fi}</env>
    </environment_variables>
  </machine>

  <machine MACH="sandiatoss3">
    <DESC>SNL clust</DESC>
    <NODENAME_REGEX>(skybridge|chama)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>fy210162</PROJECT>
    <SAVE_TIMING_DIR>/projects/ccsm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/gpfs/$USER/acme_scratch/sandiatoss3</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/cprnc/build.toss3/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
        <arg name="num_tasks"> --n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/lmod/lmod/init/python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">sems-archive-env</command>
        <command name="load">acme-env</command>
        <command name="load">sems-archive-git</command>
        <command name="load">sems-archive-cmake/3.19.1</command>
        <command name="load">gnu/6.3.1</command>
        <command name="load">sems-archive-intel/17.0.0</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">sems-archive-openmpi/1.10.5</command>
        <command name="load">acme-netcdf/4.7.4/acme</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">sems-archive-netcdf/4.4.1/exo</command>
      </modules>
    </module_system>
    <RUNDIR>/nscratch/$USER/acme_scratch/sandiatoss3/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <!-- complete path to a short term archiving directory -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>

    <environment_variables>
      <env name="NETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="boca">
    <DESC>SNL clust</DESC>
    <NODENAME_REGEX>(boca)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>fy210162</PROJECT>
    <SAVE_TIMING_DIR>/projects/ccsm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/gpfs/$USER/acme_scratch/boca</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/cprnc/build.toss3/cprnc</CCSM_CPRNC>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
        <arg name="num_tasks"> --n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/lmod/lmod/init/python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="use">/projects/sems/acme-boca-modulefiles/env-module</command>
        <command name="load">acme-boca-env</command>
        <command name="load">sems-archive-git</command>
        <command name="load">sems-archive-cmake/3.19.1</command>
        <command name="load">gnu/10.2</command>
        <command name="load">sems-archive-intel/21.3.0</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">sems-archive-openmpi/4.1.4</command>
        <command name="load">acme-netcdf/4.7.4/acme</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">sems-archive-netcdf/4.4.1/exo</command>
      </modules>
    </module_system>
    <RUNDIR>/nscratch/$USER/acme_scratch/boca/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <!-- complete path to a short term archiving directory -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>

    <environment_variables>
      <env name="NETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="NETCDF_INCLUDES">$ENV{SEMS_NETCDF_ROOT}/include</env>
      <env name="NETCDF_LIBS">$ENV{SEMS_NETCDF_ROOT}/lib</env>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="CLDERA_PATH">/projects/cldera/cldera-tools/install-master/intel</env>
    </environment_variables>
  </machine>


  <machine MACH="flight">
    <DESC>SNL clust</DESC>
    <NODENAME_REGEX>(flight)</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>fy210162</PROJECT>
    <SAVE_TIMING_DIR>/projects/ccsm/timings</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/gpfs/$USER/acme_scratch/flight</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/cprnc/build.toss3/cprnc</CCSM_CPRNC>
    <GMAKE_J>56</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>112</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>112</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
        <arg name="num_tasks"> --n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/lmod/lmod/init/python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="use">/projects/sems/acme-boca-modulefiles/env-module</command>
        <command name="load">acme-boca-env</command>
        <command name="load">sems-archive-git</command>
        <command name="load">sems-archive-cmake/3.19.1</command>
        <command name="load">gnu/10.3.1</command>
        <command name="load">sems-archive-intel/21.3.0</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">sems-archive-openmpi/4.1.4</command>
        <command name="load">acme-netcdf/4.7.4/acme</command>
      </modules>
      <modules mpilib="mpi-serial">
        <command name="load">sems-archive-netcdf/4.4.1/exo</command>
      </modules>
    </module_system>
    <RUNDIR>/nscratch/$USER/acme_scratch/flight/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <!-- complete path to a short term archiving directory -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>

    <environment_variables>
      <env name="NETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="NETCDF_INCLUDES">$ENV{SEMS_NETCDF_ROOT}/include</env>
      <env name="NETCDF_LIBS">$ENV{SEMS_NETCDF_ROOT}/lib</env>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="CLDERA_PATH">/projects/cldera/cldera-tools/install-master/intel</env>
    </environment_variables>
  </machine>


  <machine MACH="ghost">
    <DESC>SNL clust</DESC>
    <NODENAME_REGEX>ghost-login</NODENAME_REGEX>
    <OS>LINUX</OS>
    <PROXY>proxy.sandia.gov:80</PROXY>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>fy210162</PROJECT>

    <CIME_OUTPUT_ROOT>/gscratch/$USER/acme_scratch/ghost</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/projects/ccsm/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/projects/ccsm/cprnc/build.toss3/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpiexec</executable>
      <arguments>
        <arg name="num_tasks"> --n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/lmod/lmod/init/python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">sems-env</command>
        <command name="load">sems-git</command>
        <command name="load">sems-python/3.5.2</command>
        <command name="load">sems-cmake</command>
        <command name="load">gnu/4.9.2</command>
        <command name="load">sems-intel/16.0.2</command>
        <command name="load">mkl/16.0</command>
        <command name="load">sems-netcdf/4.4.1/exo_parallel</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">sems-openmpi/1.10.5</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <!-- complete path to a short term archiving directory -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <environment_variables>
      <env name="NETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$ENV{SEMS_NETCDF_ROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="anvil">
    <DESC>ANL/LCRC Linux Cluster</DESC>
    <NODENAME_REGEX>(b\d+|blues.*).lcrc.anl.gov</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>impi,openmpi,mvapich</MPILIBS>
    <PROJECT>condo</PROJECT>
    <SAVE_TIMING_DIR>/lcrc/group/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/anvil</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/lcrc/group/e3sm/public_html/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>https://web.lcrc.anl.gov/public/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/anvil/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/soft/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks"> -l -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit </arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/sh;export MODULEPATH=$MODULEPATH:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core</init_path>
      <init_path lang="csh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/csh;setenv MODULEPATH $MODULEPATH\:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core</init_path>
      <init_path lang="python">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">export MODULEPATH=$MODULEPATH:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core;/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="use">/lcrc/group/e3sm/soft/modulefiles/anvil</command>
        <command name="load">cmake/3.26.3-nszudya</command>
      </modules>
      <modules compiler="intel">
        <command name="load">gcc/8.2.0</command>
        <command name="load">intel/20.0.4-lednsve</command>
        <command name="load">intel-mkl/2020.4.304-voqlapk</command>
      </modules>
      <modules compiler="intel" mpilib="mvapich">
        <command name="load">mvapich2/2.3.6-verbs-x4iz7lq</command>
        <command name="load">netcdf-c/4.4.1-gei7x7w</command>
        <command name="load">netcdf-cxx/4.2-db2f5or</command>
        <command name="load">netcdf-fortran/4.4.4-b4ldb3a</command>
        <command name="load">parallel-netcdf/1.11.0-kj4jsvt</command>
      </modules>
      <modules compiler="intel" mpilib="impi">
        <command name="load">intel-mpi/2019.9.304-i42whlw</command>
        <command name="load">netcdf-c/4.4.1-blyisdg</command>
        <command name="load">netcdf-cxx/4.2-gkqc6fq</command>
        <command name="load">netcdf-fortran/4.4.4-eanrh5t</command>
        <command name="load">parallel-netcdf/1.11.0-y3nmmej</command>
      </modules>
      <modules compiler="intel" mpilib="openmpi">
        <command name="load">openmpi/4.1.1-v3b3npd</command>
        <command name="load">netcdf-c/4.4.1-smyuxme</command>
        <command name="load">netcdf-cxx/4.2-kfb2aag</command>
        <command name="load">netcdf-fortran/4.4.4-mablvyc</command>
        <command name="load">parallel-netcdf/1.11.0-x4n5s7k</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/8.2.0-xhxgy33</command>
        <command name="load">intel-mkl/2020.4.304-d6zw4xa</command>
      </modules>
      <modules compiler="gnu" mpilib="mvapich">
        <command name="load">netcdf/4.4.1-ve2zfkw</command>
        <command name="load">netcdf-cxx/4.2-2rkopdl</command>
        <command name="load">netcdf-fortran/4.4.4-thtylny</command>
        <command name="load">mvapich2/2.2-verbs-ppznoge</command>
        <command name="load">parallel-netcdf/1.11.0-c22b2bn</command>
      </modules>
      <modules compiler="gnu" mpilib="impi">
        <command name="load">intel-mpi/2019.9.304-rxpzd6p</command>
        <command name="load">netcdf-c/4.4.1-fysjgfx</command>
        <command name="load">netcdf-cxx/4.2-oaiw2v6</command>
        <command name="load">netcdf-fortran/4.4.4-kxgkaop</command>
        <command name="load">parallel-netcdf/1.11.0-fce7akl</command>
      </modules>
      <modules compiler="gnu" mpilib="openmpi">
        <command name="load">openmpi/4.1.1-x5n4m36</command>
        <command name="load">netcdf-c/4.4.1-mtfptpl</command>
        <command name="load">netcdf-cxx/4.2-osp27dq</command>
        <command name="load">netcdf-fortran/4.4.4-5yd6dos</command>
        <command name="load">parallel-netcdf/1.11.0-a7ohxsg</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
      <env name="PATH">/lcrc/group/e3sm/soft/perl/5.26.0/bin:$ENV{PATH}</env>
      <env name="UCX_TLS">self,sm,ud</env>
      <env name="UCX_UD_MLX5_RX_QUEUE_LEN">16384</env>
    </environment_variables>
    <environment_variables mpilib="mvapich">
      <env name="MV2_ENABLE_AFFINITY">0</env>
      <env name="MV2_SHOW_CPU_BINDING">1</env>
      <env name="MV2_HOMOGENEOUS_CLUSTER">1</env>
    </environment_variables>
    <environment_variables mpilib="mvapich" DEBUG="TRUE">
      <env name="MV2_DEBUG_SHOW_BACKTRACE">1</env>
      <env name="MV2_SHOW_ENV_INFO">2</env>
    </environment_variables>
    <environment_variables mpilib="impi" DEBUG="TRUE">
      <env name="I_MPI_DEBUG">10</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="intel">
      <env name="KMP_AFFINITY">granularity=core,balanced</env>
      <env name="KMP_HOT_TEAMS_MODE">1</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="chrysalis">
    <DESC>ANL LCRC cluster 512-node AMD Epyc 7532 2-sockets 64-cores per node</DESC>
    <NODENAME_REGEX>chr.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu,oneapi-ifx</COMPILERS>
    <MPILIBS>openmpi,impi</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/lcrc/group/e3sm/PERF_Chrysalis</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/chrys</CIME_OUTPUT_ROOT>
    <CIME_HTML_ROOT>/lcrc/group/e3sm/public_html/$ENV{USER}</CIME_HTML_ROOT>
    <CIME_URL_ROOT>https://web.lcrc.anl.gov/public/e3sm/$ENV{USER}</CIME_URL_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/scratch/chrys/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/chrys/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks">--mpi=pmi2 -l -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit </arg>
        <arg name="binding"> $SHELL{if [ 64 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
        <arg name="thread_count">-c $SHELL{echo 128/ {{ tasks_per_node }} |bc}</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="load">subversion/1.14.0-e4smcy3</command>
        <command name="load">perl/5.32.0-bsnc6lt</command>
        <command name="load">cmake/3.24.2-whgdv7y</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/20.0.4-kodw73g</command>
        <command name="load">intel-mkl/2020.4.304-g2qaxzf</command>
      </modules>
      <modules compiler="intel" mpilib="openmpi">
        <command name="load">openmpi/4.1.6-2mm63n2</command>
        <command name="load">hdf5/1.10.7-4cghwvq</command>
        <command name="load">netcdf-c/4.7.4-4qjdadt</command>
        <command name="load">netcdf-fortran/4.5.3-qozrykr</command>
        <command name="load">parallel-netcdf/1.11.0-icrpxty</command>
      </modules>
      <modules compiler="intel" mpilib="impi">
        <command name="load">intel-mpi/2019.9.304-tkzvizk</command>
        <command name="load">hdf5/1.10.7-wczt56s</command>
        <command name="load">netcdf-c/4.7.4-ba6agmb</command>
        <command name="load">netcdf-fortran/4.5.3-5lvy5p4</command>
        <command name="load">parallel-netcdf/1.11.0-b74wv4m</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/11.2.0-bgddrif</command>
        <command name="load">intel-oneapi-mkl/2022.1.0-w4kgsn4</command>
      </modules>
      <modules compiler="gnu" mpilib="openmpi">
        <command name="load">openmpi/4.1.6-ggebj5o</command>
        <command name="load">hdf5/1.10.7-ol6xuae</command>
        <command name="load">netcdf-c/4.7.4-pfocec2</command>
        <command name="load">netcdf-fortran/4.5.3-va3hoor</command>
        <command name="load">parallel-netcdf/1.11.0-d7h4ysd</command>
      </modules>
      <modules compiler="gnu" mpilib="impi">
        <command name="unload">gcc/11.2.0-bgddrif</command>
        <command name="unload">intel-oneapi-mkl/2022.1.0-w4kgsn4</command>
        <command name="load">gcc/9.2.0-ugetvbp</command>
        <command name="load">intel-mkl/2020.4.304-n3b5fye</command>
        <command name="load">intel-mpi/2019.9.304-jdih7h5</command>
        <command name="load">hdf5/1.8.16-dtbpce3</command>
        <command name="load">netcdf-c/4.7.4-seagl7g</command>
        <command name="load">netcdf-fortran/4.5.3-ova6t37</command>
        <command name="load">parallel-netcdf/1.11.0-ifdodru</command>
      </modules>
      <modules compiler="oneapi-ifx">
        <command name="load">intel-oneapi-compilers/2022.2.0-dioefq5</command>
        <command name="load">intel-oneapi-mkl/2022.2.0-w2y75l6</command>
      </modules>
      <modules compiler="oneapi-ifx" mpilib="openmpi">
        <command name="load">openmpi/4.1.3-ti25nay</command>
        <command name="load">hdf5/1.10.7-clocd2t</command>
        <command name="load">netcdf-c/4.7.4-nw7dztf</command>
        <command name="load">netcdf-fortran/4.5.3-qa6rhpj</command>
        <command name="load">parallel-netcdf/1.11.0-ejxy7kk</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.05</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.05</TEST_MEMLEAK_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="PERL5LIB">/lcrc/group/e3sm/soft/perl/chrys/lib/perl5</env>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
      <env name="OMPI_MCA_sharedfp">^lockedfile,individual</env>
      <env name="UCX_TLS">^xpmem</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="intel" MAX_TASKS_PER_NODE="!128">
      <env name="KMP_AFFINITY">granularity=core,balanced</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="intel" MAX_TASKS_PER_NODE="128">
      <env name="KMP_AFFINITY">granularity=thread,balanced</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
    <environment_variables compiler="intel" mpilib="openmpi">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /lcrc/soft/climate/moab/chrysalis/intel; else echo "$MOAB_ROOT"; fi}</env>
      <env name="Albany_ROOT">$SHELL{if [ -z "$Albany_ROOT" ]; then echo /lcrc/group/e3sm/soft/albany/2024.03.26/intel/20.0.4; else echo "$Albany_ROOT"; fi}</env>
      <env name="Trilinos_ROOT">$SHELL{if [ -z "$Trilinos_ROOT" ]; then echo /lcrc/group/e3sm/soft/trilinos/15.1.1/intel/20.0.4; else echo "$Trilinos_ROOT"; fi}</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="openmpi">
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /lcrc/soft/climate/moab/chrysalis/gnu; else echo "$MOAB_ROOT"; fi}</env>
      <env name="Albany_ROOT">$SHELL{if [ -z "$Albany_ROOT" ]; then echo /lcrc/group/e3sm/soft/albany/2024.03.26/gcc/9.2.0; else echo "$Albany_ROOT"; fi}</env>
      <env name="Trilinos_ROOT">$SHELL{if [ -z "$Trilinos_ROOT" ]; then echo /lcrc/group/e3sm/soft/trilinos/15.1.1/gcc/9.2.0; else echo "$Trilinos_ROOT"; fi}</env>
    </environment_variables>
  </machine>

  <machine MACH="blues">
    <DESC>ANL/LCRC Linux Cluster</DESC>
    <OS>LINUX</OS>
    <COMPILERS>pgigpu</COMPILERS>
    <MPILIBS>mvapich</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/lcrc/group/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/blues</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/blues/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/soft/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks"> -l -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit </arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="placement">-m plane=$SHELL{echo 16/$OMP_NUM_THREADS|bc} </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/sh;export MODULEPATH=$MODULEPATH:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core:/blues/gpfs/home/software/spack-0.10.1/share/spack/lmod/linux-centos7-x86_64/Core</init_path>
      <init_path lang="csh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/csh;setenv MODULEPATH $MODULEPATH\:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core\:/blues/gpfs/home/software/spack-0.10.1/share/spack/lmod/linux-centos7-x86_64/Core</init_path>
      <init_path lang="python">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">export MODULEPATH=$MODULEPATH:/software/centos7/spack-latest/share/spack/lmod/linux-centos7-x86_64/Core:/blues/gpfs/home/software/spack-0.10.1/share/spack/lmod/linux-centos7-x86_64/Core;/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">cmake/3.20.3-vedypwm</command>
      </modules>
      <modules compiler="pgigpu">
        <command name="load">nvhpc/20.9-5brtudu</command>
        <command name="load">cuda/11.1.0-6dvax5z</command>
        <command name="load">netcdf-c/4.7.4-ltqliri</command>
        <command name="load">netcdf-cxx/4.2-kf5ox4e</command>
        <command name="load">netcdf-fortran/4.5.3-6mgyroo</command>
        <command name="load">mvapich2/2.3.4-blues-5fwicb5</command>
        <command name="load">parallel-netcdf/1.12.1-nyuvwhn</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
      <env name="PATH">/lcrc/group/e3sm/soft/perl/5.26.0/bin:$ENV{PATH}</env>
    </environment_variables>
    <environment_variables mpilib="mvapich">
      <env name="MV2_ENABLE_AFFINITY">0</env>
      <env name="MV2_SHOW_CPU_BINDING">1</env>
    </environment_variables>
    <environment_variables mpilib="mvapich" DEBUG="TRUE">
      <env name="MV2_DEBUG_SHOW_BACKTRACE">1</env>
      <env name="MV2_SHOW_ENV_INFO">2</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="swing">
    <DESC>ANL/LCRC Linux Cluster: 6x 128c EPYC nodes with 8x A100 GPUs</DESC>
    <NODENAME_REGEX>gpulogin.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>pgigpu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <SAVE_TIMING_DIR>/lcrc/group/e3sm</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/swing</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/swing/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/soft/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_gpu</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="pgigpu">16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="pgigpu">16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks"> -l -n {{ total_tasks }} -N {{ num_nodes }} -K </arg>
        <arg name="binding">$SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;}</arg>
        <arg name="thread_count">-c $SHELL{echo 256/ {{ tasks_per_node }} |bc}</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/gpfs/fs1/soft/swing/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-5tuyfdb/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/gpfs/fs1/soft/swing/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-5tuyfdb/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/gpfs/fs1/soft/swing/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-5tuyfdb/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">/gpfs/fs1/soft/swing/spack/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/lmod-8.3-5tuyfdb/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">cmake/3.21.1-e5i6eks</command>
      </modules>
      <modules compiler="pgigpu">
        <command name="load">nvhpc/20.9-37zsymt</command>
        <command name="load">cuda/11.1.1-nkh7mm7</command>
        <command name="load">openmpi/4.1.1-r6ebr2e</command>
        <command name="load">netcdf-c/4.7.4-zppo53l</command>
        <command name="load">netcdf-cxx/4.2-wjm7fye</command>
        <command name="load">netcdf-fortran/4.5.3-srsajjs</command>
        <command name="load">parallel-netcdf/1.12.1-75szceu</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
      <env name="PATH">/lcrc/group/e3sm/soft/perl/5.26.0/bin:$ENV{PATH}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="bebop">
    <DESC>ANL/LCRC Cluster, Cray CS400, 352-nodes Xeon Phi 7230 KNLs 64C/1.3GHz + 672-nodes Xeon E5-2695v4 Broadwells 36C/2.10GHz, Intel Omni-Path network, SLURM batch system, Lmod module environment.</DESC>
    <NODENAME_REGEX>beboplogin.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>impi,mvapich</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/bebop</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/bebop/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/soft/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="impi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -l -n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks"> -l -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit </arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="placement">-m plane=$SHELL{echo 36/$OMP_NUM_THREADS|bc}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/gcc-4.8.5/lmod-7.4.9-ic63herzfgw5u3na5mdtvp3nwxy6oj2z/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">cmake/3.20.3-vedypwm</command>
        <command name="load">anaconda3/5.2.0</command>
      </modules>
      <modules compiler="intel">
        <command name="load">gcc/7.4.0</command>
        <command name="load">intel/20.0.4-lednsve</command>
        <command name="load">intel-mkl/2020.4.304-voqlapk</command>
      </modules>
      <modules compiler="intel" mpilib="impi">
        <command name="load">intel-mpi/2019.9.304-i42whlw</command>
        <command name="load">hdf5/1.10.7-ugvomvt</command>
        <command name="load">netcdf-c/4.4.1-blyisdg</command>
        <command name="load">netcdf-cxx/4.2-gkqc6fq</command>
        <command name="load">netcdf-fortran/4.4.4-eanrh5t</command>
        <command name="load">parallel-netcdf/1.11.0-y3nmmej</command>
      </modules>
      <modules compiler="intel" mpilib="mvapich">
        <command name="load">mvapich2/2.3.6-verbs-x4iz7lq</command>
        <command name="load">hdf5/1.10.7-igh6foh</command>
        <command name="load">netcdf-c/4.4.1-gei7x7w</command>
        <command name="load">netcdf-cxx/4.2-db2f5or</command>
        <command name="load">netcdf-fortran/4.4.4-b4ldb3a</command>
        <command name="load">parallel-netcdf/1.11.0-kj4jsvt</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/8.2.0-xhxgy33</command>
        <command name="load">intel-mkl/2020.4.304-d6zw4xa</command>
      </modules>
      <modules compiler="gnu" mpilib="impi">
        <command name="load">intel-mpi/2019.9.304-rxpzd6p</command>
        <command name="load">hdf5/1.10.7-oy6d2nm</command>
        <command name="load">netcdf-c/4.4.1-fysjgfx</command>
        <command name="load">netcdf-cxx/4.2-oaiw2v6</command>
        <command name="load">netcdf-fortran/4.4.4-kxgkaop</command>
        <command name="load">parallel-netcdf/1.11.0-fce7akl</command>
      </modules>
      <modules compiler="gnu" mpilib="mvapich">
        <command name="load">mvapich2/2.3-bebop-a66r4jf</command>
        <command name="load">hdf5/1.10.5-ejeshwh</command>
        <command name="load">netcdf/4.4.1-ve2zfkw</command>
        <command name="load">netcdf-cxx/4.2-2rkopdl</command>
        <command name="load">netcdf-fortran/4.4.4-thtylny</command>
        <command name="load">parallel-netcdf/1.11.0-kozyofv</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PATH">/lcrc/group/e3sm/soft/perl/5.26.0/bin:$ENV{PATH}</env>
    </environment_variables>
    <environment_variables mpilib="!mpi-serial">
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
    </environment_variables>
    <environment_variables>
      <env name="HDF5_ROOT">$SHELL{which h5dump | xargs dirname | xargs dirname}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
    <environment_variables mpilib="impi">
      <env name="I_MPI_FABRICS">shm:tmi</env>
    </environment_variables>
  </machine>

  <machine MACH="improv">
    <DESC>ANL LCRC cluster 825-node AMD 7713 2-sockets 128-cores per node</DESC>
    <NODENAME_REGEX>ilogin(1|2|3|4).lcrc.anl.gov</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>e3sm</PROJECT>
    <CIME_OUTPUT_ROOT>/lcrc/group/e3sm/$USER/scratch/improv</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lcrc/group/e3sm/data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lcrc/group/e3sm/data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lcrc/group/e3sm/$USER/scratch/improv/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lcrc/group/e3sm/baselines/improv/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lcrc/group/e3sm/tools/cprnc/cprnc.improv</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>8</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>pbspro</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">--tag-output -n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:1:core:PE=$ENV{OMP_NUM_THREADS} --bind-to core --oversubscribe </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">/gpfs/fs1/soft/chrysalis/spack/opt/spack/linux-centos8-x86_64/gcc-9.3.0/lmod-8.3-5be73rg/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="--force purge"/>
        <command name="load">cmake/3.27.4</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/12.3.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.05</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables compiler="gnu" mpilib="openmpi">
      <env name="NETCDF_C_PATH">/lcrc/group/e3sm/soft/improv/netcdf-c/4.9.2b/gcc-12.3.0/openmpi-4.1.6</env>
      <env name="NETCDF_FORTRAN_PATH">/lcrc/group/e3sm/soft/improv/netcdf-fortran/4.6.1b/gcc-12.3.0/openmpi-4.1.6</env>
      <env name="PNETCDF_PATH">/lcrc/group/e3sm/soft/improv/pnetcdf/1.12.3/gcc-12.3.0/openmpi-4.1.6</env>
      <env name="PATH">/lcrc/group/e3sm/soft/improv/pnetcdf/1.12.3/gcc-12.3.0/openmpi-4.1.6/bin:/lcrc/group/e3sm/soft/improv/netcdf-fortran/4.6.1b/gcc-12.3.0/openmpi-4.1.6/bin:/lcrc/group/e3sm/soft/improv/netcdf-c/4.9.2b/gcc-12.3.0/openmpi-4.1.6/bin:/lcrc/group/e3sm/soft/improv/openmpi/4.1.6/gcc-12.3.0/bin:/lcrc/group/e3sm/soft/perl/improv/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">$SHELL{lp=/lcrc/group/e3sm/soft/improv/netlib-lapack/3.12.0/gcc-12.3.0:/lcrc/group/e3sm/soft/improv/pnetcdf/1.12.3/gcc-12.3.0/openmpi-4.1.6/lib:/lcrc/group/e3sm/soft/improv/netcdf-fortran/4.6.1b/gcc-12.3.0/openmpi-4.1.6/lib:/lcrc/group/e3sm/soft/improv/netcdf-c/4.9.2b/gcc-12.3.0/openmpi-4.1.6/lib:/opt/pbs/lib:/lcrc/group/e3sm/soft/improv/openmpi/4.1.6/gcc-12.3.0/lib; if [ -z "$LD_LIBRARY_PATH" ]; then echo $lp; else echo "$lp:$LD_LIBRARY_PATH"; fi}</env>
      <env name="MOAB_ROOT">$SHELL{if [ -z "$MOAB_ROOT" ]; then echo /lcrc/soft/climate/moab/improv/gnu; else echo "$MOAB_ROOT"; fi}</env>
      <env name="OMPI_MCA_sharedfp">^lockedfile,individual</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="ruby">
    <DESC>LLNL Linux Cluster, Linux (pgi), 56 pes/node, batch system is Slurm</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>cbronze</PROJECT>
    <CIME_OUTPUT_ROOT>/p/lustre2/$USER/e3sm_scratch/ruby</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/usr/workspace/e3sm/ccsm3data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/usr/workspace/e3sm/ccsm3data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/p/lustre2/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/p/lustre2/$USER/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/usr/workspace/e3sm/apps/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>boutte3 -at- llnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>56</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>56</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="pmi_layer">--mpi=pmi2</arg>
        <arg name="export">--export=ALL</arg>
        <arg name="num_tasks">-n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c 1</arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <modules compiler="intel">
        <command name="load">python/3.9.12</command>
        <command name="load">git</command>
        <command name="load">subversion</command>
        <command name="load">cmake/3.19.2</command>
        <command name="load">mkl/2022.1.0</command>
        <command name="load">intel-classic/2021.6.0-magic</command>
        <command name="use --append">/usr/workspace/e3sm/spack/modules/ruby/linux-rhel8-x86_64/Core</command>
        <command name="load">mvapich2/2.3.7-ll7cmqm</command>
        <command name="load">hdf5/1.10.7-ewjpbjd</command>
        <command name="load">netcdf-c/4.4.1.1-vaxofek</command>
        <command name="load">netcdf-fortran/4.4.4-3pzbx2u</command>
        <command name="load">parallel-netcdf/1.11.0-tzgdala</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables compiler="intel">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="HDF5_ROOT">/usr/workspace/e3sm/spack/libs/linux-rhel8-cascadelake/intel-2021.6.0/hdf5-1.10.7-ewjpbjdhjgjzrzjcvwyjyuulaesbsjhg</env>
      <env name="NETCDF_C_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-cascadelake/intel-2021.6.0/netcdf-c-4.4.1.1-vaxofekwvnvngh7wptmzkwdb7tkzvesn</env>
      <env name="NETCDF_FORTRAN_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-cascadelake/intel-2021.6.0/netcdf-fortran-4.4.4-3pzbx2unddhladhubaahhhysjmprzqi2</env>
      <env name="PNETCDF_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-cascadelake/intel-2021.6.0/parallel-netcdf-1.11.0-tzgdalakmem7tod6cruhqyeackeix5q5</env>
    </environment_variables>
  </machine>

  <machine MACH="dane">
    <DESC>LLNL Linux Cluster, 112 pes/node, batch system is Slurm</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>cbronze</PROJECT>
    <CIME_OUTPUT_ROOT>/p/lustre2/$USER/e3sm_scratch/dane</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/usr/workspace/e3sm/ccsm3data/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/usr/workspace/e3sm/ccsm3data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/p/lustre2/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/p/lustre2/$USER/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/usr/workspace/e3sm/apps/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>boutte3 -at- llnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>224</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>112</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="pmi_layer">--mpi=pmi2</arg>
        <arg name="export">--export=ALL</arg>
        <arg name="num_tasks">-n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c 1</arg>
        <arg name="binding">--cpu_bind=cores</arg>
        <arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <modules compiler="intel">
        <command name="load">python/3.9.12</command>
        <command name="load">git</command>
        <command name="load">subversion</command>
        <command name="load">mkl/2022.1.0</command>
        <command name="load">intel-classic/2021.6.0-magic</command>
        <command name="load">cmake/3.19.2</command>
        <command name="use --append">/usr/workspace/e3sm/spack/modules/dane/linux-rhel8-x86_64/Core</command>
        <command name="load">mvapich2/2.3.7-27jao34</command>
        <command name="load">hdf5/1.10.7-766kapa</command>
        <command name="load">netcdf-c/4.4.1.1-2uznnlw</command>
        <command name="load">netcdf-fortran/4.4.4-itpstyo</command>
        <command name="load">parallel-netcdf/1.11.0-26sxm4m</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables compiler="intel">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="HDF5_ROOT">/usr/workspace/e3sm/spack/libs/linux-rhel8-sapphirerapids/intel-2021.6.0/hdf5-1.10.7-766kapalbrdntu2pcgdgbhg2ch26gsuv</env>
      <env name="NETCDF_C_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-sapphirerapids/intel-2021.6.0/netcdf-c-4.4.1.1-2uznnlwgiezxute6iyqzqjrpolokeaib</env>
      <env name="NETCDF_FORTRAN_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-sapphirerapids/intel-2021.6.0/netcdf-fortran-4.4.4-itpstyordbern7vlulmlnt47eeeokzfp</env>
      <env name="PNETCDF_PATH">/usr/workspace/e3sm/spack/libs/linux-rhel8-sapphirerapids/intel-2021.6.0/parallel-netcdf-1.11.0-26sxm4mormsglmhi24poix7sugbigkck</env>
    </environment_variables>
  </machine>

  <machine MACH="jlse">
    <DESC>ANL experimental/evaluation cluster, batch system is cobalt</DESC>
    <NODENAME_REGEX>jlse.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>oneapi-ifx,oneapi-ifxgpu,gnu</COMPILERS>
    <MPILIBS>mpich,impi,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/gpfs/jlse-fs0/projects/climate/$USER/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/gpfs/jlse-fs0/projects/climate/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/gpfs/jlse-fs0/projects/climate/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/gpfs/jlse-fs0/projects/climate/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/gpfs/jlse-fs0/projects/climate/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>cobalt_theta</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>112</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="oneapi-ifx">96</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="oneapi-ifxgpu">96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>112</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="oneapi-ifx">48</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="oneapi-ifxgpu">48</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-l -n {{ total_tasks }} -bind-to core</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">--tag-output -n {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_numa }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to hwthread</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <modules>
	<command name="purge"/>
        <command name="use">/soft/modulefiles</command>
        <command name="load">cmake/3.22.1</command>
        <command name="use">/soft/restricted/CNDA/modules</command>
      </modules>
      <modules compiler="!gnu">
	<command name="load">oneapi/eng-compiler/2022.10.15.006</command>
      </modules>
      <modules compiler="gnu">
        <command name="unload">cmake</command>
	<command name="load">gcc/8.2.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="PATH">/home/azamat/soft/perl/5.32.0/bin:$ENV{PATH}</env>
      <env name="NETCDF_PATH">/home/azamat/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/oneapi-2020.12.15.004-intel_mpi-2019.4.243</env>
      <env name="PNETCDF_PATH">/home/azamat/soft/pnetcdf/1.12.1/oneapi-2020.12.15.004-intel_mpi-2019.4.243</env>
      <env name="LAPACK_ROOT">/home/azamat/soft/libs</env>
    </environment_variables>
    <environment_variables mpilib="mpich" DEBUG="TRUE">
      <env name="HYDRA_TOPO_DEBUG">1</env>
    </environment_variables>
    <environment_variables mpilib="impi">
      <env name="I_MPI_DEBUG">10</env>
      <env name="I_MPI_PIN_DOMAIN">omp</env>
      <env name="I_MPI_PIN_ORDER">spread</env>
      <env name="I_MPI_PIN_CELL">unit</env>
    </environment_variables>
    <environment_variables compiler="intel" mpilib="openmpi">
      <env name="OMPI_CC">icc</env>
      <env name="OMPI_CXX">icpc</env>
      <env name="OMPI_FC">ifort</env>
      <env name="PATH">/home/azamat/soft/openmpi/2.1.6/intel19/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/home/azamat/soft/openmpi/2.1.6/intel19/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="NETCDF_PATH">/home/azamat/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/intel19-openmpi2.1.6</env>
      <env name="PNETCDF_PATH">/home/azamat/soft/pnetcdf/1.12.1/intel19-openmpi2.1.6</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="openmpi">
      <env name="OMPI_CC">gcc</env>
      <env name="OMPI_CXX">g++</env>
      <env name="OMPI_FC">gfortran</env>
      <env name="LD_LIBRARY_PATH">/home/azamat/soft/openmpi/2.1.6/gcc8.2.0/lib:/home/azamat/soft/libs:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PATH">/home/azamat/soft/openmpi/2.1.6/gcc8.2.0/bin:/home/azamat/soft/cmake/3.18.5/bin:$ENV{PATH}</env>
      <env name="CMAKE_ROOT">/home/azamat/soft/cmake/3.18.5</env>
      <env name="ACLOCAL_PATH">/home/azamat/soft/cmake/3.18.5/share/aclocal</env>
      <env name="CMAKE_PREFIX_PATH">/home/azamat/soft/cmake/3.18.5</env>
      <env name="NETCDF_PATH">/home/azamat/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/gcc8.2.0-openmpi2.1.6</env>
      <env name="PNETCDF_PATH">/home/azamat/soft/pnetcdf/1.12.1/gcc8.2.0-openmpi2.1.6</env>
    </environment_variables>
    <environment_variables compiler="oneapi-ifxgpu">
      <env name="SYCL_DEVICE_FILTER">opencl</env>
      <env name="ONEAPI_MPICH_GPU">NO_GPU</env>
      <env name="SYCL_CACHE_PERSISTENT">0</env>
      <env name="GATOR_INITIAL_MB">4000MB</env>
      <env name="GATOR_DISABLE">0</env>
    </environment_variables>
    <environment_variables compiler="oneapi-ifx">
      <env name="LIBOMPTARGET_DEBUG">0</env><!--default 0, max 5 -->
      <env name="OMP_TARGET_OFFLOAD">DISABLED</env><!--default OMP_TARGET_OFFLOAD=MANDATORY-->
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="!gnu">
      <env name="KMP_AFFINITY">verbose,granularity=thread,balanced</env>
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">threads</env>
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="polaris">
    <DESC>ALCF Polaris 560 nodes, 2.8 GHz AMD EPYC Milan 7543P 32c CPU, 4 NVIDIA A100 GPUs</DESC>
    <NODENAME_REGEX>polaris-*</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnu,gnugpu,nvidia,nvidiagpu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>E3SM_RRM</PROJECT>
    <CHARGE_ACCOUNT>E3SM_RRM</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/grand/E3SMinput/performance_archive</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>SCREAM_Calib,E3SM_RRM</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/eagle/$PROJECT/$USER/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/grand/E3SMinput/data</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/grand/E3SMinput/data/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/grand/E3SMinput/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/grand/E3SMinput/soft/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>pbspro</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnu">32</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="nvidia">32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
       <executable>mpiexec</executable>
       <arguments>
          <arg name="total_num_tasks">-np {{ total_tasks }} --label</arg>
          <arg name="ranks_per_node">-ppn {{ tasks_per_node }}</arg>
          <arg name="ranks_bind">--cpu-bind depth -envall</arg>
          <arg name="threads_per_rank">-d $ENV{OMP_NUM_THREADS}</arg>
          <arg name="gpu_maps">$ENV{GPU_TILE_COMPACT}</arg>
       </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="python">/usr/share/lmod/8.3.1/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/8.3.1/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/8.3.1/init/csh</init_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">cmake/3.23.2</command>
        <command name="load">craype-x86-rome</command>
      </modules>
      <modules compiler="gnu.*">
        <command name="load">PrgEnv-gnu/8.3.3</command>
      </modules>
      <modules compiler="gnugpu">
        <command name="swap">gcc/12.2.0 gcc/11.2.0</command>
        <command name="load">cudatoolkit-standalone/11.4.4</command>
      </modules>
      <modules compiler="nvidia.*">
        <command name="load">PrgEnv-nvhpc/8.3.3</command>
      </modules>
      <modules compiler="nvidiagpu">
        <command name="load">cudatoolkit-standalone/11.4.4</command>
        <command name="load">craype-accel-nvidia80</command>
      </modules>
      <modules>
        <command name="load">craype-network-ofi</command>
        <command name="load">libfabric/1.15.2.0</command>
        <command name="load">cray-libsci/23.02.1.1</command>
        <command name="load">cray-hdf5-parallel/1.12.2.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.3</command>
        <command name="load">cray-parallel-netcdf/1.12.3.3</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
      <env name="MPICH_GPU_SUPPORT_ENABLED">0</env>
      <env name="CRAY_ACCEL_TARGET">host</env>
      <env name="GPU_TILE_COMPACT"> </env>
    </environment_variables>
    <environment_variables compiler=".*gpu">
      <env name="MPICH_GPU_SUPPORT_ENABLED">1</env>
      <env name="CRAY_ACCEL_TARGET">nvidia80</env>
      <env name="GPU_TILE_COMPACT">/grand/E3SMinput/soft/qsub/set_affinity_gpu_polaris.sh</env>
    </environment_variables>
    <environment_variables compiler="gnu.*">
      <env name="LD_PRELOAD">/opt/cray/pe/gcc/11.2.0/snos/lib64/libstdc++.so</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="sunspot">
    <DESC>ANL Sunspot Test and Development System (TDS), batch system is pbspro</DESC>
    <NODENAME_REGEX>uan-.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>oneapi-ifx,oneapi-ifxgpu,gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CHARGE_ACCOUNT>CSC249ADSE15_CNDA</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/gila/CSC249ADSE15_CNDA/performance_archive</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lus/gila/projects/CSC249ADSE15_CNDA/$USER/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lus/gila/projects/CSC249ADSE15_CNDA/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lus/gila/projects/CSC249ADSE15_CNDA/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lus/gila/projects/CSC249ADSE15_CNDA/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lus/gila/projects/CSC249ADSE15_CNDA/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>pbspro</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>208</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="oneapi-ifxgpu">96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>104</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="oneapi-ifxgpu">48</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
       <executable>mpiexec</executable>
       <!--executable>numactl -m 2-3 mpiexec</executable--><!--for HBM runs-->
       <arguments>
          <arg name="total_num_tasks">-np {{ total_tasks }} --label</arg>
          <arg name="ranks_per_node">-ppn {{ tasks_per_node }}</arg>
          <arg name="ranks_bind">--cpu-bind depth -envall</arg>
          <arg name="threads_per_rank">-d $ENV{OMP_NUM_THREADS}</arg>
          <arg name="gpu_maps">$ENV{GPU_TILE_COMPACT}</arg>
       </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
       <init_path lang="sh">/soft/packaging/lmod/lmod/init/sh</init_path>
       <init_path lang="csh">/soft/packaging/lmod/lmod/init/csh</init_path>
       <init_path lang="python">/soft/packaging/lmod/lmod/init/env_modules_python.py</init_path>
       <cmd_path lang="sh">module</cmd_path>
       <cmd_path lang="csh">module</cmd_path>
       <cmd_path lang="python">/soft/packaging/lmod/lmod/libexec/lmod python</cmd_path>
       <modules>
          <command name="load">cmake</command>
        </modules>
        <modules compiler="!gnu">
          <command name="load">oneapi/eng-compiler/2024.07.30.002</command>
        </modules>
        <modules compiler="oneapi-ifxgpu">
          <command name="load">kokkos/4.4.01-omp-sycl</command>
        </modules>
        <modules compiler="gnu">
           <command name="unload">spack cmake</command>
           <command name="load">gcc/10.3.0</command>
        </modules>
     </module_system>
     <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
     <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
     <environment_variables>
        <env name="NETCDF_PATH">/lus/gila/projects/CSC249ADSE15_CNDA/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002</env>
        <env name="PNETCDF_PATH">/lus/gila/projects/CSC249ADSE15_CNDA/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002</env>
        <env name="LD_LIBRARY_PATH">/lus/gila/projects/CSC249ADSE15_CNDA/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002/lib:/lus/gila/projects/CSC249ADSE15_CNDA/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002/lib:$ENV{LD_LIBRARY_PATH}</env>
        <env name="PATH">/lus/gila/projects/CSC249ADSE15_CNDA/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002/bin:/lus/gila/projects/CSC249ADSE15_CNDA/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002/bin:$ENV{PATH}</env>
     </environment_variables>
     <environment_variables mpilib="mpich" DEBUG="TRUE">
        <env name="HYDRA_TOPO_DEBUG">1</env>
     </environment_variables>
     <environment_variables compiler="oneapi-ifxgpu">
        <env name="ONEAPI_DEVICE_SELECTOR">level_zero:gpu</env>
        <env name="ONEAPI_MPICH_GPU">NO_GPU</env>
        <env name="MPIR_CVAR_ENABLE_GPU">0</env>
        <env name="romio_cb_read">disable</env>
        <env name="romio_cb_write">disable</env>
        <env name="SYCL_CACHE_PERSISTENT">1</env>
        <env name="GATOR_INITIAL_MB">4000MB</env>
        <env name="GATOR_DISABLE">0</env>
        <env name="GPU_TILE_COMPACT">/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</env>
        <env name="FI_CXI_DEFAULT_CQ_SIZE">131072</env>
        <env name="FI_CXI_CQ_FILL_PERCENT">20</env>
        <env name="Kokkos_ROOT">$ENV{KOKKOS_ROOT}</env>
        <env name="ZES_ENABLE_SYSMAN">1</env>
        <env name="ZEX_NUMBER_OF_CCS">0:4,1:4,2:4,3:4:4:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4</env>
    </environment_variables>
    <environment_variables compiler="oneapi-ifx">
        <env name="LIBOMPTARGET_DEBUG">0</env><!--default 0, max 5 -->
        <env name="OMP_TARGET_OFFLOAD">DISABLED</env><!--default OMP_TARGET_OFFLOAD=MANDATORY-->
        <env name="FI_CXI_DEFAULT_CQ_SIZE">131072</env>
        <env name="FI_CXI_CQ_FILL_PERCENT">20</env>
        <env name="MPIR_CVAR_ENABLE_GPU">0</env>
        <env name="GPU_TILE_COMPACT"> </env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="!gnu">
        <env name="KMP_AFFINITY">verbose,granularity=thread,balanced</env>
        <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
        <env name="OMP_PLACES">threads</env>
        <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <resource_limits>
        <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="aurora">
    <DESC>ALCF Aurora, 10624 nodes, 2x52c SPR, 6x2s PVC, 2x512GB DDR5, 2x64GB CPU-HBM, 6x128GB GPU-HBM, Slingshot 11, PBSPro</DESC>
    <NODENAME_REGEX>aurora-uan-.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>oneapi-ifx,oneapi-ifxgpu,gnu</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>E3SM_Dec</PROJECT>
    <SAVE_TIMING_DIR>/lus/flare/projects/E3SM_Dec/performance_archive</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/lus/flare/projects/$PROJECT/$USER/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lus/flare/projects/E3SM_Dec/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lus/flare/projects/E3SM_Dec/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lus/flare/projects/E3SM_Dec/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lus/flare/projects/E3SM_Dec/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>pbspro</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>208</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="oneapi-ifxgpu">96</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>104</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="oneapi-ifxgpu">48</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
       <executable>mpiexec</executable>
       <!--executable>numactl -m 2-3 mpiexec</executable--><!--for HBM runs-->
       <arguments>
          <arg name="total_num_tasks">-np {{ total_tasks }} --label</arg>
          <arg name="ranks_per_node">-ppn {{ tasks_per_node }}</arg>
          <arg name="ranks_bind">-envall</arg>
          <arg name="threads_per_rank">-d $ENV{OMP_NUM_THREADS}</arg>
          <arg name="gpu_maps">$ENV{GPU_TILE_COMPACT}</arg>
       </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
       <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
       <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
       <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
       <cmd_path lang="sh">module</cmd_path>
       <cmd_path lang="csh">module</cmd_path>
       <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
       <modules>
          <command name="load">cmake</command>
        </modules>
        <modules compiler="!gnu">
          <command name="load">oneapi/eng-compiler/2024.07.30.002</command>
        </modules>
        <modules compiler="oneapi-ifxgpu">
          <command name="load">kokkos/4.4.01-omp-sycl</command>
        </modules>
        <modules compiler="gnu">
           <command name="unload">spack-pe-gcc cmake</command>
           <command name="load">gcc/10.3.0</command>
        </modules>
     </module_system>
     <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
     <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
     <environment_variables>
        <env name="NETCDF_PATH">/lus/flare/projects/E3SM_Dec/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002</env>
        <env name="PNETCDF_PATH">/lus/flare/projects/E3SM_Dec/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002</env>
        <env name="LD_LIBRARY_PATH">/lus/flare/projects/E3SM_Dec/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002/lib:/lus/flare/projects/E3SM_Dec/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002/lib:$ENV{LD_LIBRARY_PATH}</env>
        <env name="PATH">/lus/flare/projects/E3SM_Dec/soft/pnetcdf/1.14.0/oneapi.eng.2024.07.30.002/bin:/lus/flare/projects/E3SM_Dec/soft/netcdf/4.9.2c-4.6.1f/oneapi.eng.2024.07.30.002/bin:$ENV{PATH}</env>
     </environment_variables>
     <environment_variables DEBUG="TRUE">
        <env name="HYDRA_TOPO_DEBUG">1</env>
     </environment_variables>
     <environment_variables compiler="oneapi-ifxgpu">
        <env name="ONEAPI_DEVICE_SELECTOR">level_zero:gpu</env>
        <env name="ONEAPI_MPICH_GPU">NO_GPU</env>
        <env name="MPIR_CVAR_ENABLE_GPU">0</env>
        <env name="romio_cb_read">disable</env>
        <env name="romio_cb_write">disable</env>
        <env name="SYCL_CACHE_PERSISTENT">1</env>
        <env name="GATOR_INITIAL_MB">4000MB</env>
        <env name="GATOR_DISABLE">0</env>
        <env name="GPU_TILE_COMPACT">/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</env>
        <env name="FI_CXI_DEFAULT_CQ_SIZE">131072</env>
        <env name="FI_CXI_CQ_FILL_PERCENT">20</env>
        <env name="Kokkos_ROOT">$ENV{KOKKOS_ROOT}</env>
        <env name="ZES_ENABLE_SYSMAN">1</env>
        <env name="ZEX_NUMBER_OF_CCS">0:4,1:4,2:4,3:4:4:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4</env>
    </environment_variables>
    <environment_variables compiler="oneapi-ifx">
        <env name="LIBOMPTARGET_DEBUG">0</env><!--default 0, max 5 -->
        <env name="OMP_TARGET_OFFLOAD">DISABLED</env><!--default OMP_TARGET_OFFLOAD=MANDATORY-->
        <env name="FI_CXI_DEFAULT_CQ_SIZE">131072</env>
        <env name="FI_CXI_CQ_FILL_PERCENT">20</env>
        <env name="MPIR_CVAR_ENABLE_GPU">0</env>
        <env name="GPU_TILE_COMPACT"> </env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="!gnu">
        <env name="KMP_AFFINITY">verbose,granularity=thread,balanced</env>
        <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
        <env name="OMP_PLACES">threads</env>
        <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <resource_limits>
        <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="sooty">
    <DESC>PNL cluster, OS is Linux, batch system is SLURM</DESC>
    <NODENAME_REGEX>sooty</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>mvapich2</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/$USER/cime_output_root</CIME_OUTPUT_ROOT>
    <!--
    <DIN_LOC_ROOT>/lustre/climate/csmdata/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/climate/csmdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    -->
    <DIN_LOC_ROOT>/pic/projects/sooty2/$ENV{USER}/e3sm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/pic/projects/sooty2/$ENV{USER}/e3sm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lustre/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/climate/acme_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lustre/climate/acme_baselines/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>balwinder.singh -at- pnnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=none</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
        <arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/share/apps/modules/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="python">/share/apps/modules/Modules/3.2.10/init/python.py</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="perl">/share/apps/modules/Modules/3.2.10/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/share/apps/modules/Modules/3.2.10/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules>
        <command name="load">perl/5.20.0</command>
        <command name="load">cmake/3.17.1</command>
        <command name="load">svn/1.8.13</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/19.0.5</command>
        <command name="load">mkl/2019u5</command>
      </modules>
      <modules compiler="pgi">
        <command name="load">pgi/14.10</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich2/2.3.1</command>
      </modules>
      <modules>
        <command name="load">netcdf/4.6.3</command>
      </modules>
    </module_system>
    <RUNDIR>/lustre/$USER/csmruns/$CASE/run</RUNDIR>
    <EXEROOT>/lustre/$USER/csmruns/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="MKL_PATH">$ENV{MKLROOT} </env>
      <env name="NETCDF_PATH">$ENV{NETCDF_LIB}/../</env>
      <env name="OMP_STACKSIZE">64M</env>
      <!-- PJR next lines force a modern python from python/3.7.2
      module first and supply some gcc libraries last -->
      <env name="LD_LIBRARY_PATH">/share/apps/python/3.7.2/lib/:/share/apps/openssl/1.0.2r/lib:$ENV{LD_LIBRARY_PATH}:/share/apps/gcc/8.1.0/lib:/share/apps/gcc/8.1.0/lib64:</env>
      <env name="PATH">/share/apps/python/3.7.2/bin:$ENV{PATH}</env>
    </environment_variables>
  </machine>

  <machine MACH="cascade">
    <DESC>PNNL Intel KNC cluster, OS is Linux, batch system is SLURM</DESC>
    <NODENAME_REGEX>glogin</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mvapich2</MPILIBS>
    <CIME_OUTPUT_ROOT>/dtemp/$PROJECT/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/dtemp/st49401/sing201/acme/inputdata/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/dtemp/st49401/sing201/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$CIME_OUTPUT_ROOT/acme/acme_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$CIME_OUTPUT_ROOT/acme/acme_baselines/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>balwinder.singh -at- pnnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="impi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=none</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
        <arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/opt/lmod/7.8.4/init/env_modules_python.py</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="python">/opt/lmod/7.8.4/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules>
        <command name="load">python/2.7.9</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/ips_18</command>
        <command name="load">mkl/14.0</command>
      </modules>
      <modules mpilib="impi">
        <command name="load">impi/4.1.2.040</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich2/1.9</command>
      </modules>
      <modules>
        <command name="load">netcdf/4.3.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/csmruns/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/csmruns/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="NETCDF_PATH">$ENV{NETCDF_ROOT}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MKL_PATH">$ENV{MLIBHOME}</env>
      <env name="COMPILER">intel</env>
    </environment_variables>
  </machine>

  <machine MACH="constance">
    <DESC>PNL Haswell cluster, OS is Linux, batch system is SLURM</DESC>
    <NODENAME_REGEX>constance</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi,nag</COMPILERS>
    <MPILIBS>mvapich2,openmpi,intelmpi,mvapich</MPILIBS>
    <CIME_OUTPUT_ROOT>/pic/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/pic/projects/climate/csmdata/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/pic/projects/climate/csmdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/pic/scratch/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/pic/projects/climate/acme_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/pic/projects/climate/acme_baselines/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>balwinder.singh -at- pnnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=none</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
        <arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mvapich">
      <executable>srun</executable>
      <arguments>
        <arg name="num_tasks">--ntasks={{ total_tasks }}</arg>
        <arg name="cpu_bind">--cpu_bind=sockets --cpu_bind=verbose</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="intelmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/share/apps/modules/Modules/3.2.10/init/perl.pm</init_path>
      <init_path lang="python">/share/apps/modules/Modules/3.2.10/init/python.py</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="perl">/share/apps/modules/Modules/3.2.10/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/share/apps/modules/Modules/3.2.10/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules>
        <command name="load">perl/5.20.0</command>
        <!--command name="load">cmake/3.3.0</command-->
        <command name="load">cmake/3.17.1</command>
      </modules>
      <modules compiler="intel">
        <command name="load">gcc/8.1.0</command>
        <command name="load">intel/19.0.5</command>
        <command name="load">mkl/2019u5</command>
      </modules>
      <modules compiler="pgi">
        <command name="load">pgi/14.10</command>
      </modules>
      <modules compiler="nag">
        <command name="load">nag/6.0</command>
        <command name="load">mkl/15.0.1</command>
      </modules>
      <modules mpilib="mvapich">
        <command name="load">mvapich2/2.1</command>
      </modules>
      <modules mpilib="mvapich2" compiler="intel">
        <command name="load">mvapich2/2.3.1</command>
      </modules>
      <modules mpilib="mvapich2" compiler="pgi">
        <command name="load">mvapich2/2.1</command>
      </modules>
      <modules mpilib="mvapich2" compiler="nag">
        <command name="load">mvapich2/2.3b</command>
      </modules>
      <modules mpilib="intelmpi">
        <command name="load">intelmpi/5.0.1.035</command>
      </modules>
      <modules mpilib="openmpi">
        <command name="load">openmpi/1.8.3</command>
      </modules>
      <modules compiler="intel">
        <command name="load">netcdf/4.6.3</command>
      </modules>
      <modules compiler="pgi">
        <command name="load">netcdf/4.3.2</command>
      </modules>
      <modules compiler="nag">
        <command name="load">netcdf/4.4.1.1</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables compiler="intel">
      <env name="LD_LIBRARY_PATH">/share/apps/gcc/8.1.0/lib:/share/apps/gcc/8.1.0/lib64:$ENV{LD_LIBRARY_PATH}</env>
    </environment_variables>
    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
      <env name="NETCDF_PATH">$ENV{NETCDF_LIB}/../</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="MKL_PATH">$ENV{MKLROOT}</env>
    </environment_variables>
    <environment_variables compiler="nag">
      <env name="MKL_PATH">$ENV{MKLROOT}</env>
    </environment_variables>
  </machine>

  <machine MACH="compy">
    <DESC>PNL E3SM Intel Xeon Gold 6148(Skylake) nodes, OS is Linux, SLURM</DESC>
    <NODENAME_REGEX>compy</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,pgi</COMPILERS>
    <MPILIBS>impi,mvapich2</MPILIBS>
    <SAVE_TIMING_DIR>/compyfs</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/compyfs/$USER/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/compyfs/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/compyfs/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/compyfs/$USER/e3sm_scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/compyfs/e3sm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/compyfs/e3sm_baselines/cprnc/cprnc.intel.v20.0.04/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>bibi.mathew -at- pnnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>40</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>40</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=none</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }} --nodes={{ num_nodes }}</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
        <arg name="cpu_bind">-l --cpu_bind=cores -c $ENV{OMP_NUM_THREADS} -m plane=$SHELL{echo 40/$OMP_NUM_THREADS|bc}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="impi">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=pmi2</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }} --nodes={{ num_nodes }}</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
        <arg name="cpu_bind">-l --cpu_bind=cores -c $ENV{OMP_NUM_THREADS} -m plane=$SHELL{echo 40/$OMP_NUM_THREADS|bc}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/share/apps/modules/init/perl.pm</init_path>
      <init_path lang="python">/share/apps/modules/init/python.py</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="perl"> /share/apps/modules/bin/modulecmd  perl</cmd_path>
      <cmd_path lang="python">/share/apps/modules/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules>
        <command name="load">cmake/3.19.6</command>
      </modules>
      <modules compiler="intel">
        <command name="load">gcc/8.1.0</command>
        <command name="load">intel/20.0.0</command>
      </modules>
      <modules compiler="pgi">
        <command name="load">pgi/19.10</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich2/2.3.1</command>
      </modules>
      <modules mpilib="impi" compiler="intel">
	<command name="load">intelmpi/2020</command>
      </modules>
      <modules mpilib="impi" compiler="pgi">
	<command name="load">intelmpi/2019u3</command>
      </modules>
      <modules>
        <command name="load">netcdf/4.6.3</command>
        <command name="load">pnetcdf/1.9.0</command>
        <command name="load">mkl/2020</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.05</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="NETCDF_PATH">$ENV{NETCDF_ROOT}/</env>
      <env name="PNETCDF_PATH">$ENV{PNETCDF_ROOT}/</env>
      <env name="MKL_PATH">$ENV{MKLROOT}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="LD_LIBRARY_PATH">/share/apps/gcc/8.1.0/lib:/share/apps/gcc/8.1.0/lib64:$ENV{LD_LIBRARY_PATH}</env>
    </environment_variables>
    <environment_variables mpilib="mvapich2">
      <env name="MV2_ENABLE_AFFINITY">0</env>
      <env name="MV2_SHOW_CPU_BINDING">1</env>
    </environment_variables>
    <environment_variables mpilib="impi">
      <env name="I_MPI_ADJUST_ALLREDUCE">1</env>
    </environment_variables>
    <environment_variables mpilib="impi" DEBUG="TRUE">
      <env name="I_MPI_DEBUG">10</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="tahoma">
    <DESC>EMSL, OS is Linux, SLURM</DESC>
    <NODENAME_REGEX>tlogin</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi,mvapich2</MPILIBS>
    <SAVE_TIMING_DIR>/tahoma/emsls60153</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/tahoma/emsls60153/$USER/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/tahoma/emsls60153/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/tahoma/emsls60153/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/tahoma/emsls60153/$USER/e3sm_scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/tahoma/emsls60153/e3sm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/tahoma/emsls60153/e3sm_baselines/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>balwinder.singh -at- pnnl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="mvapich2">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=none</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }} --nodes={{ num_nodes }}</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
        <arg name="cpu_bind">-l --cpu_bind=cores -c $ENV{OMP_NUM_THREADS} -m plane=$SHELL{echo 40/$OMP_NUM_THREADS|bc}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="impi">
      <executable>srun</executable>
      <arguments>
        <arg name="mpi">--mpi=pmi2</arg>
        <arg name="num_tasks">--ntasks={{ total_tasks }} --nodes={{ num_nodes }}</arg>
        <arg name="kill-on-bad-exit">--kill-on-bad-exit</arg>
        <arg name="cpu_bind">-l --cpu_bind=cores -c $ENV{OMP_NUM_THREADS} -m plane=$SHELL{echo 40/$OMP_NUM_THREADS|bc}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/opt/lmod/7.8.4/init/env_modules_python.py</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <cmd_path lang="python">/opt/lmod/7.8.4/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules>
        <command name="load">cmake/3.19.5-intel</command>
      </modules>
      <modules compiler="intel">
        <command name="load">gcc/10.2.0</command>
        <command name="load">intel/ips_20_u2</command>
      </modules>
      <modules mpilib="mvapich2">
        <command name="load">mvapich2~cuda/2.3.5-intel</command>
      </modules>
      <modules mpilib="impi" compiler="intel">
	<command name="load">impi/ips_20_u2</command>
      </modules>
      <modules>
	<command name="load">netcdf-c/4.7.4-intel-intel-mpi-2019.10.317</command>
        <command name="load">netcdf-fortran/4.5.3-intel-intel-mpi-2019.10.317</command>
        <command name="load">mkl/scalapack</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.05</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>0</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="HDF5_ROOT">$SHELL{dirname $(dirname $(which h5diff))}</env>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="MKL_PATH">$ENV{MKLROOT}</env>
    </environment_variables>
    <environment_variables mpilib="mvapich2">
      <env name="MV2_ENABLE_AFFINITY">0</env>
      <env name="MV2_SHOW_CPU_BINDING">1</env>
    </environment_variables>
    <environment_variables mpilib="impi">
      <env name="I_MPI_ADJUST_ALLREDUCE">1</env>
      <env name="I_MPI_PMI_LIBRARY">$SHELL{which libpmi2.so}</env>
    </environment_variables>
    <environment_variables mpilib="impi" DEBUG="TRUE">
      <env name="I_MPI_DEBUG">10</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="oic5">
    <DESC>ORNL XK6, os is Linux, 32 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>oic5</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>mpich,openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/$USER/models/ACME</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/zdr/models/ccsm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/zdr/models/ccsm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/home/$USER/models/ACME/run/archive/$CASE</DOUT_S_ROOT>
    <GMAKE_J>32</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>dmricciuto</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="mpich">
      <executable>/projects/cesm/devtools/mpich-3.0.4-gcc4.8.1/bin/mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
        <arg name="machine_file">--hostfile $ENV{PBS_NODEFILE}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable> </executable>
    </mpirun>
    <module_system type="none" />
    <RUNDIR>/home/$USER/models/ACME/run/$CASE/run</RUNDIR>
    <EXEROOT>/home/$USER/models/ACME/run/$CASE/bld</EXEROOT>
  </machine>

  <machine MACH="cades">
    <DESC>OR-CONDO, CADES-CCSI, os is Linux, 16 pes/nodes, batch system is PBS</DESC>
    <NODENAME_REGEX>or-condo</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu,intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/or-hydra/cades-ccsi/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/lustre/or-hydra/cades-ccsi/proj-shared/project_acme/ACME_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/lustre/or-hydra/cades-ccsi/proj-shared/project_acme/ACME_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/or-hydra/cades-ccsi/proj-shared/project_acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/lustre/or-hydra/cades-ccsi/proj-shared/tools/cprnc.orcondo</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>yinj -at- ornl.gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi" compiler="gnu">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable> </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <modules>
        <command name="purge"/>
      </modules>
      <modules compiler="gnu">
        <command name="load">PE-gnu</command>
      </modules>
      <modules>
        <command name="load">mkl/2017</command>
        <command name="load">cmake/3.12.0</command>
        <command name="load">python/2.7.12</command>
        <command name="load">nco/4.6.9</command>
        <command name="load">hdf5-parallel/1.8.17</command>
        <command name="load">netcdf-hdf5parallel/4.3.3.1</command>
        <command name="load">pnetcdf/1.9.0</command>
      </modules>
    </module_system>

    <!-- customize these fields as appropriate for your system (max tasks) and
                            desired layout (change '${group}/${USER}' to your
      prefered location). -->
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <!-- for CLM-PFLOTRAN coupling, the PETSC_PATH must be defined specifically upon machines -->
    <environment_variables compiler="gnu" mpilib="openmpi">
      <env name="PETSC_PATH">/software/user_tools/current/cades-ccsi/petsc4pf/openmpi-1.10-gcc-5.3</env>
    </environment_variables>
    <environment_variables>
      <env name="PERL5LIB">/software/user_tools/current/cades-ccsi/perl5/lib/perl5/</env>
      <env name="NETCDF_PATH">/software/dev_tools/swtree/cs400_centos7.2_pe2016-08/netcdf-hdf5parallel/4.3.3.1/centos7.2_gnu5.3.0</env>
      <env name="PNETCDF_PATH">/software/dev_tools/swtree/cs400_centos7.2_pe2016-08/pnetcdf/1.9.0/centos7.2_gnu5.3.0</env>
      <env name="LAPACK_ROOT">/software/tools/compilers/intel_2017/mkl/lib/intel64</env>
    </environment_variables>

  </machine>

  <machine MACH="chicoma-cpu">
    <DESC>Chicoma CPU-only nodes at LANL IC. Each node has 2 AMD EPYC 7H12 64-Core (Milan) 512GB</DESC>
    <NODENAME_REGEX>ch-fe*</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnu,intel,nvidia,amdclang</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/scratch/chicoma-cpu</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/usr/projects/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/usr/projects/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/input_data/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/usr/projects/e3sm/software/chicoma-cpu/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $SHELL{echo 256/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;}</arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <!-- does not exist -->
      <init_path lang="python">/usr/share/lmod/lmod/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">gcc</command>
        <command name="unload">gcc-native</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc-native/12.3</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">PrgEnv-nvidia/8.5.0</command>
        <command name="load">nvidia/24.7</command>
        <command name="load">cray-libsci/23.12.5</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.5.0</command>
        <command name="load">intel/2023.2.0</command>
      </modules>

      <modules compiler="amdclang">
        <command name="load">PrgEnv-aocc/8.4.0</command>
        <command name="load">aocc/3.2.0</command>
        <command name="load">cray-libsci/23.05.1.4</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.30</command>
        <command name="load">cray-mpich/8.1.28</command>
        <command name="load">cray-hdf5-parallel/1.12.2.9</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.9</command>
        <command name="load">cray-parallel-netcdf/1.12.3.9</command>
        <command name="load">cmake/3.27.7</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/usr/projects/climate/SHARED_CLIMATE/software/chicoma-cpu/perl5-only-switch/lib/perl5</env>
      <env name="PNETCDF_HINTS">romio_ds_write=disable;romio_ds_read=disable;romio_cb_write=enable;romio_cb_read=enable</env>
      <env name="FI_CXI_RX_MATCH_MODE">software</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="LD_LIBRARY_PATH">/usr/lib64/gcc/x86_64-suse-linux/12:$ENV{LD_LIBRARY_PATH}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="chicoma-gpu">
    <DESC>Chicoma GPU nodes at LANL IC. Each GPU node has single AMD EPYC 7713 64-Core (Milan) (256GB) and 4 nvidia A100'</DESC>
    <NODENAME_REGEX>ch-fe*</NODENAME_REGEX>
    <OS>Linux</OS>
    <COMPILERS>gnugpu,gnu,nvidiagpu,nvidia</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CIME_OUTPUT_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/scratch/chicoma-gpu</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/usr/projects/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/usr/projects/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/lustre/scratch5/$ENV{USER}/E3SM/input_data/ccsm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/usr/projects/e3sm/software/chicoma-cpu/cprnc</CCSM_CPRNC>
    <GMAKE_J>10</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="gnu">256</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="nvidia">256</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnu">64</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="nvidia">64</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
        <arg name="label"> --label</arg>
        <arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }}</arg>
        <arg name="thread_count">-c $ENV{OMP_NUM_THREADS}</arg>
        <arg name="binding"> $SHELL{if [ 128 -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;}</arg>
        <arg name="placement"> -m plane=$SHELL{echo `./xmlquery --value MAX_MPITASKS_PER_NODE`}</arg>
    </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl</init_path>
      <!-- does not exist -->
      <init_path lang="python">/usr/share/lmod/lmod/init/python</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
        <command name="unload">cray-hdf5-parallel</command>
        <command name="unload">cray-netcdf-hdf5parallel</command>
        <command name="unload">cray-parallel-netcdf</command>
        <command name="unload">cray-netcdf</command>
        <command name="unload">cray-hdf5</command>
        <command name="unload">intel</command>
        <command name="unload">intel-oneapi</command>
        <command name="unload">nvidia</command>
        <command name="unload">aocc</command>
        <command name="unload">cudatoolkit</command>
        <command name="unload">climate-utils</command>
        <command name="unload">cray-libsci</command>
        <command name="unload">craype-accel-nvidia80</command>
        <command name="unload">craype-accel-host</command>
        <command name="unload">perftools-base</command>
        <command name="unload">perftools</command>
        <command name="unload">darshan</command>
        <command name="unload">PrgEnv-gnu</command>
        <command name="unload">PrgEnv-intel</command>
        <command name="unload">PrgEnv-nvidia</command>
        <command name="unload">PrgEnv-cray</command>
        <command name="unload">PrgEnv-aocc</command>
      </modules>

      <modules compiler="gnu.*">
        <command name="load">PrgEnv-gnu/8.5.0</command>
        <command name="load">gcc/12.2.0</command>
        <command name="load">cray-libsci/23.05.1.4</command>
      </modules>

      <modules compiler="nvidia.*">
        <command name="load">PrgEnv-nvidia/8.4.0</command>
        <command name="load">nvidia/22.7</command>
        <command name="load">cray-libsci/23.05.1.4</command>
      </modules>

      <modules compiler="gnugpu">
        <command name="load">cudatoolkit/22.7_11.7</command>
        <command name="load">craype-accel-nvidia80</command>
      </modules>

      <modules compiler="nvidiagpu">
        <command name="load">cudatoolkit/22.7_11.7</command>
        <command name="load">craype-accel-nvidia80</command>
        <command name="load">gcc-mixed/11.2.0</command>
      </modules>

      <modules compiler="gnu">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules compiler="nvidia">
        <command name="load">craype-accel-host</command>
      </modules>

      <modules>
        <command name="load">craype-accel-host</command>
        <command name="load">craype/2.7.21</command>
        <command name="load">cray-mpich/8.1.26</command>
        <command name="load">cray-hdf5-parallel/1.12.2.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.9.0.3</command>
        <command name="load">cray-parallel-netcdf/1.12.3.3</command>
        <command name="load">cmake/3.27.7</command>
      </modules>
    </module_system>

    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>

    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="PERL5LIB">/usr/projects/climate/SHARED_CLIMATE/software/chicoma-cpu/perl5-only-switch/lib/perl5</env>
      <env name="PNETCDF_HINTS">romio_ds_write=disable;romio_ds_read=disable;romio_cb_write=enable;romio_cb_read=enable</env>
      <env name="FI_CXI_RX_MATCH_MODE">software</env>
      <env name="MPICH_COLL_SYNC">MPI_Bcast</env>
      <env name="NETCDF_PATH">$ENV{CRAY_NETCDF_HDF5PARALLEL_PREFIX}</env>
      <env name="PNETCDF_PATH">$ENV{CRAY_PARALLEL_NETCDF_PREFIX}</env>
      <env name="PKG_CONFIG_PATH">/usr/projects/e3sm/cudatoolkit:$ENV{PKG_CONFIG_PATH}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="LD_LIBRARY_PATH">/opt/cray/pe/gcc/12.2.0/snos/lib64:$ENV{LD_LIBRARY_PATH}</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="mesabi">
    <DESC>Mesabi batch queue</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/reichpb/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/reichpb/shared/cesm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/reichpb/shared/cesm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>USERDEFINED_optional_run</DOUT_S_ROOT>
    <BASELINE_ROOT>USERDEFINED_optional_run/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>USERDEFINED_optional_test</CCSM_CPRNC>
    <GMAKE_J>2</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>chen1718 at umn dot edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>24</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
        <arg name="num_tasks"> -n {{ total_tasks }}</arg>
        <arg name="tasks_per_numa"> -S {{ tasks_per_numa }}</arg>
        <arg name="tasks_per_node"> -N $MAX_MPITASKS_PER_NODE</arg>
        <arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$CASEROOT/run</RUNDIR>
    <!-- complete path to the run directory -->
    <EXEROOT>$CASEROOT/exedir</EXEROOT>
    <!-- complete path to the build directory -->
    <!-- complete path to the inputdata directory -->

    <!-- path to the optional forcing data for CLM (for CRUNCEP forcing) -->
    <!--<DOUT_S>FALSE</DOUT_S>-->
    <!-- logical for short term archiving -->
    <!-- complete path to a short term archiving directory -->
    <!-- complete path to a long term archiving directory -->
    <!-- where the cesm testing scripts write and read baseline results -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
  </machine>

  <machine MACH="itasca">
    <DESC>Itasca batch queue</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/reichpb/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/reichpb/shared/cesm_inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/reichpb/shared/cesm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>USERDEFINED_optional_run</DOUT_S_ROOT>
    <BASELINE_ROOT>USERDEFINED_optional_run/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>USERDEFINED_optional_test</CCSM_CPRNC>
    <GMAKE_J>2</GMAKE_J>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>chen1718 at umn dot edu</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
        <arg name="num_tasks"> -n {{ total_tasks }}</arg>
        <arg name="tasks_per_numa"> -S {{ tasks_per_numa }}</arg>
        <arg name="tasks_per_node"> -N $MAX_MPITASKS_PER_NODE</arg>
        <arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <RUNDIR>$CASEROOT/run</RUNDIR>
    <!-- complete path to the run directory -->
    <EXEROOT>$CASEROOT/exedir</EXEROOT>
    <!-- complete path to the build directory -->
    <!-- complete path to the inputdata directory -->

    <!-- path to the optional forcing data for CLM (for CRUNCEP forcing) -->
    <!--<DOUT_S>FALSE</DOUT_S>-->
    <!-- logical for short term archiving -->
    <!-- complete path to a short term archiving directory -->
    <!-- complete path to a long term archiving directory -->
    <!-- where the cesm testing scripts write and read baseline results -->
    <!-- path to the cprnc tool used to compare netcdf history files in testing -->
    <environment_variables>
      <env name="NETCDF_PATH">/soft/netcdf/fortran-4.4-intel-sp1-update3-parallel/lib</env>
    </environment_variables>
  </machine>

  <machine MACH="lawrencium-lr3">
    <DESC>Lawrencium LR3 cluster at LBL, OS is Linux (intel), batch system is SLURM</DESC>
    <NODENAME_REGEX>n000*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CHARGE_ACCOUNT>ac_acme</CHARGE_ACCOUNT>
    <CIME_OUTPUT_ROOT>/global/scratch/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/scratch/$ENV{USER}/cesm_input_datasets/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/scratch/$ENV{USER}/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/$CIME_OUTPUT_ROOT/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>rgknox and glemieux at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>8</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-np {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-np {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="perl">/usr/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/Modules/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="perl">/usr/Modules/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/Modules/bin/modulecmd python</cmd_path>
      <modules>
        <command name="purge"/>
	     <command name="load">cmake/3.15.0</command>
        <command name="load">perl</command>
	     <command name="load">xml-libxml/2.0116</command>
	     <command name="load">python/3.6</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/2016.4.072</command>
        <command name="load">mkl</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
        <command name="load">netcdf/4.4.1.1-intel-s</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial">
        <command name="load">openmpi</command>
        <command name="load">netcdf/4.4.1.1-intel-p</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/6.3.0</command>
        <command name="load">lapack/3.8.0-gcc</command>
      </modules>
      <modules compiler="gnu" mpilib="mpi-serial">
        <command name="load">netcdf/5.4.1.1-gcc-s</command>
        <command name="unload">openmpi/2.0.2-gcc</command>
      </modules>
      <modules compiler="gnu" mpilib="!mpi-serial">
        <command name="load">openmpi/3.0.1-gcc</command>
        <command name="load">netcdf/4.4.1.1-gcc-p</command>
        <command name="unload">openmpi/2.0.2-gcc</command>
      </modules>

    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>

    <environment_variables>
      <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env>
      <env name="PNETCDF_PATH">$ENV{PNETCDF_DIR}</env>
      <env name="LAPACK_ROOT">/global/software/sl-6.x86_64/modules/intel/2016.1.150/lapack/3.6.0-intel</env>
    </environment_variables>
  </machine>

  <machine MACH="lawrencium-lr6">
    <DESC>Lawrencium LR6 cluster at LBL, OS is Linux (intel), batch system is SLURM</DESC>
    <NODENAME_REGEX>n000*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>intel,gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CHARGE_ACCOUNT>ac_acme</CHARGE_ACCOUNT>
    <CIME_OUTPUT_ROOT>/global/scratch/$ENV{USER}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/global/scratch/$ENV{USER}/cesm_input_datasets/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/global/scratch/$ENV{USER}/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/$CIME_OUTPUT_ROOT/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>4</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>rgknox and glemieux at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
      <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
      <init_path lang="perl">/usr/Modules/init/perl.pm</init_path>
      <init_path lang="python">/usr/Modules/python.py</init_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="perl">/usr/Modules/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/usr/Modules/bin/modulecmd python</cmd_path>
      <modules>
	<command name="purge"/>
        <command name="load">cmake/3.15.0</command>
        <command name="load">perl</command>
	<command name="load">xml-libxml/2.0116</command>
        <command name="load">python/3.6</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/2016.4.072</command>
        <command name="load">mkl</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
        <command name="load">netcdf/4.4.1.1-intel-s</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial">
        <command name="load">openmpi</command>
        <command name="load">netcdf/4.4.1.1-intel-p</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/6.3.0</command>
        <command name="load">lapack/3.8.0-gcc</command>
      </modules>
      <modules compiler="gnu" mpilib="mpi-serial">
        <command name="load">netcdf/5.4.1.1-gcc-s</command>
        <command name="unload">openmpi/2.0.2-gcc</command>
      </modules>
      <modules compiler="gnu" mpilib="!mpi-serial">
        <command name="load">openmpi/3.0.1-gcc</command>
        <command name="load">netcdf/4.4.1.1-gcc-p</command>
        <command name="unload">openmpi/2.0.2-gcc</command>
      </modules>

    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>

    <environment_variables>
      <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env>
      <env name="PNETCDF_PATH">$ENV{PNETCDF_DIR}</env>
      <env name="LAPACK_ROOT">/global/software/sl-6.x86_64/modules/intel/2016.1.150/lapack/3.6.0-intel</env>
    </environment_variables>
  </machine>

  <machine MACH="lobata">
    <DESC>FATES development machine at LBNL, System76 Thelio Massive Workstation Pop!_OS 20.04</DESC>
    <NODENAME_REGEX>lobata</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>$ENV{HOME}/scratch/</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/data/cesmdataroot/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/data/cesmdataroot/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{HOME}/scratch/ctsm-baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/home/glemieux/Repos/cime/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE>make</GMAKE>
    <GMAKE_J>16</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>glemieux at lbl dot gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>4</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> --map-by ppr:{{ tasks_per_node }}:socket:PE=$ENV{OMP_NUM_THREADS} --bind-to hwthread</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">hdf5</command>
        <command name="load">netcdf-c</command>
        <command name="load">netcdf-fortran</command>
        <command name="load">esmf</command>
      </modules>
    </module_system>
</machine>

  <machine MACH="eddi">
    <DESC>small developer workhorse at lbl climate sciences</DESC>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <PROJECT>ngeet</PROJECT>
    <CIME_OUTPUT_ROOT>/raid1/lbleco/e3sm/</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/rgknox/Models/InputDatasets/cesm_input_data/</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/rgknox/Models/InputDatasets/cesm_input_data/atm/datm7/</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/home/rgknox/Models//cesm_archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/rgknox/Models//cesm_baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/raid1/lbleco/cesm/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>1</GMAKE_J>
    <BATCH_SYSTEM>none</BATCH_SYSTEM>
    <SUPPORTED_BY>rgknox at lbl gov</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>16</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable/>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-np {{ total_tasks }}</arg>
        <arg name="tasks_per_node"> -npernode $MAX_MPITASKS_PER_NODE</arg>
      </arguments>
    </mpirun>
    <module_system type="none"/>
    <environment_variables compiler="gnu" >
      <env name="CMAKE_ROOT">/usr/local/share/cmake-3.21/</env>
      <env name="NETCDF_PATH">$ENV{NETCDF_HOME}</env>
    </environment_variables>
  </machine>

  <machine MACH="summit">
    <DESC>ORNL Summit. Node: 2x POWER9 + 6x Volta V100, 22 cores/socket, 4 HW threads/core.</DESC>
    <NODENAME_REGEX>.*summit.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>ibm,ibmgpu,pgi,pgigpu,gnu,gnugpu</COMPILERS>
    <MPILIBS>spectrum-mpi</MPILIBS>
    <PROJECT>cli115</PROJECT>
    <CHARGE_ACCOUNT>cli115</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/gpfs/alpine/proj-shared/cli115</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/gpfs/alpine2/$PROJECT/proj-shared/$ENV{USER}/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/gpfs/alpine2/atm146/world-shared/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/gpfs/alpine2/atm146/world-shared/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/gpfs/alpine/$PROJECT/proj-shared/$ENV{USER}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/gpfs/alpine2/atm146/world-shared/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/gpfs/alpine2/atm146/world-shared/e3sm/tools/cprnc.summit/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>84</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="pgigpu">18</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="gnugpu">42</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="ibmgpu">42</MAX_TASKS_PER_NODE>
    <!-- Need to activate following attribute after CIME update from upstream -->
    <!-- <MAX_GPUS_PER_NODE>6</MAX_GPUS_PER_NODE> -->
    <MAX_MPITASKS_PER_NODE>84</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="pgigpu">18</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnugpu">42</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="ibmgpu">42</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="spectrum-mpi">
      <executable>jsrun</executable>
      <arguments>
        <arg name="exit_on_error">-X 1</arg>
        <arg name="num_rs">$SHELL{if [ {{ total_tasks }} -eq 1 ];then echo --nrs 1 --rs_per_host 1;else echo --nrs $NUM_RS --rs_per_host $RS_PER_NODE;fi}</arg>
        <arg name="tasks_per_rs">--tasks_per_rs $SHELL{echo "({{ tasks_per_node }} + $RS_PER_NODE - 1)/$RS_PER_NODE"|bc}</arg>
        <arg name="distribute">-d plane:$SHELL{echo "({{ tasks_per_node }} + $RS_PER_NODE - 1)/$RS_PER_NODE"|bc}</arg>
        <arg name="cpu_per_rs">--cpu_per_rs $ENV{CPU_PER_RS}</arg>
        <arg name="gpu_per_rs">--gpu_per_rs $ENV{GPU_PER_RS}</arg>
        <arg name="task_bind">--bind packed:smt:$ENV{OMP_NUM_THREADS}</arg>
        <arg name="latency_priority">--latency_priority $ENV{LTC_PRT}</arg>
        <arg name="stdio_mode">--stdio_mode prepended</arg>
        <arg name="thread_vars">$ENV{JSRUN_THREAD_VARS}</arg>
        <arg name="smpiargs">$ENV{SMPIARGS}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/sw/summit/lmod/8.4/init/sh</init_path>
      <init_path lang="csh">/sw/summit/lmod/8.4/init/csh</init_path>
      <init_path lang="python">/sw/summit/lmod/8.4/init/env_modules_python.py</init_path>
      <init_path lang="perl">/sw/summit/lmod/8.4/init/perl</init_path>
      <cmd_path lang="perl">module</cmd_path>
      <cmd_path lang="python">/sw/summit/lmod/8.4/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">DefApps-2023</command>
        <command name="load">python/3.7-anaconda3</command>
        <command name="load">subversion/1.14.0</command>
        <command name="load">git/2.31.1</command>
        <command name="load">cmake/3.20.2</command>
        <command name="load">essl/6.3.0</command>
        <command name="load">netlib-lapack/3.8.0</command>
      </modules>
      <modules compiler="pgi.*">
        <command name="load">nvhpc/21.11</command>
      </modules>
      <modules compiler="ibm.*">
        <command name="load">xl/16.1.1-10</command>
      </modules>
      <modules compiler="gnu.*">
        <command name="load">gcc/9.1.0</command>
      </modules>
      <modules compiler="pgigpu">
        <command name="load">cuda/10.1.243</command>
      </modules>
      <modules compiler="gnugpu">
        <command name="load">cuda/11.0.3</command>
      </modules>
      <modules compiler="ibmgpu">
        <command name="load">cuda/10.1.243</command>
      </modules>
      <modules>
        <command name="load">spectrum-mpi/10.4.0.3-20210112</command>
        <command name="load">hdf5/1.10.7</command>
        <command name="load">netcdf-c/4.8.0</command>
        <command name="load">netcdf-fortran/4.4.5</command>
        <command name="load">parallel-netcdf/1.12.2</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="NETCDF_C_PATH">$ENV{OLCF_NETCDF_C_ROOT}</env>
      <env name="NETCDF_FORTRAN_PATH">$ENV{OLCF_NETCDF_FORTRAN_ROOT}</env>
      <env name="LAPACK_ROOT">$ENV{OLCF_NETLIB_LAPACK_ROOT}</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="ESSL_PATH">$ENV{OLCF_ESSL_ROOT}</env>
      <env name="HDF5_ROOT">$ENV{OLCF_HDF5_ROOT}</env>
      <env name="HDF5_USE_STATIC_LIBRARIES">True</env>
      <env name="PNETCDF_PATH">$ENV{OLCF_PARALLEL_NETCDF_ROOT}</env>
      <env name="PGI_ACC_POOL_ALLOC">0</env>
      <env name="SMPIARGS"> </env>
      <env name="CRAYPE_VERSION">True</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="FALSE">
      <env name="JSRUN_THREAD_VARS"> </env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="JSRUN_THREAD_VARS">-E OMP_NUM_THREADS=$ENV{OMP_NUM_THREADS} -E OMP_PROC_BIND=spread -E OMP_PLACES=threads -E OMP_STACKSIZE=256M</env>
    </environment_variables>
    <environment_variables compiler=".*gpu.*">
      <env name="SMPIARGS">--smpiargs="-gpu"</env>
    </environment_variables>
    <environment_variables compiler="ibm">
      <env name="RS_PER_NODE">2</env>
      <env name="CPU_PER_RS">21</env>
      <env name="GPU_PER_RS">0</env>
      <env name="LTC_PRT">cpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "2*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="SMT_MODE">$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="RS_PER_NODE">2</env>
      <env name="CPU_PER_RS">21</env>
      <env name="GPU_PER_RS">0</env>
      <env name="LTC_PRT">cpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "2*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="SMT_MODE">$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</env>
      <env name="PAMI_ENABLE_STRIPING">1</env>
      <env name="PAMI_IBV_ADAPTER_AFFINITY">1</env>
      <env name="PAMI_IBV_DEVICE_NAME_1">mlx5_3:1,mlx5_0:1</env>
      <env name="PAMI_IBV_DEVICE_NAME">mlx5_0:1,mlx5_3:1</env>
    </environment_variables>
    <environment_variables compiler="pgi">
      <env name="RS_PER_NODE">2</env>
      <env name="CPU_PER_RS">21</env>
      <env name="GPU_PER_RS">0</env>
      <env name="LTC_PRT">cpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "2*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="SMT_MODE">$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</env>
    </environment_variables>
    <environment_variables compiler="ibmgpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">7</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
    </environment_variables>
    <environment_variables compiler="pgigpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">3</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">7</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="PAMI_ENABLE_STRIPING">1</env>
      <env name="PAMI_IBV_ADAPTER_AFFINITY">1</env>
      <env name="PAMI_IBV_DEVICE_NAME_1">mlx5_3:1,mlx5_0:1</env>
      <env name="PAMI_IBV_DEVICE_NAME">mlx5_0:1,mlx5_3:1</env>
    </environment_variables>
  </machine>

  <machine MACH="ascent">
    <DESC>ORNL Ascent. Node: 2x POWER9 + 6x Volta V100, 22 cores/socket, 4 HW threads/core.</DESC>
    <NODENAME_REGEX>.*ascent.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>ibm,ibmgpu,pgi,pgigpu,gnu,gnugpu</COMPILERS>
    <MPILIBS>spectrum-mpi</MPILIBS>
    <PROJECT>cli115</PROJECT>
    <CHARGE_ACCOUNT>cli115</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/gpfs/wolf/proj-shared/$PROJECT</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>cli115</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/gpfs/wolf/$PROJECT/proj-shared/$ENV{USER}/e3sm_scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/gpfs/wolf/cli115/world-shared/e3sm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/gpfs/wolf/cli115/world-shared/e3sm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/gpfs/wolf/$PROJECT/proj-shared/$ENV{USER}/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/gpfs/wolf/cli115/world-shared/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/gpfs/wolf/cli115/world-shared/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>lsf</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>84</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="pgigpu">18</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="gnugpu">42</MAX_TASKS_PER_NODE>
    <MAX_TASKS_PER_NODE compiler="ibmgpu">42</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>84</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="pgigpu">18</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="gnugpu">42</MAX_MPITASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE compiler="ibmgpu">42</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="spectrum-mpi">
      <executable>jsrun</executable>
      <arguments>
        <arg name="exit_on_error">-X 1</arg>
        <arg name="num_rs">$SHELL{if [ {{ total_tasks }} -eq 1 ];then echo --nrs 1 --rs_per_host 1;else echo --nrs $NUM_RS --rs_per_host $RS_PER_NODE;fi}</arg>
        <arg name="tasks_per_rs">--tasks_per_rs $SHELL{echo "({{ tasks_per_node }} + $RS_PER_NODE - 1)/$RS_PER_NODE"|bc}</arg>
        <arg name="distribute">-d plane:$SHELL{echo "({{ tasks_per_node }} + $RS_PER_NODE - 1)/$RS_PER_NODE"|bc}</arg>
        <arg name="cpu_per_rs">--cpu_per_rs $ENV{CPU_PER_RS}</arg>
        <arg name="gpu_per_rs">--gpu_per_rs $ENV{GPU_PER_RS}</arg>
        <arg name="task_bind">--bind packed:smt:$ENV{OMP_NUM_THREADS}</arg>
        <arg name="latency_priority">--latency_priority $ENV{LTC_PRT}</arg>
        <arg name="stdio_mode">--stdio_mode prepended</arg>
        <arg name="thread_vars">$ENV{JSRUN_THREAD_VARS}</arg>
        <arg name="smpiargs">$ENV{SMPIARGS}</arg>
      </arguments>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="sh">/sw/ascent/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/sw/ascent/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/sw/ascent/lmod/init/env_modules_python.py</init_path>
      <cmd_path lang="python">/sw/ascent/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">DefApps</command>
        <command name="load">python/3.8-anaconda3</command>
        <command name="load">git/2.31.1</command>
        <command name="load">subversion</command>
        <command name="load">cmake/3.22.2</command>
        <command name="load">essl/6.3.0</command>
        <command name="load">netlib-lapack/3.9.1</command>
      </modules>
      <modules compiler="pgi.*">
        <command name="load">nvhpc/21.11</command>
      </modules>
      <modules compiler="ibmgpu">
        <command name="load">cuda/10.1.243</command>
      </modules>
      <modules compiler="ibm.*">
        <command name="load">xl/16.1.1-10</command>
      </modules>
      <modules compiler="gnugpu">
        <command name="load">cuda/11.0.3</command>
      </modules>
      <modules compiler="gnu.*">
        <command name="load">gcc/9.1.0</command>
      </modules>
      <modules>
        <command name="load">spectrum-mpi/10.4.0.3-20210112</command>
        <command name="load">hdf5/1.10.7</command>
        <command name="load">netcdf-c/4.8.1</command>
        <command name="load">netcdf-fortran/4.4.5</command>
        <command name="load">parallel-netcdf/1.12.2</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>9000</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="PATH">/gpfs/wolf/cli115/world-shared/e3sm/soft/perl/5.26.0/bin:$ENV{PATH}</env>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
      <env name="LAPACK_ROOT">$ENV{OLCF_NETLIB_LAPACK_ROOT}</env>
      <env name="BLA_VENDOR">Generic</env>
      <env name="ESSL_PATH">$ENV{OLCF_ESSL_ROOT}</env>
      <env name="HDF5_ROOT">$ENV{OLCF_HDF5_ROOT}</env>
      <env name="HDF5_USE_STATIC_LIBRARIES">True</env>
      <env name="PGI_ACC_POOL_ALLOC">0</env>
      <env name="SMPIARGS"> </env>
    </environment_variables>
    <environment_variables BUILD_THREADED="FALSE">
      <env name="JSRUN_THREAD_VARS"> </env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="JSRUN_THREAD_VARS">-E OMP_NUM_THREADS=$ENV{OMP_NUM_THREADS} -E OMP_PROC_BIND=spread -E OMP_PLACES=threads -E OMP_STACKSIZE=256M</env>
    </environment_variables>
    <environment_variables compiler=".*gpu.*">
      <env name="SMPIARGS">--smpiargs="-gpu"</env>
    </environment_variables>
    <environment_variables>
      <env name="RS_PER_NODE">2</env>
      <env name="CPU_PER_RS">21</env>
      <env name="GPU_PER_RS">0</env>
      <env name="LTC_PRT">cpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "2*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="SMT_MODE">$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</env>
    </environment_variables>
    <environment_variables compiler="ibmgpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">7</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
    </environment_variables>
    <environment_variables compiler="pgigpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">3</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
      <env name="NVCC_WRAPPER_DEFAULT_COMPILER">pgc++</env>
    </environment_variables>
    <environment_variables compiler="gnugpu">
      <env name="RS_PER_NODE">6</env>
      <env name="CPU_PER_RS">7</env>
      <env name="GPU_PER_RS">1</env>
      <env name="LTC_PRT">gpu-cpu</env>
      <env name="NUM_RS">$SHELL{echo "6*((`./xmlquery --value TOTAL_TASKS` + `./xmlquery --value TASKS_PER_NODE` - 1)/`./xmlquery --value TASKS_PER_NODE`)"|bc}</env>
    </environment_variables>
  </machine>

  <machine MACH="modex">
      <DESC>Medium sized linux cluster at BNL, torque scheduler.</DESC>
      <OS>LINUX</OS>
      <COMPILERS>gnu</COMPILERS>
      <MPILIBS>openmpi,mpi-serial</MPILIBS>
      <CIME_OUTPUT_ROOT>/data/$ENV{USER}</CIME_OUTPUT_ROOT>
      <DIN_LOC_ROOT>/data/Model_Data/cesm_input_datasets/</DIN_LOC_ROOT>
      <DIN_LOC_ROOT_CLMFORC>/data/Model_Data/cesm_input_datasets/atm/datm7</DIN_LOC_ROOT_CLMFORC>
      <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/cesm_archive/$CASE</DOUT_S_ROOT>
      <BASELINE_ROOT>$CIME_OUTPUT_ROOT/cesm_baselines</BASELINE_ROOT>
      <CCSM_CPRNC>/data/software/cesm_tools/cprnc/cprnc</CCSM_CPRNC>
      <GMAKE_J>4</GMAKE_J>
      <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
      <SUPPORTED_BY>sserbin@bnl.gov</SUPPORTED_BY>
      <MAX_TASKS_PER_NODE>12</MAX_TASKS_PER_NODE>
      <MAX_MPITASKS_PER_NODE>12</MAX_MPITASKS_PER_NODE>
      <COSTPES_PER_NODE>12</COSTPES_PER_NODE>
      <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
      <mpirun mpilib="mpi-serial">
      		<executable></executable>
      </mpirun>
      <mpirun mpilib="default">
          <executable>mpirun</executable>
          <arguments>
              <arg name="num_tasks">-np {{ total_tasks }}</arg>
              <arg name="tasks_per_node">-npernode $MAX_TASKS_PER_NODE</arg>
          </arguments>
      </mpirun>
      <module_system type="module">
          <init_path lang="sh">/etc/profile.d/modules.sh</init_path>
          <init_path lang="csh">/etc/profile.d/modules.csh</init_path>
          <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
          <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
          <cmd_path lang="sh">module</cmd_path>
          <cmd_path lang="csh">module</cmd_path>
          <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
          <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
          <modules>
              <command name="purge"/>
              <command name="load">perl/5.22.1</command>
              <command name="load">libxml2/2.9.2</command>
              <command name="load">maui/3.3.1</command>
              <command name="load">python/2.7.15</command>
              <command name="load">python/3.6.2</command>
          </modules>
          <modules compiler="gnu">
              <command name="load">gcc/5.4.0</command>
              <command name="load">gfortran/5.4.0</command>
              <command name="load">hdf5/1.8.19fates</command>
              <command name="load">netcdf/4.4.1.1-gnu540-fates</command>
              <command name="load">openmpi/2.1.1-gnu540</command>
          </modules>
          <modules compiler="gnu" mpilib="!mpi-serial">
              <command name="load">openmpi/2.1.1-gnu540</command>
          </modules>
       </module_system>
       <environment_variables>
         <env name="HDF5_HOME">/data/software/hdf5/1.8.19fates</env>
         <env name="NETCDF_PATH">/data/software/netcdf/4.4.1.1-gnu540-fates</env>
       </environment_variables>
  </machine>

  <machine MACH="tulip">
    <DESC>ORNL experimental/evaluation cluster</DESC>
    <NODENAME_REGEX>tulip.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/groups/coegroup/e3sm/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/groups/coegroup/e3sm/inputdata2</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/groups/coegroup/e3sm/inputdata2/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/groups/coegroup/e3sm/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/home/groups/coegroup/e3sm/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">--tag-output -n {{ total_tasks }} </arg>
        <arg name="tasks_per_node"> --map-by ppr:1:core:PE=$ENV{OMP_NUM_THREADS} --bind-to core </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/cm/local/apps/environment-modules/current/init/python</init_path>
      <init_path lang="sh">/cm/local/apps/environment-modules/current/init/sh</init_path>
      <init_path lang="csh">/cm/local/apps/environment-modules/current/init/csh</init_path>
      <cmd_path lang="python">/cm/local/apps/environment-modules/current/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="rm">gcc</command>
        <command name="rm">cce</command>
        <command name="rm">PrgEnv-cray</command>
        <command name="rm">cray-mvapich2</command>
        <command name="load">cmake/3.17.0</command>
        <command name="use">/home/users/twhite/share/modulefiles</command>
        <command name="load">svn/1.10.6</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gcc/8.1.0</command>
        <command name="load">blas/gcc/64/3.8.0</command>
        <command name="load">lapack/gcc/64/3.8.0</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <environment_variables>
      <env name="PERL5LIB">/home/groups/coegroup/e3sm/soft/perl5/lib/perl5</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="NETCDF_PATH">/home/groups/coegroup/e3sm/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/gcc/8.2.0</env>
    </environment_variables>
    <environment_variables compiler="gnu" mpilib="openmpi">
      <env name="OMPI_CC">gcc</env>
      <env name="OMPI_CXX">g++</env>
      <env name="OMPI_FC">gfortran</env>
      <env name="PATH">/home/groups/coegroup/e3sm/soft/openmpi/2.1.6/gcc/8.2.0/bin:$ENV{PATH}</env>
      <env name="LD_LIBRARY_PATH">/home/groups/coegroup/e3sm/soft/openmpi/2.1.6/gcc/8.2.0/lib:/home/groups/coegroup/e3sm/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/gcc/8.2.0/lib:$ENV{LD_LIBRARY_PATH}</env>
      <env name="PNETCDF_PATH">/home/groups/coegroup/e3sm/soft/pnetcdf/1.12.1/gcc/8.2.0/openmpi/2.1.6</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="gcp12">
    <DESC>Google Cloud cluster using compute nodes c2d-compute-112's gcpe3sm12</DESC>
    <NODENAME_REGEX>gcpe3sm12*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/$USER/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/home/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>20</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>8</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>112</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>56</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>srun</executable>
      <arguments>
	<arg name="pmi_layer"> --mpi=pmi2</arg>
	<arg name="label"> --label</arg>
	<arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit</arg>
	<arg name="thread_count">-c $SHELL{echo `./xmlquery --value MAX_TASKS_PER_NODE`/ {{ tasks_per_node }} |bc}</arg>
	<arg name="binding"> $SHELL{if [ `./xmlquery --value MAX_TASKS_PER_NODE` -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
	<arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>

    <module_system type="module" allow_error="true">
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>

      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
	<command name="use">/opt/apps/spack/share/spack/modules/linux-centos7-zen2</command>
	<command name="use">/opt/apps/spack/share/spack/modules/linux-centos7-x86_64_v3</command>
	<command name="unload">gcc</command>
	<command name="unload">openmpi</command>
	<command name="unload">binutils</command>
	<command name="unload">netlib-lapack</command>
	<command name="unload">openblas</command>
	<command name="unload">hdf5</command>
	<command name="unload">netcdf-c</command>
	<command name="unload">parallel-netcdf</command>
      </modules>

      <modules compiler="gnu">
	<command name="load">gcc/12.2.0</command>
      </modules>

      <modules mpilib="openmpi">
	<command name="load">openmpi/4.1.4</command>
      </modules>

      <modules compiler="gnu">
	<command name="load">python</command>
	<command name="load">cmake</command>
	<command name="load">perl</command>
	<command name="load">perl-xml-libxml</command>
	<command name="load">netcdf-c</command>
	<command name="load">netcdf-cxx</command>
	<command name="load">netcdf-fortran</command>
	<command name="load">parallel-netcdf</command>
	<command name="load">hdf5</command>
	<command name="load">netlib-lapack</command>
	<command name="load">openblas</command>
      </modules>

    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.2</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>
    <environment_variables compiler="gnu">
      <env name="HDF5_ROOT">$SHELL{dirname $(dirname $(which h5diff))}</env>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf-config))}</env>
      <env name="OPENBLAS_PATH">/opt/apps/spack/opt/spack/linux-centos7-zen2/gcc-12.2.0/openblas-0.3.21-z66r7lyxwkhsshgreexm4cedffp73scp</env>
      <env name="LAPACK_ROOT">/opt/apps/spack/opt/spack/linux-centos7-zen2/gcc-12.2.0/netlib-lapack-3.10.1-lkhddpuidlw2z74g5ui6eq5iattsfjxp</env>
      <env name="PERL5LIB">$ENV{PERL5LIB}:/opt/apps/spack/opt/spack/linux-centos7-zen2/gcc-12.2.0/perl-5.36.0-sly2pft2edg2p3iyijfyy6dzntusokno/lib/site_perl/5.36.0</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
    </environment_variables>

    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="gcp10">
    <DESC>Google Cloud cluster with c2-compute-60's gcp-e3sm10</DESC>
    <NODENAME_REGEX>gcp-e3sm10*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/home/$USER/e3sm/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/home/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/home/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/home/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/home/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>16</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>60</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>30</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="openmpi">
      <executable>srun</executable>
      <arguments>
	<arg name="pmi_layer"> --mpi=pmi2</arg>
	<arg name="label"> --label</arg>
	<arg name="num_tasks"> -n {{ total_tasks }} -N {{ num_nodes }} --kill-on-bad-exit</arg>
	<arg name="thread_count">-c $SHELL{echo `./xmlquery --value MAX_TASKS_PER_NODE`/ {{ tasks_per_node }} |bc}</arg>
	<arg name="binding"> $SHELL{if [ `./xmlquery --value MAX_TASKS_PER_NODE` -ge `./xmlquery --value MAX_MPITASKS_PER_NODE` ]; then echo "--cpu_bind=cores"; else echo "--cpu_bind=threads";fi;} </arg>
	<arg name="placement">-m plane={{ tasks_per_node }}</arg>
      </arguments>
    </mpirun>

    <module_system type="module">
      <init_path lang="python">/usr/share/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>

      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>

      <modules>
	<command name="use">/apps/spack/share/spack/modules/linux-centos7-cascadelake</command>
	<command name="unload">gcc</command>
	<command name="unload">openmpi</command>
      </modules>

      <modules compiler="gnu">
	<command name="load">gcc/12.2.0</command>
      </modules>

      <modules mpilib="openmpi">
	<command name="load">openmpi-gcc@12.2.0</command>
      </modules>

      <modules compiler="gnu">
	<command name="load">cmake</command>
	<command name="load">perl</command>
	<command name="load">perl-xml-libxml</command>
	<command name="load">netcdf-c-gcc@12.2.0</command>
	<command name="load">netcdf-cxx-gcc@12.2.0</command>
	<command name="load">netcdf-fortran-gcc@12.2.0</command>
	<command name="load">parallel-netcdf-gcc@12.2.0</command>
	<command name="load">hdf5-gcc@12.2.0</command>
	<command name="load">netlib-lapack-gcc@12.2.0</command>
	<command name="load">openblas-gcc@12.2.0</command>
      </modules>

    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.2</TEST_TPUT_TOLERANCE>
    <TEST_MEMLEAK_TOLERANCE>0.20</TEST_MEMLEAK_TOLERANCE>
    <environment_variables compiler="gnu">
      <env name="HDF5_ROOT">$SHELL{dirname $(dirname $(which h5diff))}</env>
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf-config))}</env>
      <env name="OPENBLAS_PATH">/apps/spack/opt/spack/linux-centos7-cascadelake/gcc-12.2.0/openblas-0.3.20-nxcsxdi56nj2gxyo65iyuaecp3cbd4xd</env>
      <env name="LAPACK_ROOT">/apps/spack/opt/spack/linux-centos7-cascadelake/gcc-12.2.0/netlib-lapack-3.10.1-xjw3q4abrpdihbyvx72em7l4wrzxm3zp</env>
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
    </environment_variables>

    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
      <env name="OMP_PLACES">threads</env>
    </environment_variables>
  </machine>

  <machine MACH="fugaku">
    <DESC>RIKEN-CCS Fugaku: Fujitsu A64FX 48 cores/node.</DESC>
    <NODENAME_REGEX>fn01sv.*</NODENAME_REGEX>
    <OS>LINUX</OS>
    <COMPILERS>gnu,fj</COMPILERS>
    <MPILIBS>fujitsu</MPILIBS>
    <PROJECT>hp210190</PROJECT>
    <SAVE_TIMING_DIR>/data/hp210190/</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>/data/hp210190/$USER/scratch</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/data/hp210190/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/data/hp210190/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/data/hp210190/$USER/scratch/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/data/hp210190/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/data/hp210190/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_integration</TESTS>
    <NTEST_PARALLEL_JOBS>4</NTEST_PARALLEL_JOBS>
    <BATCH_SYSTEM>moab</BATCH_SYSTEM>
    <SUPPORTED_BY>E3SM</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>48</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>FALSE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }} -std e3sm.log.$LID </arg>
      </arguments>
    </mpirun>
    <module_system type="soft">
      <init_path lang="sh">/vol0003/hp210190/data/soft/spack-v0.17.0/share/spack/setup-env.sh</init_path>
      <init_path lang="csh">/vol0003/hp210190/data/soft/spack-v0.17.0/share/spack/setup-env.csh</init_path>
      <cmd_path lang="csh">spack</cmd_path>
      <cmd_path lang="sh">spack</cmd_path>
      <modules compiler="gnu">
        <command name="unload">--all</command>
        <command name="load">gcc @11.2.0%gcc@8.4.1  arch=linux-rhel8-a64fx</command>
        <command name="load">fujitsu-mpi @head%gcc@11.2.0  arch=linux-rhel8-a64fx</command>
        <command name="find">--loaded;ln -sf /lib64/libhwloc.so.15 /tmp/libhwloc.so.5</command>
      </modules>
      <modules compiler="fj">
        <command name="unload">--all</command>
        <command name="load">netcdf-c       @4.8.1 %fj@4.7.0 arch=linux-rhel8-a64fx</command>
        <command name="load">netcdf-cxx     @4.2   %fj@4.7.0 arch=linux-rhel8-a64fx</command>
        <command name="load">netcdf-fortran @4.5.3 %fj@4.7.0 arch=linux-rhel8-a64fx</command>
        <command name="load">parallel-netcdf@1.12.2%fj@4.7.0 arch=linux-rhel8-a64fx</command>
        <command name="load">netlib-lapack  @3.9.1 %fj@4.7.0 arch=linux-rhel8-a64fx</command>
        <command name="find">--loaded</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <MAX_GB_OLD_TEST_DATA>1000</MAX_GB_OLD_TEST_DATA>
    <environment_variables>
      <env name="PERL5LIB">/data/hp210190/soft/perl5/lib/perl5:/home/hp210190/u02380/perl5/lib/perl5</env>
      <env name="OMPI_MCA_plm_ple_numanode_assign_policy">share_band</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <env name="NETCDF_PATH">/data/hp210190/soft/netcdf/4.4.1c-4.2cxx-4.4.4f/gcc8.3.1</env>
      <env name="LAPACK_PATH">/data/hp210190/soft/spack-v0.16/opt/spack/linux-rhel8-a64fx/gcc-8.3.1/netlib-lapack-3.8.0-jhmofiqoky6ajxmda5caawfhqnrirmm5</env>
      <env name="LD_LIBRARY_PATH">/tmp:/data/hp210190/soft/spack-v0.16/opt/spack/linux-rhel8-a64fx/gcc-8.3.1/netlib-lapack-3.8.0-jhmofiqoky6ajxmda5caawfhqnrirmm5/lib64:$ENV{LD_LIBRARY_PATH}</env>
    </environment_variables>
    <environment_variables compiler="fj">
      <env name="NETCDF_C_PATH">$SHELL{dirname $(dirname $(which nc-config))}</env>
      <env name="NETCDF_FORTRAN_PATH">$SHELL{dirname $(dirname $(which nf-config))}</env>
      <env name="PNETCDF_PATH">$SHELL{dirname $(dirname $(which pnetcdf_version))}</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE">
      <env name="OMP_STACKSIZE">128M</env>
    </environment_variables>
    <environment_variables BUILD_THREADED="TRUE" compiler="gnu">
      <env name="OMP_PLACES">cores</env>
    </environment_variables>
  </machine>

  <machine MACH="onyx">
    <DESC>ERDC XC40, os is CNL, 44 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>onyx</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CHARGE_ACCOUNT>NPSCA07935242</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/p/app/unsupported/RASM/acme</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{WORKDIR}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/p/app/unsupported/RASM/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/p/app/unsupported/RASM/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/p/app/unsupported/RASM/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/p/app/unsupported/RASM/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>rasm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>44</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>44</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }}</arg>
        <arg name="tasks_per_node">-N $SHELL{if [ `./xmlquery --value MAX_MPITASKS_PER_NODE` -gt `./xmlquery --value TOTAL_TASKS` ];then echo `./xmlquery --value TOTAL_TASKS`;else echo `./xmlquery --value MAX_MPITASKS_PER_NODE`;fi;}</arg>
        <arg name="hyperthreading">--cc depth -d $SHELL{echo `./xmlquery --value MAX_TASKS_PER_NODE`/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc} -j $SHELL{if [ 64 -ge `./xmlquery --value MAX_TASKS_PER_NODE` ];then echo 1;else echo `./xmlquery --value MAX_TASKS_PER_NODE`/64|bc;fi;}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/modules/default/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="rm">PrgEnv-intel</command>
        <command name="rm">PrgEnv-cray</command>
        <command name="rm">PrgEnv-gnu</command>
        <command name="rm">PrgEnv-pgi</command>
        <command name="rm">intel</command>
        <command name="rm">cce</command>
        <command name="rm">gcc</command>
        <command name="rm">cray-parallel-netcdf</command>
        <command name="rm">cray-parallel-hdf5</command>
        <command name="rm">pmi</command>
        <command name="rm">cray-libsci</command>
        <command name="rm">cray-mpich2</command>
        <command name="rm">cray-mpich</command>
        <command name="rm">cray-netcdf</command>
        <command name="rm">cray-hdf5</command>
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">craype-mic-knl</command>
        <command name="rm">craype-sandybridge</command>
        <command name="rm">craype-ivybridge</command>
        <command name="rm">craype</command>
        <command name="rm">papi</command>
        <command name="rm">cray-petsc</command>
        <command name="rm">cray-libsci</command>
        <command name="rm">esmf</command>
        <command name="rm">craype</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/6.0.9</command>
        <command name="rm">intel</command>
        <command name="load">intel/19.1.3.304</command>
        <command name="rm">cray-mpich</command>
        <command name="load">cray-mpich/7.7.16</command>
        <command name="rm">cray-hdf5</command>
        <command name="rm">cray-hdf5-parallel</command>
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">cray-parallel-netcdf</command>
        <command name="rm">netcdf</command>
        <command name="load">cray-netcdf/4.7.4.0</command>
        <command name="load">cray-hdf5/1.12.0.0</command>
        <command name="load">cray-parallel-netcdf/1.12.1.0</command>
        <command name="rm">cray-libsci</command>
        <command name="load">cmake/intel-19.1.3.304/3.21.0</command>
        <command name="load">cray-libsci/20.09.1</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <env name="NETCDF_DIR">/opt/cray/pe/netcdf/4.7.4.0/intel/19.1</env>
      <!--env name="MPICH_CPUMASK_DISPLAY">1</env-->

      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>

      <!--env name="HDF5_DISABLE_VERSION_CHECK">2</env-->
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
      <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
    <environment_variables compiler="intel18">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
  </machine>

  <machine MACH="narwhal">
    <DESC>NavyDSRC Cray EX, os is CNL, 128 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>narwhal</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <PROJECT>NPSCA07935242</PROJECT>
    <SAVE_TIMING_DIR>/p/work1/projects/RASM/acme</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{WORKDIR}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/p/work1/projects/RASM/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/p/work1/projects/RASM/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/p/work1/projects/RASM/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/p/work1/projects/RASM/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>e3sm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }}</arg>
        <arg name="tasks_per_node">-N $SHELL{if [ `./xmlquery --value MAX_MPITASKS_PER_NODE` -gt `./xmlquery --value TOTAL_TASKS` ];then echo `./xmlquery --value TOTAL_TASKS`;else echo `./xmlquery --value MAX_MPITASKS_PER_NODE`;fi;}</arg>
        <arg name="hyperthreading">--cc depth -d $SHELL{echo `./xmlquery --value MAX_TASKS_PER_NODE`/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc} -j $SHELL{if [ 64 -ge `./xmlquery --value MAX_TASKS_PER_NODE` ];then echo 1;else echo `./xmlquery --value MAX_TASKS_PER_NODE`/64|bc;fi;}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/cray/pe/modules/3.2.11.5/init/perl.pm</init_path>
      <init_path lang="python">/opt/cray/pe/modules/3.2.11.5/init/python.py</init_path>
      <init_path lang="sh">/opt/cray/pe/modules/3.2.11.5/init/sh</init_path>
      <init_path lang="csh">/opt/cray/pe/modules/3.2.11.5/init/csh</init_path>
      <cmd_path lang="perl">/opt/cray/pe/modules/3.2.11.5/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/cray/pe/modules/3.2.11.5/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="rm">PrgEnv-intel</command>
        <command name="rm">PrgEnv-cray</command>
        <command name="rm">PrgEnv-gnu</command>
        <command name="rm">PrgEnv-nvidia</command>
        <command name="rm">PrgEnv-aocc</command>
        <command name="rm">intel</command>
        <command name="rm">cray-mpich</command>
        <command name="rm">cray-hdf5</command>
        <command name="rm">cray-hdf5-parallel</command>
        <command name="rm">cray-netcdf</command>
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">cray-parallel-netcdf</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.0.0</command>
        <command name="rm">intel</command>
        <command name="rm">intel-classic</command>
        <command name="load">intel-classic/2021.3.0</command>
        <command name="rm">cray-mpich</command>
        <command name="load">cray-mpich/8.1.14</command>
        <command name="load">cray-netcdf/4.7.4.7</command>
        <command name="load">cray-hdf5/1.12.0.7</command>
        <command name="load">cray-parallel-netcdf/1.12.1.7</command>
        <command name="rm">cray-libsci</command>
        <command name="load">cray-libsci/21.08.1.2</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <!--env name="NETCDF_DIR">/opt/cray/pe/netcdf/4.7.4.0/intel/19.1</env-->
      <!--env name="MPICH_CPUMASK_DISPLAY">1</env-->

      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>

      <!--env name="HDF5_DISABLE_VERSION_CHECK">2</env-->
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
    <environment_variables compiler="intel18">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
  </machine>

  <machine MACH="warhawk">
    <DESC>AFRL Cray EX, os is CNL, 128 pes/node, batch system is PBS</DESC>
    <NODENAME_REGEX>warhawk</NODENAME_REGEX>
    <OS>CNL</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>mpich</MPILIBS>
    <CHARGE_ACCOUNT>NPSCA07935242</CHARGE_ACCOUNT>
    <SAVE_TIMING_DIR>/p/work1/projects/RASM/acme</SAVE_TIMING_DIR>
    <SAVE_TIMING_DIR_PROJECTS>.*</SAVE_TIMING_DIR_PROJECTS>
    <CIME_OUTPUT_ROOT>$ENV{WORKDIR}</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/p/work1/projects/RASM/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/p/work1/projects/RASM/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/p/work1/projects/RASM/acme/baselines/$COMPILER</BASELINE_ROOT>
    <CCSM_CPRNC>/p/work1/projects/RASM/tools/cprnc/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <TESTS>e3sm_developer</TESTS>
    <BATCH_SYSTEM>pbs</BATCH_SYSTEM>
    <SUPPORTED_BY>rasm</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <mpirun mpilib="default">
      <executable>aprun</executable>
      <arguments>
        <arg name="num_tasks">-n {{ total_tasks }}</arg>
        <arg name="tasks_per_node">-N $SHELL{if [ `./xmlquery --value MAX_MPITASKS_PER_NODE` -gt `./xmlquery --value TOTAL_TASKS` ];then echo `./xmlquery --value TOTAL_TASKS`;else echo `./xmlquery --value MAX_MPITASKS_PER_NODE`;fi;}</arg>
        <arg name="hyperthreading">--cc depth -d $SHELL{echo `./xmlquery --value MAX_TASKS_PER_NODE`/`./xmlquery --value MAX_MPITASKS_PER_NODE`|bc} -j $SHELL{if [ 64 -ge `./xmlquery --value MAX_TASKS_PER_NODE` ];then echo 1;else echo `./xmlquery --value MAX_TASKS_PER_NODE`/64|bc;fi;}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/p/app/Modules/4.7.1/init/perl.pm</init_path>
      <init_path lang="python">/p/app/Modules/4.7.1/init/python.py</init_path>
      <init_path lang="sh">/p/app/Modules/4.7.1/init/sh</init_path>
      <init_path lang="csh">/p/app/Modules/4.7.1/init/csh</init_path>
      <cmd_path lang="perl">/p/app/Modules/4.7.1/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/p/app/Modules/4.7.1/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="rm">PrgEnv-intel</command>
        <command name="rm">PrgEnv-cray</command>
        <command name="rm">PrgEnv-gnu</command>
        <command name="rm">PrgEnv-nvidia</command>
        <command name="rm">craype-x86-rome</command>
        <command name="rm">craype-network-ofi</command>
        <command name="rm">cray-dsmml</command>
        <command name="rm">perftools-base</command>
        <command name="rm">cray-libsci</command>
        <command name="rm">cce</command>
        <command name="rm">intel</command>
        <command name="rm">gcc</command>
        <command name="rm">cray-mpich</command>
        <command name="rm">cray-hdf5</command>
        <command name="rm">cray-hdf5-parallel</command>
        <command name="rm">cray-netcdf</command>
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">cray-parallel-netcdf</command>
      </modules>

      <modules compiler="intel">
        <command name="load">PrgEnv-intel/8.0.0</command>
        <command name="rm">intel</command>
        <command name="load">intel-classic/2021.3.0</command>
        <command name="rm">cray-mpich</command>
        <command name="load">cray-mpich/8.1.9</command>
        <command name="load">cray-pals/1.0.17</command>
        <command name="load">cray-netcdf/4.7.4.4</command>
        <command name="load">cray-hdf5/1.12.0.4</command>
        <command name="load">cray-parallel-netcdf/1.12.1.4</command>
        <command name="rm">cray-libsci</command>
        <command name="load">cmake/3.21.4</command>
        <command name="load">cray-libsci/21.08.1.2</command>
      </modules>
    </module_system>
    <RUNDIR>$CIME_OUTPUT_ROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CIME_OUTPUT_ROOT/$CASE/bld</EXEROOT>
    <TEST_TPUT_TOLERANCE>0.1</TEST_TPUT_TOLERANCE>
    <environment_variables>
      <env name="MPICH_ENV_DISPLAY">1</env>
      <env name="MPICH_VERSION_DISPLAY">1</env>
      <!--env name="NETCDF_DIR">/opt/cray/pe/netcdf/4.7.4.0/intel/19.1</env-->
      <!--env name="MPICH_CPUMASK_DISPLAY">1</env-->

      <env name="OMP_STACKSIZE">64M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>

      <!--env name="HDF5_DISABLE_VERSION_CHECK">2</env-->
      <env name="HDF5_USE_FILE_LOCKING">FALSE</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
    <environment_variables compiler="intel18">
      <env name="FORT_BUFFERED">yes</env>
    </environment_variables>
  </machine>

  <default_run_suffix>
    <default_run_exe>${EXEROOT}/e3sm.exe </default_run_exe>
    <default_run_misc_suffix> &gt;&gt; e3sm.log.$LID 2&gt;&amp;1 </default_run_misc_suffix>
  </default_run_suffix>

</config_machines>
