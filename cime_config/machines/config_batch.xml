<?xml version="1.0"?>
<config_batch version="2.0">
  <!--
     File:    config_batch.xml
     Purpose: abstract out the parts of run scripts that are different, and use this configuration to
     create acme run scripts from a single template.

     batch_system:     the batch system type and version
     batch_query:      the batch query command for each batch system.
     batch_redirect:   Whether a redirect character is needed to submit jobs.
     batch_directive:  The string that prepends a batch directive for the batch system.
     jobid_pattern:    A perl regular expression used to filter out the returned job id from a
                       queue submission.
     depend_pattern:

 ===============================================================
 batch_system
 ===============================================================
 The batch_system and associated tags are meant for configuring batch systems and
 queues across machines.  The batch_system tag denotes the name for a particular
 batch system, these can either be shared between one or more machines, or can be
 defined for a specific machine if need be.

 Machine specific entries take precidence over generic entries, directives are appended

 queues:
 one or more queues can be defined per batch_system. if the attribute default="true"
 is used, then that queue will be used by default. Alternatively, multiple queues can
 be used.  The following variables can be used to choose a queue :
 walltimemin: Giving the minimum amount of walltime for the queue.
 walltimemax: The maximum amount of walltime for a queue.
 nodemin:      The minimum node count required to use this queue.
 nodemax:      The maximum node count required to use this queue.
 jobmin:      The minimum task count required to use this queue. This should only rarely be used to select queues that only use a fraction of a node. This cannot be used in conjuction with nodemin.
 jobmax:      The maximum task count required to use this queue. This should only rarely be used to select queues that only use a fraction of a node. This cannot be used in conjuction with nodemax.
    -->
  <batch_system type="template" >
    <batch_query args=""></batch_query>
    <batch_submit></batch_submit>
    <batch_cancel></batch_cancel>
    <batch_redirect></batch_redirect>
    <batch_directive></batch_directive>
    <directives>
      <directive></directive>
    </directives>
  </batch_system>

  <batch_system type="none" >
    <batch_query args=""></batch_query>
    <batch_submit></batch_submit>
    <batch_cancel></batch_cancel>
    <batch_redirect></batch_redirect>
    <batch_directive></batch_directive>
    <directives>
      <directive></directive>
    </directives>
  </batch_system>

   <batch_system MACH="spock" type="slurm">
     <queues>
       <queue strict="true" nodemin="1" nodemax="4" walltimemax="03:00:00" default="true">ecp</queue>
       <queue strict="true" nodemin="1" nodemax="4" walltimemax="03:00:00">caar</queue>
       <queue strict="true" nodemin="5" nodemax="16" walltimemax="01:00:00">caar</queue>
     </queues>
   </batch_system>

   <batch_system MACH="crusher" type="slurm">
     <batch_submit>/lustre/orion/cli115/world-shared/e3sm/tools/sbatch/throttle</batch_submit>
     <directives>
       <directive> --core-spec=$SHELL{tpn=`./xmlquery --value MAX_TASKS_PER_NODE`; if [[ $tpn > 56 ]]; then echo "64-$tpn"|bc; else echo 8; fi; }</directive>
     </directives>
     <queues>
       <queue strict="true" nodemin="1" nodemax="8" walltimemax="08:00:00" default="true">batch</queue>
       <queue strict="true" nodemin="9" nodemax="64" walltimemax="04:00:00">batch</queue>
       <queue strict="true" nodemin="65" nodemax="160" walltimemax="02:00:00">batch</queue>
     </queues>
   </batch_system>

  <batch_system type="cobalt" >
    <batch_query>qstat</batch_query>
    <batch_submit>qsub</batch_submit>
    <batch_cancel>qdel</batch_cancel>
    <batch_env>--env</batch_env>
    <batch_directive></batch_directive>
    <jobid_pattern>(\d+)</jobid_pattern>
    <depend_string> --dependencies</depend_string>
    <walltime_format>%H:%M:%s</walltime_format>
    <batch_mail_flag>-M</batch_mail_flag>
    <batch_mail_type_flag></batch_mail_type_flag>
    <batch_mail_type></batch_mail_type>
    <submit_args>
      <arg flag="--cwd" name="CASEROOT"/>
      <arg flag="-A" name="CHARGE_ACCOUNT"/>
      <arg flag="-t" name="JOB_WALLCLOCK_TIME"/>
      <arg flag="-q" name="JOB_QUEUE"/>
      <arg flag="--mode script"/>
    </submit_args>
    <directives>
      <directive> -n {{ num_nodes }} </directive>
    </directives>
  </batch_system>

  <batch_system type="cobalt_theta" >
    <batch_query>qstat</batch_query>
    <batch_submit>/projects/ccsm/acme/tools/cobalt/dsub</batch_submit>
    <batch_cancel>qdel</batch_cancel>
    <batch_env>--env</batch_env>
    <batch_directive>#COBALT</batch_directive>
    <jobid_pattern>(\d+)</jobid_pattern>
    <depend_string>--dependencies jobid</depend_string>
    <depend_separator>:</depend_separator>
    <batch_mail_flag>-M</batch_mail_flag>
    <batch_mail_type_flag></batch_mail_type_flag>
    <batch_mail_type></batch_mail_type>
    <submit_args>
      <arg flag="-A" name="CHARGE_ACCOUNT"/>
      <arg flag="-t" name="JOB_WALLCLOCK_TIME"/>
      <arg flag="-q" name="JOB_QUEUE"/>
      <arg flag="--mode script"/>
    </submit_args>
    <directives>
      <directive> -n {{ num_nodes }} </directive>
    </directives>
  </batch_system>

<!-- This is the new version on Summit, released as IBM 10.1.0.0 build 476197, Nov 21 2017.  -->
  <batch_system type="lsf" version="10.1">
    <batch_query args=" -w" >bjobs</batch_query>
    <batch_submit>bsub</batch_submit>
    <batch_cancel>bkill</batch_cancel>
    <batch_env>-env</batch_env>
    <batch_directive>#BSUB</batch_directive>
    <jobid_pattern>(\d+)</jobid_pattern>
    <depend_string> -w 'done(jobid)'</depend_string>
    <depend_allow_string> -w 'ended(jobid)'</depend_allow_string>
    <depend_separator>&amp;&amp;</depend_separator>
    <walltime_format>%H:%M</walltime_format>
    <batch_mail_flag>-u</batch_mail_flag>
    <batch_mail_type_flag> </batch_mail_type_flag>
    <batch_mail_type>, -B -N, -B,-N,-N</batch_mail_type>
    <submit_args>
      <arg flag="-q" name="$JOB_QUEUE"/>
      <arg flag="-W" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-P" name="$CHARGE_ACCOUNT"/>
    </submit_args>
    <directives>
      <directive                       > -nnodes {{ num_nodes }} </directive>
      <directive default="e3sm.stdout" > -o {{ output_error_path }}.%J  </directive>
      <directive default="e3sm.stderr" > -e {{ output_error_path }}.%J  </directive>
      <directive                       > -J {{ job_id }} </directive>
    </directives>
  </batch_system>

  <batch_system type="pbs" >
    <batch_query args="-f" >qstat</batch_query>
    <batch_submit>qsub </batch_submit>
    <batch_cancel>qdel</batch_cancel>
    <batch_env>-v</batch_env>
    <batch_directive>#PBS</batch_directive>
    <jobid_pattern>^(\S+)$</jobid_pattern>
    <depend_string>-W depend=afterok:jobid</depend_string>
    <depend_allow_string>-W depend=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>-M</batch_mail_flag>
    <batch_mail_type_flag>-m</batch_mail_type_flag>
    <batch_mail_type>, bea, b, e, a</batch_mail_type>
    <submit_args>
      <arg flag="-q" name="$JOB_QUEUE"/>
      <arg flag="-l walltime=" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-A" name="$CHARGE_ACCOUNT"/>
    </submit_args>
    <directives>
      <directive> -N {{ job_id }}</directive>
      <directive default="n"> -r {{ rerunnable }} </directive>
      <!-- <directive> -j oe {{ job_id }} </directive> -->
      <directive> -j oe </directive>
      <directive> -V </directive>
    </directives>
  </batch_system>

  <batch_system type="pbspro" >
    <batch_query args="-f">qstat</batch_query>
    <batch_submit>qsub </batch_submit>
    <batch_cancel>qdel</batch_cancel>
    <batch_env>-v</batch_env>
    <batch_directive>#PBS</batch_directive>
    <jobid_pattern>^(\S+)$</jobid_pattern>
    <depend_string>-W depend=afterok:jobid</depend_string>
    <depend_allow_string>-W depend=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>-M</batch_mail_flag>
    <batch_mail_type_flag>-m</batch_mail_type_flag>
    <batch_mail_type>, bea, b, e, a</batch_mail_type>
    <submit_args>
      <arg flag="-q" name="$JOB_QUEUE"/>
      <arg flag="-l walltime=" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-A" name="$CHARGE_ACCOUNT"/>
    </submit_args>
    <directives>
      <directive> -N {{ job_id }}</directive>
      <directive> -l select={{ num_nodes }} </directive>
      <directive default="n"> -r {{ rerunnable }} </directive>
      <directive> -j oe </directive>
      <directive> -V </directive>
    </directives>
  </batch_system>

  <batch_system type="moab">
    <batch_query>pjstat</batch_query>
    <batch_submit>pjsub</batch_submit>
    <batch_cancel>pjdel</batch_cancel>
    <batch_directive>#PJM</batch_directive>
    <jobid_pattern>(\d\d\d\d\d+)</jobid_pattern>
    <walltime_format>%H:%M:%S</walltime_format>
    <directives>
      <directive> -L "node={{ num_nodes }}"</directive>
      <directive> -L "elapse={{ job_wallclock_time }}"</directive>
      <directive> --mpi "proc={{ total_tasks }}"</directive>
      <directive> -s </directive>
    </directives>
  </batch_system>

  <!-- for lawrence livermore computing -->
  <batch_system MACH="ruby" type="slurm">
    <queues>
      <queue walltimemax="24:00:00" nodemax="520" default="true">pbatch</queue>
      <queue walltimemax="01:00:00" nodemax="12">pdebug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="dane" type="slurm">
    <queues>
      <queue walltimemax="24:00:00" nodemax="520" default="true">pbatch</queue>
      <queue walltimemax="01:00:00" nodemax="20">pdebug</queue>
    </queues>
  </batch_system>
  <!-- for lawrence livermore computing -->

  <!-- for NERSC machines: perlmutter etc -->
  <batch_system type="nersc_slurm" >
    <batch_query per_job_arg="-j">squeue</batch_query>
    <batch_submit>sbatch</batch_submit>
    <batch_cancel>scancel</batch_cancel>
    <batch_directive>#SBATCH</batch_directive>
    <jobid_pattern>(\d+)$</jobid_pattern>
    <depend_string>--dependency=afterok:jobid</depend_string>
    <depend_allow_string>--dependency=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>--mail-user</batch_mail_flag>
    <batch_mail_type_flag>--mail-type</batch_mail_type_flag>
    <batch_mail_type>none, all, begin, end, fail</batch_mail_type>
    <submit_args>
      <arg flag="--time" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-q" name="$JOB_QUEUE"/>
      <arg flag="--account" name="$PROJECT"/>
    </submit_args>
    <directives>
      <directive> --job-name={{ job_id }}</directive>
      <directive> --nodes={{ num_nodes }}</directive>
      <directive> --output={{ job_id }}.%j </directive>
    </directives>
  </batch_system>


  <batch_system type="miller_slurm" >
    <batch_query per_job_arg="-j">squeue</batch_query>
    <batch_submit>sbatch</batch_submit>
    <batch_cancel>scancel</batch_cancel>
    <batch_directive>#SBATCH</batch_directive>
    <jobid_pattern>(\d+)</jobid_pattern>
    <depend_string>--dependency=afterok:jobid</depend_string>
    <depend_allow_string>--dependency=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>--mail-user</batch_mail_flag>
    <batch_mail_type_flag>--mail-type</batch_mail_type_flag>
    <batch_mail_type>none, all, begin, end, fail</batch_mail_type>
    <submit_args>
      <arg flag="--time" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-q" name="$JOB_QUEUE"/>
      <arg flag="--account" name="$PROJECT"/>
      <arg flag="--cluster-constraint='green'"/>
    </submit_args>
    <directives>
      <directive> --job-name={{ job_id }}</directive>
      <directive> --nodes={{ num_nodes }}</directive>
      <directive> --output={{ job_id }}.%j </directive>
      <directive> --exclusive </directive>
    </directives>
  </batch_system>


  <batch_system type="slurm" >
    <batch_query per_job_arg="-j">squeue</batch_query>
    <batch_submit>sbatch</batch_submit>
    <batch_cancel>scancel</batch_cancel>
    <batch_directive>#SBATCH</batch_directive>
    <jobid_pattern>(\d+)$</jobid_pattern>
    <depend_string>--dependency=afterok:jobid</depend_string>
    <depend_allow_string>--dependency=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>--mail-user</batch_mail_flag>
    <batch_mail_type_flag>--mail-type</batch_mail_type_flag>
    <batch_mail_type>none, all, begin, end, fail</batch_mail_type>
    <submit_args>
      <arg flag="--time" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-p" name="$JOB_QUEUE"/>
      <arg flag="--account" name="$PROJECT"/>
    </submit_args>
    <directives>
      <directive> --job-name={{ job_id }}</directive>
      <directive> --nodes={{ num_nodes }}</directive>
      <directive> --output={{ job_id }}.%j </directive>
      <directive> --exclusive </directive>
    </directives>
  </batch_system>

  <batch_system type="slurm_single_node" >
    <batch_query per_job_arg="-j">squeue</batch_query>
    <batch_submit>sbatch</batch_submit>
    <batch_cancel>scancel</batch_cancel>
    <batch_directive>#SBATCH</batch_directive>
    <jobid_pattern>(\d+)$</jobid_pattern>
    <depend_string>--dependency=afterok:jobid</depend_string>
    <depend_allow_string>--dependency=afterany:jobid</depend_allow_string>
    <depend_separator>:</depend_separator>
    <walltime_format>%H:%M:%S</walltime_format>
    <batch_mail_flag>--mail-user</batch_mail_flag>
    <batch_mail_type_flag>--mail-type</batch_mail_type_flag>
    <batch_mail_type>none</batch_mail_type>
    <submit_args>
      <arg flag="--time" name="$JOB_WALLCLOCK_TIME"/>
      <arg flag="-p" name="$JOB_QUEUE"/>
    </submit_args>
    <directives>
      <directive> --job-name={{ job_id }}</directive>
      <directive> --nodes=1</directive>
      <directive> --ntasks={{ total_tasks }}</directive>
      <directive> --cpus-per-task={{ thread_count }}</directive>
      <directive> --output={{ job_id }}.%j </directive>
    </directives>
  </batch_system>

    <batch_system MACH="blues" type="slurm">
      <queues>
        <queue walltimemax="01:00:00" nodemax="6" default="true" strict="true">biggpu</queue>
      </queues>
    </batch_system>

    <batch_system MACH="mappy" type="slurm_single_node">
      <queues>
        <queue walltimemax="12:00:00" nodemax="1" default="true" strict="true">batch</queue>
      </queues>
    </batch_system>

    <batch_system MACH="swing" type="slurm">
      <directives>
        <directive> --gres=gpu:1</directive>
      </directives>
      <queues>
        <queue walltimemax="01:00:00" nodemax="6" default="true" strict="true">gpu</queue>
      </queues>
    </batch_system>

    <batch_system MACH="anvil" type="slurm" >
      <queues>
	<queue walltimemax="48:00:00" nodemax="5" default="true" strict="true">acme-small</queue>
	<queue walltimemax="24:00:00" nodemin="6" nodemax="60" strict="true">acme-medium</queue>
	<queue walltimemax="12:00:00" nodemin="61" strict="true">acme-large</queue>
      </queues>
    </batch_system>

    <batch_system MACH="chrysalis" type="slurm" >
      <directives>
        <directive> --switches=$SHELL{echo "(`./xmlquery --value NUM_NODES` + 19) / 20" |bc}</directive>
      </directives>
      <queues>
	<queue walltimemax="48:00:00" strict="true" nodemin="1" nodemax="492">compute</queue>
	<queue walltimemax="04:00:00" strict="true" nodemin="1" nodemax="20" default="true">debug</queue>
      </queues>
    </batch_system>

    <batch_system MACH="bebop" type="slurm" >
      <queues>
	<queue walltimemax="00:30:00" nodemax="64" strict="true">debug</queue>
        <queue walltimemax="01:00:00" nodemax="608" default="true">bdw</queue>
        <queue walltimemax="01:00:00" nodemax="512">knl</queue>
      </queues>
    </batch_system>

  <batch_system MACH="improv" type="pbspro">
    <directives>
      <directive> -l select={{ num_nodes }}:mpiprocs={{ tasks_per_node }} </directive>
    </directives>
    <queues>
      <queue walltimemax="00:30:00" jobmin="1" jobmax="8" strict="true" default="true">debug</queue>
      <queue walltimemax="00:30:00" jobmin="9" jobmax="737" strict="true">compute</queue>
    </queues>
  </batch_system>

  <batch_system MACH="miller" type="miller_slurm">
    <queues>
      <queue walltimemax="06:00:00" nodemax="800" default="true">collaboration</queue>
    </queues>
  </batch_system>

  <batch_system MACH="pm-gpu" type="nersc_slurm">
    <directives>
      <directive> --constraint=gpu</directive>
    </directives>
    <directives COMPSET="!.*MMF.*" compiler="gnugpu">
      <directive> --gpus-per-node=4</directive>
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives COMPSET=".*MMF.*" compiler="gnugpu">
      <directive> --gpus-per-task=1</directive>
      <directive> --gpu-bind=map_gpu:0,1,2,3</directive>
    </directives>
    <directives compiler="nvidiagpu">
      <directive> --gpus-per-node=4</directive>
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives compiler="gnu">
      <directive> -G 0</directive>
    </directives>
    <directives compiler="nvidia">
      <directive> -G 0</directive>
    </directives>
    <queues>
      <queue walltimemax="00:45:00" nodemax="1792" default="true">regular</queue>
      <queue walltimemax="00:45:00" nodemax="1792" strict="true">preempt</queue>
      <queue walltimemax="00:45:00" nodemax="1792" strict="true">shared</queue>
      <queue walltimemax="00:45:00" nodemax="1792" strict="true">overrun</queue>
      <queue walltimemax="00:15:00" nodemax="4" strict="true">debug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="muller-cpu" type="nersc_slurm">
    <directives>
      <directive> --constraint=cpu</directive>
    </directives>
    <queues>
      <!-- Note: walltime is not the max walltime, but the default - see NERSC docs for Q limits, https://docs.nersc.gov/jobs/policy/ -->
      <queue walltimemax="00:30:00" nodemax="16" default="true">regular</queue>
      <queue walltimemax="00:30:00" nodemax="16" strict="true">preempt</queue>
      <queue walltimemax="00:30:00" nodemax="8" strict="true">debug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="muller-gpu" type="nersc_slurm">
    <directives>
      <directive> --constraint=gpu</directive>
    </directives>
    <directives COMPSET="!.*MMF.*" compiler="gnugpu">
      <directive> --gpus-per-node=4</directive>
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives COMPSET=".*MMF.*" compiler="gnugpu">
      <directive> --gpus-per-task=1</directive>
      <directive> --gpu-bind=map_gpu:0,1,2,3</directive>
    </directives>
    <directives compiler="nvidiagpu">
      <directive> --gpus-per-node=4</directive>
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives compiler="gnu">
      <directive> -G 0</directive>
    </directives>
    <directives compiler="nvidia">
      <directive> -G 0</directive>
    </directives>
    <queues>
      <queue walltimemax="00:45:00" nodemax="64" default="true">regular</queue>
      <queue walltimemax="00:45:00" nodemax="64" strict="true">preempt</queue>
      <queue walltimemax="00:15:00" nodemax="4" strict="true">debug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="pm-cpu" type="nersc_slurm">
    <directives>
      <directive> --constraint=cpu</directive>
    </directives>
    <queues>
      <!-- Note: walltime is not the max walltime, but the default - see NERSC docs for Q limits, https://docs.nersc.gov/jobs/policy/ -->
      <queue walltimemax="00:30:00" nodemax="3072" default="true">regular</queue>
      <queue walltimemax="00:30:00" nodemax="3072" strict="true">preempt</queue>
      <queue walltimemax="00:30:00" nodemax="3072" strict="true">shared</queue>
      <queue walltimemax="00:30:00" nodemax="3072" strict="true">overrun</queue>
      <queue walltimemax="00:30:00" nodemax="8" strict="true">debug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="alvarez" type="nersc_slurm">
    <directives>
      <directive> --constraint=cpu</directive>
    </directives>
    <queues>
      <queue walltimemax="00:30:00" nodemax="256" default="true">regular</queue>
      <queue walltimemax="00:30:00" nodemax="4" strict="true">debug</queue>
    </queues>
  </batch_system>

  <batch_system MACH="stampede2" type="slurm">
    <directives>
      <directive>-n {{ total_tasks }}</directive>
    </directives>
    <queues>
      <queue walltimemax="00:30:00" nodemax="4" strict="true">skx-dev</queue>
      <queue walltimemax="00:30:00" nodemax="868" strict="true">skx-large</queue>
      <queue walltimemax="01:00:00" nodemax="128" default="true">skx-normal</queue>
    </queues>
  </batch_system>

    <batch_system MACH="jlse" type="cobalt_theta">
      <batch_submit>qsub</batch_submit>
      <queues>
        <queue walltimemax="01:00:00" jobmin="1" jobmax="12">skylake_8180</queue>
        <queue walltimemax="01:00:00" jobmin="1" jobmax="16">iris</queue>
        <queue walltimemax="01:00:00" jobmin="1" jobmax="18">yarrow</queue>
        <queue walltimemax="01:00:00" jobmin="1" jobmax="12" default="true">arcticus</queue>
      </queues>
    </batch_system>

    <batch_system MACH="sunspot" type="pbspro">
      <batch_submit>/lus/gila/projects/CSC249ADSE15_CNDA/tools/qsub/throttle</batch_submit>
      <queues>
        <queue walltimemax="00:30:00" jobmin="1" jobmax="128" default="true">workq</queue>
        <queue walltimemax="00:30:00" jobmin="1" jobmax="128">diag</queue>
      </queues>
    </batch_system>

    <batch_system MACH="aurora" type="pbspro">
      <batch_submit>/lus/flare/projects/E3SM_Dec/tools/qsub/throttle</batch_submit>
      <directives>
        <directive> -l filesystems=home:flare </directive>
      </directives>
      <queues>
        <queue walltimemax="01:00:00" nodemin="1"    nodemax="2" default="true">debug</queue>
        <queue walltimemax="01:00:00" nodemin="3"    nodemax="31"   >debug-scaling</queue>
        <queue walltimemax="01:00:00" nodemin="32"   nodemax="2048" >prod</queue>
        <queue walltimemax="01:00:00" nodemin="2049" nodemax="10624">prod-large</queue>
      </queues>
    </batch_system>
    
    <batch_system MACH="polaris" type="pbspro">
      <batch_submit>/grand/E3SMinput/soft/qsub/throttle</batch_submit>
      <directives>
        <directive> -l filesystems=home:grand:eagle </directive>
      </directives>
      <queues>
        <queue walltimemax="00:30:00" nodemin="1"  nodemax="2"   strict="true" default="true">debug</queue>
        <queue walltimemax="00:30:00" nodemin="3"  nodemax="9"   strict="true">debug-scaling</queue>
        <queue walltimemax="01:00:00" nodemin="10" nodemax="496" strict="true">prod</queue>
      </queues>
    </batch_system>

    <batch_system MACH="cascade" type="slurm">
      <directives>
	<directive>--output=slurm.out</directive>
	<directive>--error=slurm.err</directive>
      </directives>
      <queues>
	<queue walltimemax="00:59:00" nodemin="1"  nodemax="15"  >small</queue>
	<queue walltimemax="00:59:00" nodemin="16" nodemax="127" >medium</queue>
	<queue walltimemax="00:59:00" nodemin="128" default="true" >large</queue>
      </queues>
    </batch_system>

   <batch_system MACH="constance" type="slurm">
    <directives>
      <directive>--output=slurm.out</directive>
      <directive>--error=slurm.err</directive>
    </directives>
    <queues>
      <queue walltimemax="00:59:00" default="true">slurm</queue>
    </queues>
   </batch_system>

   <batch_system MACH="compy" type="slurm">
    <queues>
      <queue walltimemax="02:00:00" strict="true" nodemin="1" nodemax="40" default="true">short</queue>
      <queue walltimemax="06:00:00" nodemin="1" nodemax="460">slurm</queue>
    </queues>
   </batch_system>

   <batch_system MACH="tahoma" type="slurm">
    <queues>
      <queue walltimemax="02:00:00" strict="true" nodemin="1" nodemax="36" default="true">normal</queue>
      <queue walltimemax="06:00:00" nodemin="1" nodemax="460">slurm</queue>
    </queues>
   </batch_system>

   <batch_system MACH="sooty" type="slurm" >
     <directives>
       <directive>--ntasks-per-node={{ tasks_per_node }}</directive>
       <directive>--output=slurm.out</directive>
       <directive>--error=slurm.err</directive>
     </directives>
     <queues>
       <queue walltimemax="00:59:00" default="true">slurm</queue>
     </queues>
   </batch_system>

  <batch_system MACH="sandiatoss3" type="slurm" >
    <queues>
      <queue nodemax="16" walltimemax="04:00:00" strict="true" default="true">short,batch</queue>
      <queue walltimemax="24:00:00">batch</queue>
    </queues>
  </batch_system>

  <batch_system MACH="boca" type="slurm" >
    <submit_args>
      <arg flag="--reservation=boca-cldera"/>
    </submit_args>
    <queues>
      <queue nodemax="20" walltimemax="04:00:00" strict="true" default="true">batch</queue>
      <queue walltimemax="48:00:00">batch</queue>
    </queues>
  </batch_system>

  <batch_system MACH="flight" type="slurm" >
    <submit_args>
      <arg flag="--reservation=flight-cldera"/>
    </submit_args>
    <queues>
      <queue nodemax="20" walltimemax="04:00:00" strict="true" default="true">batch</queue>
      <queue walltimemax="48:00:00">batch</queue>
    </queues>
  </batch_system>

  <batch_system MACH="ghost" type="slurm" >
    <queues>
      <queue nodemax="12" walltimemax="04:00:00" strict="true" default="true">short,batch</queue>
      <queue walltimemax="24:00:00">batch</queue>
    </queues>
  </batch_system>

   <batch_system MACH="chicoma-cpu" type="slurm">
    <directives>
     <directive>--partition=standard </directive>
     <directive>--qos=standard </directive>
    </directives>
    <queues>
      <queue walltimemax="16:00:00" nodemax="1792" default="true">standard</queue>
    </queues>
   </batch_system>

   <batch_system MACH="chicoma-gpu" type="slurm">
    <directives>
      <directive> --partition=gpu</directive>
    </directives>
    <directives compiler="gnugpu">
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives COMPSET="!.*MMF.*" compiler="gnugpu">
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives COMPSET=".*MMF.*" compiler="gnugpu">
      <directive> --gpu-bind=map_gpu:0,1,2,3</directive>
    </directives>
    <directives compiler="nvidiagpu">
      <directive> --gpu-bind=none</directive>
    </directives>
    <directives compiler="gnu">
      <directive> -G 0</directive>
    </directives>
    <directives compiler="nvidia">
      <directive> -G 0</directive>
    </directives>
    <queues>
      <queue walltimemax="16:00:00" nodemax="96" default="true">gpu</queue>
    </queues>
   </batch_system>

    <batch_system MACH="mesabi" type="pbs">
      <queues>
        <queue walltimemax="24:00" default="true">mesabi</queue>
        <queue walltimemax="24:00">debug</queue>
      </queues>
    </batch_system>

   <batch_system MACH="oic5" type="pbs" >
         <directives>
                 <directive>-l nodes={{ num_nodes }}:ppn={{ tasks_per_node }}</directive>
		 <directive>-q esd13q</directive>
         </directives>
         <queues>
           <queue default="true">esd13q</queue>
           <queue walltimemax="1:00">esddbg13q</queue>
         </queues>
   </batch_system>

   <batch_system MACH="cades" type="slurm" >
         <submit_args>
           <arg flag="-A ccsi"/>
           <arg flag="--mem=128G"/>
           <arg flag="--ntasks-per-node 32"/>
         </submit_args>
         <queues>
           <queue default="true">burst</queue>
         </queues>
   </batch_system>

   <batch_system MACH="itasca" type="pbs">
     <queues>
       <queue walltimemax="24:00" default="true">batch</queue>
       <queue walltimemax="24:00">debug</queue>
     </queues>
   </batch_system>

   <batch_system MACH="summit" type="lsf" >
     <directives>
       <directive>-P {{ project }}</directive>
     </directives>
     <directives compiler="!.*gpu">
       <directive>-alloc_flags smt$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</directive>
     </directives>
     <directives compiler=".*gpu">
       <directive>-alloc_flags "gpumps smt$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}"</directive>
     </directives>
     <queues>
       <queue walltimemax="02:00" nodemin="1" nodemax="4608" strict="true" default="true">batch</queue>
       <queue walltimemax="02:00" nodemin="1" nodemax="54" strict="true" default="false">batch-hm</queue>
       <queue walltimemax="02:00" nodemin="1" nodemax="91" strict="true" default="false">killable</queue>
       <queue walltimemax="02:00" nodemin="1" nodemax="4608" strict="true" default="false">debug</queue>
     </queues>
   </batch_system>

   <batch_system MACH="lassen" type="lsf" >
     <queues>
       <queue walltimemax="02:00" nodemin="1" nodemax="18" strict="true" default="true">pdebug</queue>
       <queue walltimemax="02:00" nodemin="1" nodemax="256" default="false">pbatch</queue>
     </queues>
   </batch_system>

   <batch_system MACH="ascent" type="lsf" >
     <batch_submit>/gpfs/wolf/cli115/world-shared/e3sm/tools/bsub/throttle</batch_submit>
     <directives>
       <directive>-P {{ project }}</directive>
     </directives>
     <directives compiler="!.*gpu">
       <directive>-alloc_flags smt$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}</directive>
     </directives>
     <directives compiler=".*gpu">
       <directive>-alloc_flags "gpumps smt$SHELL{echo "(`./xmlquery --value MAX_TASKS_PER_NODE`+41)/42"|bc}"</directive>
     </directives>
     <queues>
       <queue walltimemax="02:00" default="true" strict="true" nodemin="1" nodemax="2">batch</queue>
       <queue walltimemax="01:00" strict="true" nodemin="3" nodemax="16">batch</queue>
     </queues>
   </batch_system>

   <batch_system MACH="frontier" type="slurm">
    <batch_submit>sbatch</batch_submit>
     <directives>
       <directive> --core-spec=$SHELL{tpn=`./xmlquery --value MAX_TASKS_PER_NODE`; if [[ $tpn > 56 ]]; then echo "64-$tpn"|bc; else echo 8; fi; }</directive>
     </directives>
     <queues>
       <queue strict="true" nodemin="1" nodemax="91" walltimemax="2:00:00" default="true">batch</queue>
       <queue strict="true" nodemin="92" nodemax="183" walltimemax="6:00:00">batch</queue>
       <queue strict="true" nodemin="184" nodemax="9408" walltimemax="12:00:00">batch</queue>
     </queues>
   </batch_system>

   <batch_system MACH="snl-white" type="lsf" >
     <queues>
       <queue walltimemax="02:00" default="true">rhel7G</queue>
     </queues>
   </batch_system>

   <batch_system MACH="weaver" type="lsf" >
     <submit_args>
       <arg flag="-n" name=" $TOTALPES"/>
     </submit_args>
     <queues>
       <queue walltimemax="02:00" default="true">rhel7W</queue>
     </queues>
   </batch_system>

   <batch_system MACH="snl-blake" type="slurm" >
     <queues>
       <queue walltimemax="02:00" default="true">blake</queue>
     </queues>
   </batch_system>

   <batch_system MACH="lawrencium-lr3" type="slurm" >
     <directives>
       <directive>--ntasks-per-node={{ tasks_per_node }}</directive>
       <directive>--qos=lr_normal</directive>
       <directive>--account={{ project }}</directive>
     </directives>
    <queues>
      <queue walltimemax="01:00:00" default="true">lr3</queue>
    </queues>
   </batch_system>

   <batch_system MACH="lawrencium-lr6" type="slurm" >
     <directives>
       <directive>--ntasks-per-node={{ tasks_per_node }}</directive>
       <directive>--qos=lr_normal </directive>
       <directive>--account={{ project }}</directive>
     </directives>
    <queues>
      <queue walltimemax="01:00:00" default="true">lr6</queue>
    </queues>
   </batch_system>

   <batch_system MACH="gcp10" type="slurm" >
     <queues>
       <queue walltimemax="01:30:00" default="true">compute-30</queue> <!--                 0.14214 -->
       <queue walltimemax="01:30:00" default="true">computep</queue>   <!--enable_placement 0.28428 -->
       <queue walltimemax="01:30:00" default="true">compute</queue>    <!--                 0.28428 -->
     </queues>
   </batch_system>

   <batch_system MACH="gcp12" type="slurm" >
     <queues>
       <queue walltimemax="01:30:00" default="true">c2dh112</queue>   <!-- c2d-highcpu-112     1.88984 -->
       <queue walltimemax="01:30:00" default="true">c2d32</queue>     <!-- c2d-standard-32     0.30794 -->
       <queue walltimemax="01:30:00" default="true">c2d56</queue>     <!-- c2d-standard-56     0.53889 -->
       <queue walltimemax="01:30:00" default="true">c2o60</queue>     <!-- "old" c2-compute-60 0.28428 -->
       <queue walltimemax="01:30:00" default="true">compute</queue>   <!-- c2d-standard-112    1.07776 -->
     </queues>
   </batch_system>

  <!-- modex is PBS -->
  <batch_system MACH="modex" type="pbs">
    <directives>
      <directive>-l nodes={{ num_nodes }}:ppn={{ tasks_per_node }}</directive>
      <directive default="/bin/bash" > -S {{ shell }}  </directive>
    </directives>
    <queues>
      <queue walltimemax="06:00:00" default="true">batch</queue>
    </queues>
  </batch_system>

   <batch_system MACH="tulip" type="slurm">
    <queues>
      <queue walltimemax="01:00:00" strict="true" nodemin="1" nodemax="2" default="true">v100</queue>
      <queue walltimemax="01:00:00" nodemin="1" nodemax="5">amdMI60</queue>
      <queue walltimemax="01:00:00" nodemin="1" nodemax="7">amdMI100</queue>
    </queues>
   </batch_system>

   <batch_system MACH="fugaku" type="moab">
     <queues>
       <queue walltimemax="1:00:00" default="true" strict="true">default</queue>
     </queues>
   </batch_system>

  <batch_system MACH="onyx" type="pbs" >
    <directives queue="debug">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="standard">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="transfer">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select=1:ncpus=1</directive>
    </directives>
    <queues>
      <queue walltimemax="00:59:00" nodemax="155" strict="true">debug</queue>
      <queue walltimemax="04:00:00" default="true">standard</queue>
      <queue default="true" walltimemax="12:00:00" jobmin="1" jobmax="1">transfer</queue>
    </queues>
  </batch_system>

  <batch_system MACH="narwhal" type="pbs" >
    <directives queue="debug">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="standard">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="transfer">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select=1:ncpus=1</directive>
    </directives>
    <queues>
      <queue walltimemax="00:30:00" nodemax="64" strict="true">debug</queue>
      <queue walltimemax="04:00:00" default="true">standard</queue>
      <queue default="true" walltimemax="12:00:00" jobmin="1" jobmax="1">transfer</queue>
    </queues>
  </batch_system>

  <batch_system MACH="warhawk" type="pbs" >
    <directives queue="debug">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="standard">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select={{ num_nodes }}:ncpus={{ MAX_MPITASKS_PER_NODE }}:mpiprocs={{ tasks_per_node }}</directive>
    </directives>
    <directives queue="transfer">
      <directive>-A {{ PROJECT }}</directive>
      <directive>-l application=Regional-Arctic-System-Model</directive>
      <directive>-l select=1:ncpus=1</directive>
    </directives>
    <queues>
      <queue walltimemax="00:59:00" nodemax="22" strict="true">debug</queue>
      <queue walltimemax="04:00:00" default="true">standard</queue>
      <queue default="true" walltimemax="12:00:00" jobmin="1" jobmax="1">transfer</queue>
    </queues>
  </batch_system>

</config_batch>
