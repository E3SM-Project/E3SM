<?xml version="1.0"?>

<config_machines>

<!--

 ===============================================================
 COMPILER and COMPILERS
 ===============================================================
 If a machine supports multiple compilers - then
  - the settings for COMPILERS should reflect the supported compilers
    as a comma separated string
  - the setting for COMPILER should be the default compiler
    (which is one of the values in COMPILERS)

 ===============================================================
 MPILIB and MPILIBS
 ===============================================================
 If a machine supports only one MPILIB is supported - then
 the setting for  MPILIB and MPILIBS should be blank ("")
 If a machine supports multiple mpi libraries (e.g. mpich and openmpi)
  - the settings for MPILIBS should reflect the supported mpi libraries
    as a comma separated string

 The default settings for COMPILERS and MPILIBS is blank (in config_machines.xml)

 Normally variable substitutions are not made until the case scripts are run, however variables
 of the form $ENV{VARIABLE_NAME} are substituted in create_newcase from the environment
 variable of the same name if it exists.

 ===============================================================
 PROJECT_REQUIRED
 ===============================================================
 A machine may need the PROJECT xml variable to be defined either because it is
 used in some paths, or because it is used to give an account number in the job
 submission script. If either of these are the case, then PROJECT_REQUIRED
 should be set to TRUE for the given machine.

 ===============================================================
 batch_system
 ===============================================================
 The batch_system and associated tags are meant for configuring batch systems and
 queues across machines.  The batch_system tag denotes the name for a particular
 batch system, these can either be shared between one or more machines, or can be
 defined for a specific machine if need be.
 queues:
 one or more queues can be defined per batch_system. if the attribute default="true"
 is used, then that queue will be used by default. Alternatively, multiple queues can
 be used.  The following variables can be used to choose a queue :
 walltimemin: Giving the minimum amount of walltime for the queue.
 walltimemax: The maximum amount of walltime for a queue.
 jobmin:      The minimum node count required to use this queue.
 jobmax:      The maximum node count required to use this queue.

 walltimes:
 Denotes the walltimes that can be used for a particular machine.
 walltime: as before, if default="true" is defined, this walltime will be used
 by default.
 Alternatively, ccsm_estcost must be used to choose the queue based on the estimated cost of the run.

 mpirun: the mpirun command that will be used to actually launch the model.
 The attributes used to choose the mpirun command are:

 mpilib: can either be 'default' the name of an mpi library, or a compiler name so one can choose the mpirun
         based on the mpi library in use.

   the 'executable' tag must have arguments required for the chosen mpirun, as well as the executable name.


-->
<machine MACH="userdefined">
        <DESC>User Defined Machine</DESC>                                 <!-- can be anything -->
        <NODENAME_REGEX></NODENAME_REGEX>                           <!-- regex for auto-detect that you're on this machine -->
        <TESTS>acme_developer</TESTS>                               <!-- preferred test suite to run on this machine -->
        <OS>USERDEFINED_required_macros</OS>                              <!-- LINUX,Darwin,CNL,AIX,BGL,BGP -->
        <COMPILERS>intel,ibm,pgi,pathscale,gnu,cray,nag</COMPILERS>     <!-- intel,ibm,pgi,pathscale,gnu,cray,nag -->
        <MPILIBS>openmpi,mpich,mpt,mpt,ibm,mpi-serial</MPILIBS>                <!-- openmpi, mpich, ibm, mpi-serial -->
        <CESMSCRATCHROOT>USERDEFINED_required_build</CESMSCRATCHROOT>                     <!-- complete path to the 'scratch' directory -->
        <RUNDIR>USERDEFINED_required_build</RUNDIR>                       <!-- complete path to the run directory -->
        <EXEROOT>USERDEFINED_required_build</EXEROOT>                     <!-- complete path to the build directory -->
        <DIN_LOC_ROOT>USERDEFINED_required_build</DIN_LOC_ROOT>           <!-- complete path to the inputdata directory -->
        <DIN_LOC_ROOT_CLMFORC>USERDEFINED_optional_build</DIN_LOC_ROOT_CLMFORC> <!-- path to the optional forcing data for CLM (for CRUNCEP forcing) -->
        <DOUT_S>FALSE</DOUT_S>                                            <!-- logical for short term archiving -->
        <DOUT_S_ROOT>USERDEFINED_optional_run</DOUT_S_ROOT>               <!-- complete path to a short term archiving directory -->
        <DOUT_L_MSROOT>USERDEFINED_optional_run</DOUT_L_MSROOT>           <!-- complete path to a long term archiving directory -->
        <CCSM_BASELINE>USERDEFINED_optional_run</CCSM_BASELINE>           <!-- where the cesm testing scripts write and read baseline results -->
        <CCSM_CPRNC>USERDEFINED_optional_test</CCSM_CPRNC>                <!-- path to the cprnc tool used to compare netcdf history files in testing -->
        <BATCHQUERY>USERDEFINED_optional_run</BATCHQUERY>
        <BATCHSUBMIT>USERDEFINED_optional_run</BATCHSUBMIT>
        <BATCHREDIRECT></BATCHREDIRECT>
        <SUPPORTED_BY>USERDEFINED_optional</SUPPORTED_BY>
        <GMAKE_J>1</GMAKE_J>
        <MAX_TASKS_PER_NODE>USERDEFINED_required_build</MAX_TASKS_PER_NODE>
</machine>

  <machine MACH="edison">
    <DESC>NERSC XC30, os is CNL, 24 pes/node, batch system is SLURM</DESC>
    <NODENAME_REGEX>edison</NODENAME_REGEX>
    <TESTS>acme_developer</TESTS>
    <COMPILERS>intel,gnu,cray</COMPILERS>
    <MPILIBS>mpt,mpi-serial</MPILIBS>
    <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
    <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
    <OS>CNL</OS>
    <BATCHQUERY>squeue</BATCHQUERY>
    <BATCHSUBMIT>sbatch</BATCHSUBMIT>
    <BATCHREDIRECT></BATCHREDIRECT>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <GMAKE_J>8</GMAKE_J>
    <MAX_TASKS_PER_NODE>48</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>24</PES_PER_NODE>
    <CESMSCRATCHROOT>$ENV{SCRATCH}/acme_scratch</CESMSCRATCHROOT>
    <DIN_LOC_ROOT>/project/projectdirs/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/project/projectdirs/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <CCSM_BASELINE>/project/projectdirs/acme/baselines</CCSM_BASELINE>
    <CCSM_CPRNC>/project/projectdirs/acme/tools/cprnc.edison/cprnc</CCSM_CPRNC>
    <batch_system type="slurm" version="x.y">
      <queues>
        <queue walltimemax="36:00:00" jobmin="1" jobmax="130181" >regular</queue>
	<queue walltimemax="00:30:00" jobmin="1" jobmax="12288" default="true">debug</queue>
      </queues>
      <walltimes>
	<walltime default="true">00:30:00</walltime>
      </walltimes>
    </batch_system>

    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="label"> --label</arg>
	<arg name="num_tasks" > -n {{ num_tasks }}</arg>

     	<arg name="thread_count" > -c {{ thread_count }}</arg>

      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="python">/opt/modules/default/init/python.py</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="python">/opt/modules/default/bin/modulecmd python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="rm">PrgEnv-intel</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">intel</command>
	<command name="rm">cce</command>
	<command name="rm">cray-parallel-netcdf</command>
	<command name="rm">cray-parallel-hdf5</command>
	<command name="rm">pmi</command>
	<command name="rm">cray-libsci</command>
	<command name="rm">cray-mpich2</command>
	<command name="rm">cray-mpich</command>
	<command name="rm">cray-netcdf</command>
	<command name="rm">cray-hdf5</command>
	<command name="rm">cray-netcdf-hdf5parallel</command>
	<command name="rm">craype-sandybridge</command>
	<command name="rm">craype-ivybridge</command>
	<command name="rm">craype</command>
      </modules>

      <modules compiler="intel">
	<command name="load">PrgEnv-intel</command>
	<command name="switch">intel intel/16.0.0.109</command>
	<command name="rm">cray-libsci</command>
	<command name="use">/global/project/projectdirs/ccsm1/modulefiles/edison</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" >
	<command name="load">esmf/6.3.0rp1-defio-intel15.0-mpi-O</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" >
        <command name="load">esmf/6.3.0rp1-defio-intel15.0-mpiuni-O</command>
      </modules>
      <modules compiler="cray">
	<command name="load">PrgEnv-cray</command>
	<command name="switch">cce cce/8.4.3</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">PrgEnv-gnu</command>
	<command name="switch">gcc gcc/5.2.0</command>
      </modules>
      <modules>
	<command name="load">papi/5.4.1.3</command>
	<command name="swap">craype craype/2.5.1</command>
	<command name="load">craype-ivybridge</command>
      </modules>
      <modules compiler="!intel">
	<command name="switch">cray-libsci/13.3.0</command>
      </modules>
      <modules>
	<command name="load">cray-mpich/7.3.1</command>
<!--	<command name="load">pmi/5.0.6-1.0000.10439.140.2.ari</command> -->
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">cray-hdf5/1.8.16</command>
	<command name="load">cray-netcdf/4.3.3.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">cray-netcdf-hdf5parallel/4.3.3.1</command>
	<command name="load">cray-hdf5-parallel/1.8.16</command>
	<command name="load">cray-parallel-netcdf/1.6.1</command>
      </modules>
      <modules>
	<command name="load">perl/5.20.0</command>
	<command name="load">cmake/3.0.0</command>
      </modules>
    </module_system>

    <environment_variables>
      <env name="OMP_STACKSIZE">64M</env>
    </environment_variables>

  </machine>

<machine MACH="hopper">
         <DESC>NERSC XE6, os is CNL, 24 pes/node, batch system is PBS</DESC>
         <NODENAME_REGEX>hopper</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
	 <COMPILERS>pgi,intel,gnu,cray,pathscale</COMPILERS>
	 <MPILIBS>mpt,mpi-serial</MPILIBS>
         <CESMSCRATCHROOT>$ENV{SCRATCH}</CESMSCRATCHROOT>
         <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/project/projectdirs/acme/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/project/projectdirs/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/project/projectdirs/acme/ccsm_baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/project/projectdirs/acme/tools/cprnc/cprnc</CCSM_CPRNC>
         <SAVE_TIMING_DIR>/project/projectdirs/$PROJECT</SAVE_TIMING_DIR>
         <OS>CNL</OS>
         <BATCHQUERY>qstat -f</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>cseg</SUPPORTED_BY>
         <GMAKE_J>4</GMAKE_J>
         <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
         <PIO_CONFIG_OPTS> -D PIO_BUILD_TIMING:BOOL=ON </PIO_CONFIG_OPTS>
         <batch_system type="pbs" version="x.y">
            <queues>
               <queue walltimemax="00:30:00" jomin="1" jobmax="512" default="true">debug</queue>
            </queues>
            <walltimes>
               <walltime default="true">00:30:00</walltime>
            </walltimes>
         </batch_system>
         <mpirun mpilib="default">
            <executable>aprun</executable>
            <arguments>
             <arg name="num_tasks" > -n {{ num_tasks }}</arg>
             <arg name="tasks_per_node" > -N {{ tasks_per_node }}</arg>
             <arg name="thread_count" > -d {{ thread_count }}</arg>
            </arguments>
         </mpirun>
</machine>

<machine MACH="corip1">
    <DESC>NERSC XC30 Haswell, os is CNL, 32 pes/node, batch system is Slurm</DESC>
    <NODENAME_REGEX>cori</NODENAME_REGEX>
    <TESTS>acme_developer</TESTS>
    <COMPILERS>intel,gnu,cray</COMPILERS>
    <MPILIBS>mpt,mpi-serial</MPILIBS>
    <CESMSCRATCHROOT>$ENV{SCRATCH}/acme_scratch</CESMSCRATCHROOT>
    <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
    <DIN_LOC_ROOT>/project/projectdirs/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/project/projectdirs/acme/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
    <CCSM_BASELINE>/project/projectdirs/acme/baselines</CCSM_BASELINE>
    <CCSM_CPRNC>/project/projectdirs/acme/tools/cprnc.cori/cprnc</CCSM_CPRNC>
    <SAVE_TIMING_DIR>/project/projectdirs/$PROJECT</SAVE_TIMING_DIR>
    <OS>CNL</OS>
    <BATCHQUERY>squeue</BATCHQUERY>
    <BATCHSUBMIT>sbatch</BATCHSUBMIT>
    <BATCHREDIRECT></BATCHREDIRECT>
    <SUPPORTED_BY>acme</SUPPORTED_BY>
    <GMAKE_J>8</GMAKE_J>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>32</PES_PER_NODE>
    <PROJECT>acme</PROJECT>
    <batch_system type="slurm" version="x.y">
      <queues>
        <queue walltimemax="00:06:00" jobmin="1" jobmax="16384" default="true">regular</queue>
        <queue walltimemax="00:04:00" jobmin="16385" jobmax="32768" >regular</queue>
        <queue walltimemax="00:02:00" jobmin="32769" jobmax="52096" >regular</queue>
	<queue walltimemax="00:30:00" jobmin="1" jobmax="4096">debug</queue>
      </queues>
      <walltimes>
        <walltime default="true">01:15:00</walltime>
        <walltime ccsm_estcost="1">01:50:00</walltime>
        <walltime ccsm_estcost="3">05:00:00</walltime>
      </walltimes>
    </batch_system>
    <mpirun mpilib="default">
      <executable>srun</executable>
      <arguments>
	<arg name="label"> --label</arg>
	<arg name="num_tasks" > -n {{ num_tasks }}</arg>
	<arg name="thread_count" > -c {{ thread_count }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/opt/modules/default/init/perl.pm</init_path>
      <init_path lang="sh">/opt/modules/default/init/sh</init_path>
      <init_path lang="csh">/opt/modules/default/init/csh</init_path>
      <cmd_path lang="perl">/opt/modules/default/bin/modulecmd perl</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
	<command name="rm">PrgEnv-intel</command>
	<command name="rm">PrgEnv-cray</command>
	<command name="rm">PrgEnv-gnu</command>
	<command name="rm">intel</command>
	<command name="rm">cce</command>
	<command name="rm">cray-parallel-netcdf</command>
	<command name="rm">cray-parallel-hdf5</command>
	<command name="rm">pmi</command>
	<command name="rm">cray-libsci</command>
	<command name="rm">cray-mpich2</command>
	<command name="rm">cray-mpich</command>
	<command name="rm">cray-netcdf</command>
	<command name="rm">cray-hdf5</command>
	<command name="rm">cray-netcdf-hdf5parallel</command>
	<command name="rm">craype-sandybridge</command>
	<command name="rm">craype-ivybridge</command>
	<command name="rm">craype</command>
      </modules>
      <modules compiler="intel">
	<command name="load">PrgEnv-intel</command>
	<command name="switch">intel intel/2016.0.109</command>
	<command name="rm">cray-libsci</command>
      </modules>
      <modules compiler="cray">
	<command name="load">PrgEnv-cray</command>
	<command name="switch">cce cce/8.4.0</command>
      </modules>
      <modules compiler="gnu">
	<command name="load">PrgEnv-gnu</command>
	<command name="switch">gcc gcc/5.1.0</command>
      </modules>
      <modules>
	<command name="load">papi/5.4.1.2</command>
	<command name="swap">craype craype/2.4.2</command>
      </modules>
      <modules compiler="!intel">
	<command name="switch">cray-libsci/13.2.0</command>
      </modules>
      <modules>
	<command name="load">cray-mpich/7.2.5</command>
      </modules>
      <modules mpilib="mpi-serial">
	<command name="load">cray-hdf5/1.8.14</command>
	<command name="load">cray-netcdf/4.3.3.1</command>
      </modules>
      <modules mpilib="!mpi-serial">
	<command name="load">cray-netcdf-hdf5parallel/4.3.3.1</command>
	<command name="load">cray-hdf5-parallel/1.8.14</command>
	<command name="load">cray-parallel-netcdf/1.6.1</command>
      </modules>
      <modules>
	<command name="load">cmake/3.3.2</command>
      </modules>
    </module_system>
</machine>


<machine MACH="mac">
    <DESC>Mac OS/X workstation or laptop</DESC>
    <NODENAME_REGEX></NODENAME_REGEX>
    <TESTS>acme_developer</TESTS>
    <OS>Darwin</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpich,mpi-serial</MPILIBS>
    <RUNDIR>$ENV{HOME}/projects/acme/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/acme/scratch/$CASE/bld</EXEROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/acme/cesm-inputdata</DIN_LOC_ROOT>    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/acme/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/acme/scratch/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
    <CESMSCRATCHROOT>$ENV{HOME}/projects/acme/scratch</CESMSCRATCHROOT>
    <CCSM_BASELINE>$ENV{HOME}/projects/acme/baselines</CCSM_BASELINE>
    <!-- cmake -DCMAKE_Fortran_COMPILER=/opt/local/bin/mpif90-mpich-gcc48 -DHDF5_DIR=/opt/local -DNetcdf_INCLUDE_DIR=/opt/local/include .. -->
>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <BATCHQUERY></BATCHQUERY>
    <BATCHSUBMIT></BATCHSUBMIT>
    <SUPPORTED_BY>jnjohnson at lbl dot gov</SUPPORTED_BY>
<!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <GMAKE_J>4</GMAKE_J>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>2</PES_PER_NODE>
</machine>

<machine MACH="linux-generic">
    <DESC>Linux workstation or laptop</DESC>
    <NODENAME_REGEX></NODENAME_REGEX>
    <TESTS>acme_developer</TESTS>
    <OS>Linux</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpich,mpi-serial</MPILIBS>
    <RUNDIR>$ENV{HOME}/projects/acme/scratch/$CASE/run</RUNDIR>
    <EXEROOT>$ENV{HOME}/projects/acme/scratch/$CASE/bld</EXEROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/projects/acme/cesm-inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/projects/acme/ptclm-data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$ENV{HOME}/projects/acme/scratch/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
    <CESMSCRATCHROOT>$ENV{HOME}/projects/acme/scratch</CESMSCRATCHROOT>
    <CCSM_BASELINE>$ENV{HOME}/projects/acme/baselines</CCSM_BASELINE>
    <!-- cmake -DCMAKE_Fortran_COMPILER=/opt/local/bin/mpif90-mpich-gcc48 -DHDF5_DIR=/opt/local -DNetcdf_INCLUDE_DIR=/opt/local/include .. -->>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <BATCHQUERY></BATCHQUERY>
    <BATCHSUBMIT></BATCHSUBMIT>
    <SUPPORTED_BY>jayesh at mcs dot anl dot gov</SUPPORTED_BY>
<!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <GMAKE_J>4</GMAKE_J>
    <MAX_TASKS_PER_NODE>4</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>2</PES_PER_NODE>
    <batch_system type="none" version="x.y">
      <queues>
      </queues>
      <walltimes>
      </walltimes>
    </batch_system>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ num_tasks }}</arg>
      </arguments>
    </mpirun>
</machine>

<machine MACH="melvin">
    <DESC>Linux workstation for Jenkins testing</DESC>
    <NODENAME_REGEX>(melvin|watson)</NODENAME_REGEX>
    <PROXY>sonproxy.sandia.gov:80</PROXY>
    <TESTS>acme_developer</TESTS>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpi-serial</MPILIBS>
    <CESMSCRATCHROOT>$ENV{HOME}/acme/scratch</CESMSCRATCHROOT>
    <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
    <DIN_LOC_ROOT>/sems-data-store/ACME/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/sems-data-store/ACME/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
    <CCSM_BASELINE>/sems-data-store/ACME/baselines</CCSM_BASELINE>
    <CCSM_CPRNC>/sems-data-store/ACME/cprnc/build/cprnc</CCSM_CPRNC>
    <BATCHQUERY></BATCHQUERY>
    <BATCHSUBMIT></BATCHSUBMIT>
    <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
<!--    <GMAKE>make</GMAKE> <- this doesn't actually work! -->
    <GMAKE_J>32</GMAKE_J>
    <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>64</PES_PER_NODE>
    <batch_system type="none" version="x.y">
      <queues>
      </queues>
      <walltimes>
      </walltimes>
    </batch_system>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="num_tasks"> -np {{ num_tasks }}</arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
      <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
      <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
      <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
      <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <modules>
	<command name="purge"/>
	<command name="load">python/2.7.9</command>
	<command name="load">gcc/5.1.0/openmpi/1.8.7</command>
	<command name="load">cmake/2.8.12</command>
	<command name="load">netcdf/4.3.2/gcc/5.1.0/openmpi/1.8.7</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="NETCDFROOT">$SEMS_NETCDF_ROOT</env>
      <env name="PNETCDFROOT">$SEMS_NETCDF_ROOT</env>
    </environment_variables>
</machine>

<machine MACH="skybridge">
  <DESC>SNL clust</DESC>
  <NODENAME_REGEX>skybridge-login</NODENAME_REGEX>
  <PROXY>wwwproxy.sandia.gov:80</PROXY>
  <TESTS>acme_integration</TESTS>
  <COMPILERS>intel</COMPILERS>
  <MPILIBS>openmpi,mpi-serial</MPILIBS>
  <OS>LINUX</OS>
  <CESMSCRATCHROOT>/gscratch/$USER/acme_scratch/skybridge</CESMSCRATCHROOT>
  <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
  <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
  <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
  <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
  <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>               <!-- complete path to a short term archiving directory -->
  <DOUT_L_MSROOT>USERDEFINED_optional_run</DOUT_L_MSROOT>           <!-- complete path to a long term archiving directory -->
  <CCSM_BASELINE>/projects/ccsm/ccsm_baselines</CCSM_BASELINE>
  <CCSM_CPRNC>/projects/ccsm/cprnc/build/cprnc</CCSM_CPRNC>                <!-- path to the cprnc tool used to compare netcdf history files in testing -->
  <BATCHQUERY>qstat</BATCHQUERY>
  <BATCHSUBMIT>sbatch</BATCHSUBMIT>
  <BATCHREDIRECT></BATCHREDIRECT>
  <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
  <GMAKE_J>4</GMAKE_J>
  <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
  <PES_PER_NODE>8</PES_PER_NODE>
  <PIO_BUFFER_SIZE_LIMIT>1</PIO_BUFFER_SIZE_LIMIT>
  <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
  <PROJECT>fy150001</PROJECT>
  <batch_system type="slurm" version="x.y">
    <queues>
      <queue jobmin="1" jobmax="480" default="true">ec</queue>
    </queues>
    <walltimes>
      <walltime default="true">0:50:00</walltime>
      <walltime ccsm_estcost="0">1:50:00</walltime>
      <walltime ccsm_estcost="1">5:00:00</walltime>
    </walltimes>
  </batch_system>
  <mpirun mpilib="default">
    <executable>mpirun</executable>
    <arguments>
      <arg name="num_tasks"> -np {{ num_tasks }}</arg>
      <arg name="tasks_per_node"> -npernode {{ tasks_per_node }}</arg>
    </arguments>
  </mpirun>
  <module_system type="module">
    <init_path lang="python">/usr/share/Modules/init/python.py</init_path>
    <init_path lang="perl">/usr/share/Modules/init/perl.pm</init_path>
    <init_path lang="sh">/usr/share/Modules/init/sh</init_path>
    <init_path lang="csh">/usr/share/Modules/init/csh</init_path>
    <cmd_path lang="python">/usr/bin/modulecmd python</cmd_path>
    <cmd_path lang="perl">/usr/bin/modulecmd perl</cmd_path>
    <cmd_path lang="csh">module</cmd_path>
    <cmd_path lang="sh">module</cmd_path>
    <modules>
      <command name="purge"/>
      <command name="unload">intel</command>
      <command name="unload">openmpi-intel</command>
      <command name="use">/projects/ccsm/tpl/modules/skybridge/acme/tpl</command>
      <command name="load">sierra-python</command>
      <command name="load">gnu/4.9.2</command>
      <command name="load">intel/intel-15.0.3.187</command>
      <command name="load">openmpi-intel/1.6</command>
      <command name="load">libraries/intel-mkl-15.0.2.164</command>
      <command name="load">libraries/intel-mkl-15.0.2.164</command>
      <command name="load">hdf5/1.8.11/intel/13.0.1/openmpi/1.6.5</command>
      <command name="load">netcdf/4.3.2/intel/13.0.1/openmpi/1.6.5</command>
    </modules>
  </module_system>
  <environment_variables>
    <env name="NETCDFROOT">$SEMS_NETCDF_ROOT</env>
    <env name="PNETCDFROOT">$SEMS_NETCDF_ROOT</env>
  </environment_variables>
</machine>

<machine MACH="redsky">
         <DESC>SNL clust</DESC>
         <NODENAME_REGEX>redsky-login</NODENAME_REGEX>
         <PROXY>wwwproxy.sandia.gov:80</PROXY>
         <TESTS>acme_integration</TESTS>
	 <COMPILERS>intel</COMPILERS>
	 <MPILIBS>openmpi,mpi-serial</MPILIBS>
         <OS>LINUX</OS>
         <CESMSCRATCHROOT>/gscratch/$USER/acme_scratch</CESMSCRATCHROOT>
         <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>               <!-- complete path to a short term archiving directory -->
         <DOUT_L_MSROOT>USERDEFINED_optional_run</DOUT_L_MSROOT>           <!-- complete path to a long term archiving directory -->
         <CCSM_BASELINE>/projects/ccsm/ccsm_baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/projects/ccsm/cprnc/build/cprnc</CCSM_CPRNC>                <!-- path to the cprnc tool used to compare netcdf history files in testing -->
         <BATCHQUERY>qstat</BATCHQUERY>
         <BATCHSUBMIT>sbatch</BATCHSUBMIT>
         <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>jgfouca at sandia dot gov</SUPPORTED_BY>
         <GMAKE_J>4</GMAKE_J>
         <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
	 <PIO_BUFFER_SIZE_LIMIT>1</PIO_BUFFER_SIZE_LIMIT>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
	 <PROJECT>fy150001</PROJECT>
         <batch_system type="slurm" version="x.y">
           <queues>
             <queue jobmin="1" jobmax="480" default="true">ec</queue>
           </queues>
           <walltimes>
             <walltime default="true">0:50:00</walltime>
             <walltime ccsm_estcost="0">1:50:00</walltime>
             <walltime ccsm_estcost="1">6:00:00</walltime>
           </walltimes>
         </batch_system>
         <mpirun mpilib="default">
           <executable>mpirun</executable>
           <arguments>
             <arg name="num_tasks"> -np {{ num_tasks }}</arg>
             <arg name="tasks_per_node"> -npernode {{ tasks_per_node }}</arg>
           </arguments>
         </mpirun>
</machine>

<machine MACH="blues">
         <DESC>ANL/LCRC Linux Cluster</DESC>
         <NODENAME_REGEX>blogin</NODENAME_REGEX>
         <TESTS>acme_integration,acme_developer</TESTS>
         <COMPILERS>gnu,intel</COMPILERS>
         <MPILIBS>mvapich</MPILIBS>
         <CESMSCRATCHROOT>/lcrc/project/ACME/$USER/acme_scratch</CESMSCRATCHROOT>
	 <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
	 <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/home/ccsm-data/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/home/ccsm-data/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/lcrc/project/ACME/$USER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>/lcrc/project/ACME/$USER/archive/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/lcrc/group/earthscience/acme_baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/home/ccsm-data/tools/cprnc</CCSM_CPRNC>
         <OS>LINUX</OS>
         <BATCHQUERY>qstat</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>acme</SUPPORTED_BY>
         <GMAKE_J>4</GMAKE_J>
         <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
	 <PES_PER_NODE>16</PES_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
	 <PROJECT>ACME</PROJECT>
         <batch_system type="pbs" version="x.y">
           <walltimes>
             <walltime default="true">15:00</walltime>
           </walltimes>
         </batch_system>
         <mpirun mpilib="mvapich">
           <executable>mpiexec</executable>
           <arguments>
             <arg name="num_tasks"> -n {{ num_tasks }} </arg>
           </arguments>
         </mpirun>
         <mpirun mpilib="mpi-serial">
             <executable></executable>
         </mpirun>
    <module_system type="soft">
      <init_path lang="csh">/etc/profile.d/a_softenv.csh</init_path>
      <init_path lang="sh">/etc/profile.d/a_softenv.sh</init_path>
      <cmd_path lang="csh">/soft/softenv/1.6.2/bin/soft-dec csh</cmd_path>
      <cmd_path lang="sh">/soft/softenv/1.6.2/bin/soft-dec sh</cmd_path>
      <modules compiler="gnu">
	<command name="add">+gcc-5.2</command>
	<command name="add">+netcdf-4.3.3.1-gnu5.2-serial</command>
	<command name="add">+cmake-2.8.12</command>
	<command name="add">+python-2.7</command>
	<command name="add">+mvapich2-2.2b-gcc-5.2</command>
      </modules>
      <modules compiler="intel">
	<command name="add">+cmake-2.8.12</command>
	<command name="add">+python-2.7</command>
	<command name="add">+intel-15.0</command>
	<command name="add">+pnetcdf-1.6.1-mvapich2-2.2a-intel-15.0</command>
	<command name="add">+mvapich2-2.2b-intel-15.0</command>
	<command name="add">+mkl-11.2.1</command>
      </modules>
    </module_system>
    <environment_variables compiler="gnu">
      <env name="NETCDFROOT">/soft/netcdf_serial/4.3.3.1/gnu-5.2</env>
      <env name="PNETCDFROOT">/soft/climate/pnetcdf/1.6.1/gcc-5.2/mvapich2-2.2b-gcc-5.2-psm</env>
      <env name="LD_LIBRARY_PATH">/soft/mvapich2/2.2b_psm/gnu-5.2/lib:/soft/netcdf_serial/4.3.3.1/gnu-5.2/lib:/soft/gcc/5.2.0/lib64:/soft/gcc/5.2.0/lib/gcc/x86_64-redhat-linux/5.2.0:/soft/python/2.7.3/lib:/soft/gcc/4.4.2/lib64:/soft/gcc/4.4.2/lib:/soft/lcrc/lib:/usr/lib64:/usr/lib:/soft/mvapich2/1.4-gcc-4.4.2/lib:/soft/tau/2.20.2/tau_latest/x86_64/lib</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <env name="NETCDFROOT">/soft/climate/netcdf/4.3.3.1c-4.2cxx-4.4.2f-serial/intel-15.0.1</env>
      <env name="NETCDF_INCLUDES">/soft/climate/netcdf/4.3.3.1c-4.2cxx-4.4.2f-serial/intel-15.0.1/include</env>
      <env name="NETCDF_LIBS">/soft/climate/netcdf/4.3.3.1c-4.2cxx-4.4.2f-serial/intel-15.0.1/lib</env>
      <env name="PNETCDFROOT">/soft/climate/pnetcdf/1.6.1/intel-15.0.1/mvapich2-2.2a-intel-15.0</env>
      <env name="PATH">$NETCDFROOT/bin:$PATH</env>
      <env name="LD_LIBRARY_PATH">$NETCDFROOT/lib:/soft/intel/15.0.1/mkl/lib/intel64:$LD_LIBRARY_PATH</env>
    </environment_variables>

    <environment_variables compiler="gnu">
      <env name="NETCDFROOT">/soft/netcdf_serial/4.3.3.1/gnu-5.2</env>
      <env name="PNETCDFROOT">/soft/climate/pnetcdf/1.6.1/gcc-5.2/mvapich2-2.2b-gcc-5.2-psm</env>
    </environment_variables>

</machine>

<machine MACH="cetus">
         <DESC>ANL IBM BG/Q, os is BGP, 16 pes/node, batch system is cobalt</DESC>
         <NODENAME_REGEX>cetus</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <COMPILERS>ibm</COMPILERS>
         <MPILIBS>ibm</MPILIBS>
         <CESMSCRATCHROOT>/projects/$PROJECT/$USER</CESMSCRATCHROOT>
         <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/projects/$PROJECT/$USER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>/home/$USER/csm/$CASE/</DOUT_L_MSROOT>
         <CCSM_BASELINE>/projects/ccsm/ccsm_baselines/</CCSM_BASELINE>
         <CCSM_CPRNC>/projects/ccsm/tools/cprnc/cprnc</CCSM_CPRNC>
         <SAVE_TIMING_DIR>/projects/$PROJECT</SAVE_TIMING_DIR>
         <OS>BGQ</OS>
         <BATCHQUERY>qstat</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>   jayesh -at- mcs.anl.gov</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
	 <PROJECT>HiRes_EarthSys</PROJECT>
         <PIO_CONFIG_OPTS> -D PIO_BUILD_TIMING:BOOL=ON </PIO_CONFIG_OPTS>
         <batch_system type="cobalt">
           <queues>
             <queue walltimemin="5" walltimemax="59" jobmin="64" jobmax="2048" default="true">default</queue>
           </queues>
           <walltimes>
             <walltime default="true">59</walltime>
           </walltimes>
         </batch_system>
         <mpirun mpilib="default">
           <executable>runjob</executable>
             <arguments>
               <arg name="label"> --label short</arg>
               <!-- Ranks per node!! -->
               <arg name="tasks_per_node"> -p {{ tasks_per_node }}</arg>
               <!-- Total MPI Tasks -->
               <arg name="num_tasks"> -n {{ num_tasks }}</arg>
               <arg name="locargs">  $LOCARGS</arg>
               <arg name="bg_threadlayout"> --envs BG_THREADLAYOUT=1</arg>
               <arg name="omp_stacksize"> --envs OMP_STACKSIZE=32M</arg>
               <arg name="thread_count"> --envs OMP_NUM_THREADS={{ thread_count }}</arg>
               <arg name="colon">:</arg>
             </arguments>
         </mpirun>
</machine>

<machine MACH="penn">
    <DESC>Linux workstation: Andy's at SNL</DESC>
    <NODENAME_REGEX>penn</NODENAME_REGEX>
    <TESTS>acme_developer</TESTS>
    <OS>LINUX</OS>
    <COMPILERS>gnu</COMPILERS>
    <MPILIBS>openmpi,mpi-serial</MPILIBS>
    <CESMSCRATCHROOT>$ENV{HOME}/acme/scratch</CESMSCRATCHROOT>
    <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
    <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
    <DIN_LOC_ROOT>$ENV{HOME}/acme/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$ENV{HOME}/acme/ptclmdata</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
    <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>

    <CCSM_BASELINE>/home/agsalin/acme/baselines</CCSM_BASELINE>
    <CCSM_CPRNC>$CCSMROOT/tools/cprnc/build/cprnc</CCSM_CPRNC>
    <BATCHQUERY></BATCHQUERY>
    <BATCHSUBMIT></BATCHSUBMIT>
    <SUPPORTED_BY>agsalin at sandia dot gov</SUPPORTED_BY>
    <GMAKE_J>20</GMAKE_J>
    <MAX_TASKS_PER_NODE>20</MAX_TASKS_PER_NODE>
    <PES_PER_NODE>2</PES_PER_NODE>
</machine>

<machine MACH="mira">
         <DESC>ANL IBM BG/Q, os is BGP, 16 pes/node, batch system is cobalt</DESC>
         <NODENAME_REGEX>mira</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <COMPILERS>ibm</COMPILERS>
         <MPILIBS>ibm</MPILIBS>
         <CESMSCRATCHROOT>/projects/$PROJECT/$USER</CESMSCRATCHROOT>
         <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/projects/ccsm/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/projects/ccsm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/projects/$PROJECT/usr/$USER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>/home/$USER/csm/$CASE/</DOUT_L_MSROOT>
         <CCSM_BASELINE>/projects/ccsm/ccsm_baselines/</CCSM_BASELINE>
         <CCSM_CPRNC>/projects/ccsm/tools/cprnc/cprnc</CCSM_CPRNC>
         <SAVE_TIMING_DIR>/projects/$PROJECT</SAVE_TIMING_DIR>
         <OS>BGQ</OS>
         <BATCHQUERY>qstat</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
	 <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>   mickelso -at- mcs.anl.gov</SUPPORTED_BY>
         <GMAKE_J>4</GMAKE_J>
         <MAX_TASKS_PER_NODE>64</MAX_TASKS_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
	 <PROJECT>HiRes_EarthSys</PROJECT>
         <PIO_CONFIG_OPTS> -D PIO_BUILD_TIMING:BOOL=ON </PIO_CONFIG_OPTS>
         <batch_system type="cobalt">
           <queues>
             <queue walltimemin="0" walltimemax="360" jobmin="512" jobmax="4096" default="true">default</queue>
           </queues>
           <walltimes>
             <walltime default="true">30</walltime>
             <walltime ccsm_estcost="-3">60</walltime>
             <walltime ccsm_estcost="0">60</walltime>
           </walltimes>
         </batch_system>
         <mpirun mpilib="default">
           <executable>runjob</executable>
             <arguments>
               <arg name="label"> --label short</arg>
               <!-- Ranks per node!! -->
               <arg name="tasks_per_node"> -p {{ tasks_per_node }}</arg>
               <!-- Total MPI Tasks -->
               <arg name="num_tasks"> -n {{ num_tasks }}</arg>
               <arg name="locargs">  $LOCARGS</arg>
               <arg name="bg_threadlayout"> --envs BG_THREADLAYOUT=1</arg>
               <arg name="omp_stacksize"> --envs OMP_STACKSIZE=32M</arg>
               <arg name="thread_count"> --envs OMP_NUM_THREADS={{ thread_count }}</arg>
               <arg name="colon">:</arg>
             </arguments>
         </mpirun>
</machine>

<machine MACH="sooty">
         <DESC>PNL cluster, os is Linux (pgi,intel,nag), batch system is SLURM</DESC>
         <NODENAME_REGEX>sooty</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <OS>LINUX</OS>
         <COMPILERS>pgi,intel</COMPILERS>
         <MPILIBS>mvapich2</MPILIBS>
         <RUNDIR>/lustre/$USER/csmruns/$CASE/run</RUNDIR>
         <EXEROOT>/lustre/$USER/csmruns/$CASE/bld</EXEROOT>
         <CESMSCRATCHROOT>/lustre/$USER/csmruns/</CESMSCRATCHROOT>
         <DIN_LOC_ROOT>/lustre/sing201/DATASETS/CAM/InitialCondFilesCam/FromMythos1/CSMDATA_CAM/aerocom/csmdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/lustre/sing201/DATASETS/CAM/InitialCondFilesCam/FromMythos1/CSMDATA_CAM/aerocom/csmdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/lustre/$USER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/UNSET_BASELINE</CCSM_BASELINE>
         <CCSM_CPRNC>/lustre/sing201/CAM/netcdfComp_cprnc/cprnc/cprnc</CCSM_CPRNC>
         <BATCHQUERY>squeue</BATCHQUERY>
         <BATCHSUBMIT>sbatch</BATCHSUBMIT>
         <SUPPORTED_BY>balwinder.singh at pnnl dot gov</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
</machine>

<machine MACH="cascade">
         <DESC>PNL cluster, os is Linux (pgi), batch system is SLURM</DESC>
         <NODENAME_REGEX>cascade</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <OS>LINUX</OS>
         <COMPILERS>intel,nag</COMPILERS>
         <MPILIBS>mpich</MPILIBS>
         <RUNDIR>/dtemp/$USER/csmruns/$CASE/run</RUNDIR>
         <EXEROOT>/dtemp/$USER/csmruns/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/dtemp/sing201/inputdata/CAM/CSMDATA_CAM/aerocom/csmdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/dtemp/sing201/inputdata/CAM/CSMDATA_CAM/aerocom/csmdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/dtemp/$USER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/dtemp/sing201/acme_testing/acme_baselines/</CCSM_BASELINE>
             <CESMSCRATCHROOT>/dtemp/$USER/csmruns</CESMSCRATCHROOT>
         <CCSM_CPRNC>/home/sing201/CAM/cprnc/cprnc</CCSM_CPRNC>
         <BATCHQUERY>showq</BATCHQUERY>
         <BATCHSUBMIT>msub</BATCHSUBMIT>
         <SUPPORTED_BY>balwinder.singh at pnnl dot gov</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
</machine>

<machine MACH="constance">
         <DESC>PNL Haswell cluster, OS is Linux, batch system is SLURM</DESC>
         <OS>LINUX</OS>
         <COMPILERS>pgi,intel</COMPILERS>
         <MPILIBS>mpich,mpi-serial</MPILIBS>
	 <NODENAME_REGEX>constance</NODENAME_REGEX>
         <RUNDIR>/pic/scratch/$CCSMUSER/csmruns/$CASE/run</RUNDIR>
         <EXEROOT>/pic/scratch/$CCSMUSER/csmruns/$CASE/bld</EXEROOT>
         <CESMSCRATCHROOT>/pic/scratch/$USER</CESMSCRATCHROOT>
         <DIN_LOC_ROOT>/pic/projects/climate/csmdata/</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/pic/projects/climate/csmdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/pic/scratch/$CCSMUSER/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>UNSET</DOUT_L_MSROOT>
         <CCSM_BASELINE>/pic/projects/climate/acme_baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/pic/projects/climate/acme_baselines/cprnc</CCSM_CPRNC>
         <BATCHQUERY>squeue</BATCHQUERY>
         <BATCHSUBMIT>sbatch</BATCHSUBMIT>
	 <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>balwinder.singh -at- pnnl.gov</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
	 <MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
	 <batch_system type="slurm" version="x.y">
	   <queues>
	     <queue jobmin="1" jobmax="9999" default="true">slurm</queue>
	   </queues>
	   <walltimes>
	     <walltime default="true">0:30:00</walltime>
	   </walltimes>
	 </batch_system>
	 <mpirun mpilib="mpich">
	   <executable>srun</executable>
	   <arguments>
	     <arg name="mpinone"> --mpi=none</arg>
	     <arg name="num_tasks"> --ntasks={{ num_tasks }}</arg>
	     <arg name="cpubind"> --cpu_bind=sockets</arg>
	     <arg name="cpubind"> --cpu_bind=verbose</arg>
             <arg name="killonbadexit"> --kill-on-bad-exit</arg>
	   </arguments>
	 </mpirun>
	 <mpirun mpilib="mpi-serial">
           <executable></executable>
	 </mpirun>

</machine>
<machine MACH="oic2">
         <DESC>ORNL XK6, os is Linux, 8 pes/node, batch system is PBS</DESC>
         <NODENAME_REGEX>oic2</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <COMPILERS>gnu</COMPILERS>
         <MPILIBS>mpich,mpi-serial,openmpi</MPILIBS>
         <RUNDIR>/home/$USER/models/ACME/run/$CASE/run</RUNDIR>
         <EXEROOT>/home/$USER/models/ACME/run/$CASE/bld</EXEROOT>
         <CESMSCRATCHROOT>/home/$USER/models/ACME</CESMSCRATCHROOT>
         <DIN_LOC_ROOT>/home/zdr/models/ccsm_inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/home/zdr/models/ccsm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/home/$USER/models/ACME/run/archive/$CASE</DOUT_S_ROOT>
         <OS>LINUX</OS>
         <BATCHQUERY>qstat -f</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <SUPPORTED_BY>dmricciuto</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <MAX_TASKS_PER_NODE>8</MAX_TASKS_PER_NODE>
         <batch_system type="pbs">
            <queues>
              <queue default="true">esd08q</queue>
            </queues>
            <walltimes>
              <walltime default="true">24:00:00</walltime>
            </walltimes>
         </batch_system>
         <mpirun mpilib="mpich">
              <executable args="default">/projects/cesm/devtools/mpich-3.0.4-gcc4.8.1/bin/mpirun </executable>
	      <arguments>
			<arg name="num_tasks"> -np {{ num_tasks }}</arg>
			<arg name="machine_file">-hostfile $ENV{'PBS_NODEFILE'}</arg>
	      </arguments>
         </mpirun>
         <mpirun mpilib = "mpi-serial">
              <executable> </executable>
         </mpirun>
</machine>

<machine MACH="oic5">
         <DESC>ORNL XK6, os is Linux, 8 pes/node, batch system is PBS</DESC>
         <NODENAME_REGEX>oic5</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <COMPILERS>gnu</COMPILERS>
         <MPILIBS>mpich,mpi-serial,openmpi</MPILIBS>
         <RUNDIR>/home/$USER/models/ACME/run/$CASE/run</RUNDIR>
         <EXEROOT>/home/$USER/models/ACME/run/$CASE/bld</EXEROOT>
         <CESMSCRATCHROOT>/home/$USER/models/ACME</CESMSCRATCHROOT>
         <DIN_LOC_ROOT>/home/zdr/models/ccsm_inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/home/zdr/models/ccsm_inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>/home/$USER/models/ACME/run/archive/$CASE</DOUT_S_ROOT>
         <OS>LINUX</OS>
         <BATCHQUERY>qstat -f</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <SUPPORTED_BY>dmricciuto</SUPPORTED_BY>
         <GMAKE_J>32</GMAKE_J>
         <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
         <batch_system type="pbs">
            <queues>
              <queue default="true">esd13q</queue>
              <queue walltimemax="1:00">esddbg13q</queue>
            </queues>
            <walltimes>
              <walltime default="true">24:00:00</walltime>
            </walltimes>
         </batch_system>
         <mpirun mpilib="mpich">
              <executable args="default">/projects/cesm/devtools/mpich-3.0.4-gcc4.8.1/bin/mpirun </executable>
	      <arguments>
			<arg name="num_tasks"> -np {{ num_tasks }}</arg>
                        <arg name="machine_file">-f $ENV{'PBS_NODEFILE'}</arg>
	      </arguments>
         </mpirun>
         <mpirun mpilib = "mpi-serial">
              <executable> </executable>
         </mpirun>
</machine>

<machine MACH="titan">
         <DESC>ORNL XK6, os is CNL, 16 pes/node, batch system is PBS</DESC>
         <NODENAME_REGEX>titan</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
	 <COMPILERS>pgi,pgicuda,intel,cray</COMPILERS>
	 <MPILIBS>mpich,mpi-serial</MPILIBS>
         <CESMSCRATCHROOT>$ENV{HOME}/acme_scratch/$PROJECT</CESMSCRATCHROOT>
         <RUNDIR>$ENV{MEMBERWORK}/$PROJECT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/lustre/atlas1/cli900/world-shared/cesm/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/lustre/atlas1/cli900/world-shared/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>$ENV{MEMBERWORK}/$PROJECT/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/lustre/atlas1/cli900/world-shared/cesm/baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/lustre/atlas1/cli900/world-shared/cesm/tools/cprnc/cprnc.titan</CCSM_CPRNC>
         <SAVE_TIMING_DIR>$ENV{PROJWORK}/$PROJECT</SAVE_TIMING_DIR>
         <OS>CNL</OS>
         <BATCHQUERY>qstat -f</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
	 <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>cseg</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
	 <PROJECT>cli115</PROJECT>
         <PIO_CONFIG_OPTS> -D PIO_BUILD_TIMING:BOOL=ON </PIO_CONFIG_OPTS>
         <batch_system type="pbs">
            <queues>
              <queue walltimemax="24:00" default="true">batch</queue>
              <queue walltimemax="24:00">debug</queue>
            </queues>
            <walltimes>
              <walltime default="true">00:45:00</walltime>
              <walltime ccsm_estcost="1">02:00:00</walltime>
            </walltimes>
         </batch_system>
         <mpirun mpilib="default">
           <executable args="default">aprun</executable>
         </mpirun>
</machine>

<machine MACH="eos">
         <DESC>ORNL XC30, os is CNL, 16 pes/node, batch system is PBS</DESC>
         <NODENAME_REGEX>eos</NODENAME_REGEX>
         <TESTS>acme_developer</TESTS>
         <COMPILERS>intel</COMPILERS>
         <MPILIBS>mpich,mpi-serial</MPILIBS>
         <CESMSCRATCHROOT>$ENV{HOME}/acme_scratch/$PROJECT</CESMSCRATCHROOT>
         <RUNDIR>$MEMBERWORK/$PROJECT/$CASE/run</RUNDIR>
         <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
         <DIN_LOC_ROOT>/lustre/atlas1/cli900/world-shared/cesm/inputdata</DIN_LOC_ROOT>
         <DIN_LOC_ROOT_CLMFORC>/lustre/atlas1/cli900/world-shared/cesm/inputdata/atm/datm7</DIN_LOC_ROOT_CLMFORC>
         <DOUT_S_ROOT>$MEMBERWORK/$PROJECT/archive/$CASE</DOUT_S_ROOT>
         <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
         <CCSM_BASELINE>/lustre/atlas1/cli900/world-shared/cesm/baselines</CCSM_BASELINE>
         <CCSM_CPRNC>/lustre/atlas1/cli900/world-shared/cesm/tools/cprnc/cprnc.eos</CCSM_CPRNC>
         <SAVE_TIMING_DIR>$ENV{PROJWORK}/$PROJECT</SAVE_TIMING_DIR>
         <OS>CNL</OS>
         <BATCHQUERY>qstat -f</BATCHQUERY>
         <BATCHSUBMIT>qsub</BATCHSUBMIT>
         <BATCHREDIRECT></BATCHREDIRECT>
         <SUPPORTED_BY>acme</SUPPORTED_BY>
         <GMAKE_J>8</GMAKE_J>
         <PES_PER_NODE>16</PES_PER_NODE>
         <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
         <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
         <PIO_CONFIG_OPTS> -D PIO_BUILD_TIMING:BOOL=ON </PIO_CONFIG_OPTS>
         <batch_system type="pbs" version="x.y">
           <queues>
             <queue jobmin="1" jobmax="9999" default="true">batch</queue>
           </queues>
           <walltimes>
             <walltime default="true">01:15:00</walltime>
             <walltime ccsm_estcost="1">01:50:00</walltime>
             <walltime ccsm_estcost="3">05:00:00</walltime>
           </walltimes>
         </batch_system>
         <mpirun mpilib="mpich">
           <executable>aprun</executable>
           <arguments>
             <arg name="hyperthreading" default="2"> -j {{ hyperthreading }}</arg>
             <arg name="tasks_per_numa" > -S {{ tasks_per_numa }}</arg>
             <arg name="num_tasks" > -n {{ num_tasks }}</arg>
             <arg name="tasks_per_node" > -N {{ tasks_per_node }}</arg>
             <arg name="thread_count" > -d {{ thread_count }}</arg>
             <arg name="numa_node" > -cc {{ numa_node }}</arg>
           </arguments>
         </mpirun>
         <mpirun mpilib="mpi-serial">
           <executable></executable>
         </mpirun>
</machine>

<machine MACH="mustang">
	<DESC>LANL Linux Cluster, 24 pes/node, batch system Moab</DESC>
        <NODENAME_REGEX>mustang</NODENAME_REGEX>
        <TESTS>acme_developer</TESTS>
	<COMPILERS>intel,gnu</COMPILERS>
	<MPILIBS>openmpi,mvapich,mpi-serial</MPILIBS>
	<OS>LINUX</OS>
	<RUNDIR>/lustre/scratch1/turquoise/$ENV{USER}/ACME/cases/$CASE/run</RUNDIR>
	<EXEROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/cases/$CASE/bld</EXEROOT>
	<DIN_LOC_ROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data</DIN_LOC_ROOT>
	<DIN_LOC_ROOT_CLMFORC>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data/atm/datm7</DIN_LOC_ROOT_CLMFORC>
	<DOUT_S_ROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/archive/$CASE</DOUT_S_ROOT>
	<DOUT_L_MSROOT>UNSET</DOUT_L_MSROOT>
	<CCSM_BASELINE>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data/ccsm_baselines</CCSM_BASELINE>
	<CESMSCRATCHROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME</CESMSCRATCHROOT>
	<CCSM_CPRNC>/turquoise/usr/projects/climate/SHARED_CLIMATE/software/mustang/cprnc/v0.40/cprnc</CCSM_CPRNC>
	<BATCHQUERY>mshow</BATCHQUERY>
	<BATCHSUBMIT>msub</BATCHSUBMIT>
	<BATCHREDIRECT></BATCHREDIRECT>
	<batch_system type="moab" version="x.y">
		<walltimes>
			<walltime default="true">00:30:00</walltime>
			<walltime ccsm_estcost="1">02:00:00</walltime>
			<walltime ccsm_estcost="3">16:00:00</walltime>
		</walltimes>
	</batch_system>
	<mpirun mpilib="default">
		<executable>mpirun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="openmpi">
		<executable>mpirun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="mvapich">
		<executable>srun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="mpi-serial">
		<executable></executable>
	</mpirun>
	<GMAKE_J>4</GMAKE_J>
	<MAX_TASKS_PER_NODE>24</MAX_TASKS_PER_NODE>
	<SUPPORTED_BY>jacobsen.douglas -at- gmail.com</SUPPORTED_BY>
</machine>

<machine MACH="wolf">
	<DESC>LANL Linux Cluster, 16 pes/node, batch system Moab</DESC>
        <NODENAME_REGEX>wolf</NODENAME_REGEX>
        <TESTS>acme_developer</TESTS>
	<COMPILERS>intel,gnu</COMPILERS>
	<MPILIBS>openmpi,mvapich,mpi-serial</MPILIBS>
	<OS>LINUX</OS>
	<RUNDIR>/lustre/scratch1/turquoise/$ENV{USER}/ACME/cases/$CASE/run</RUNDIR>
	<EXEROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/cases/$CASE/bld</EXEROOT>
	<DIN_LOC_ROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data</DIN_LOC_ROOT>
	<DIN_LOC_ROOT_CLMFORC>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data/atm/datm7</DIN_LOC_ROOT_CLMFORC>
	<DOUT_S_ROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME/archive/$CASE</DOUT_S_ROOT>
	<DOUT_L_MSROOT>UNSET</DOUT_L_MSROOT>
	<CCSM_BASELINE>/lustre/scratch1/turquoise/$ENV{USER}/ACME/input_data/ccsm_baselines</CCSM_BASELINE>
	<CESMSCRATCHROOT>/lustre/scratch1/turquoise/$ENV{USER}/ACME</CESMSCRATCHROOT>
	<CCSM_CPRNC>/turquoise/usr/projects/climate/SHARED_CLIMATE/software/wolf/cprnc/v0.40/cprnc</CCSM_CPRNC>
	<BATCHQUERY>mshow</BATCHQUERY>
	<BATCHSUBMIT>msub</BATCHSUBMIT>
	<BATCHREDIRECT></BATCHREDIRECT>
	<batch_system type="moab" version="x.y">
		<walltimes>
			<walltime default="true">00:30:00</walltime>
			<walltime ccsm_estcost="1">02:00:00</walltime>
			<walltime ccsm_estcost="3">16:00:00</walltime>
		</walltimes>
	</batch_system>
	<mpirun mpilib="default">
		<executable>mpirun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="openmpi">
		<executable>mpirun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="mvapich">
		<executable>srun</executable>
		<arguments>
			<arg name="num_tasks"> -n {{ num_tasks }}</arg>
		</arguments>
	</mpirun>
	<mpirun mpilib="mpi-serial">
		<executable></executable>
	</mpirun>
	<GMAKE_J>4</GMAKE_J>
	<MAX_TASKS_PER_NODE>16</MAX_TASKS_PER_NODE>
	<SUPPORTED_BY>jacobsen.douglas -at- gmail.com</SUPPORTED_BY>
</machine>

<machine MACH="caldera">
  <DESC>NCAR IBM, os is Linux, 16 pes/node, intended for ncar testing of acme cime</DESC>
  <NODENAME_REGEX>(pronghorn*|caldera*)</NODENAME_REGEX>
  <OS>LINUX</OS>
  <COMPILERS>intel,pgi,gnu</COMPILERS>
  <MPILIBS>mpich2,pempi,mpi-serial</MPILIBS>
  <CESMSCRATCHROOT>/glade/scratch/$USER</CESMSCRATCHROOT>
  <RUNDIR>$CESMSCRATCHROOT/$CASE/run</RUNDIR>
  <EXEROOT>$CESMSCRATCHROOT/$CASE/bld</EXEROOT>
  <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
  <DIN_LOC_ROOT_CLMFORC>$ENV{CESMROOT}/lmwg</DIN_LOC_ROOT_CLMFORC>
  <DOUT_S_ROOT>$CESMSCRATCHROOT/archive/$CASE</DOUT_S_ROOT>
  <DOUT_L_MSROOT>csm/$CASE</DOUT_L_MSROOT>
  <CCSM_BASELINE>$ENV{CESMDATAROOT}/ccsm_baselines</CCSM_BASELINE>
  <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc</CCSM_CPRNC>
  <PERL5LIB>/glade/apps/opt/perlmods/lib64/perl5:/glade/apps/opt/perlmods/share/perl5</PERL5LIB>
  <BATCHQUERY></BATCHQUERY>
  <BATCHSUBMIT></BATCHSUBMIT>
  <BATCHREDIRECT></BATCHREDIRECT>
  <SUPPORTED_BY>jedwards@ucar.edu</SUPPORTED_BY>
  <GMAKE_J>8</GMAKE_J>
  <MAX_TASKS_PER_NODE>30</MAX_TASKS_PER_NODE>
  <PES_PER_NODE>15</PES_PER_NODE>
  <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
  <batch_system type="none" version="0.0"/>
  <mpirun mpilib="mpi-serial">
    <executable></executable>
  </mpirun>
  <mpirun mpilib="default" threaded="false">
    <executable>TARGET_PROCESSOR_LIST=AUTO_SELECT mpirun.lsf $ENV{CESMDATAROOT}/tools/bin/launch </executable>
  </mpirun>
  <mpirun mpilib="default" threaded="true">
    <executable>unset MP_PE_AFFINITY; unset MP_TASK_AFFINITY; unset MP_CPU_BIND_LIST; mpirun.lsf $ENV{CESMDATAROOT}/tools/bin/hybrid_launch </executable>
  </mpirun>
  <mpirun compiler="gnu">
    <executable>mpirun.lsf </executable>
  </mpirun>
  <module_system type="module">
    <init_path lang="perl">/glade/apps/opt/lmod/lmod/init/perl</init_path>
    <init_path lang="csh">/glade/apps/opt/lmod/lmod/init/csh</init_path>
    <init_path lang="sh">/glade/apps/opt/lmod/lmod/init/sh</init_path>
    <init_path lang="python">/glade/apps/opt/lmod/lmod/init/env_modules_python.py</init_path>
    <cmd_path lang="perl">/glade/apps/opt/lmod/lmod/libexec/lmod perl</cmd_path>
    <cmd_path lang="sh">module</cmd_path>
    <cmd_path lang="csh">module</cmd_path>
    <cmd_path lang="python">/glade/apps/opt/lmod/lmod/libexec/lmod python</cmd_path>
    <modules>
      <command name="purge"/>
      <command name="load">ncarenv/1.0</command>
      <command name="load">ncarbinlibs/1.1</command>
      <command name="load">perlmods</command>
      <command name="load">gmake/4.1</command>
      <command name="load">python</command>
      <command name="load">all-python-libs</command>
    </modules>
    <modules compiler="intel">
      <command name="load">intel/15.0.3</command>
      <command name="load">mkl/11.1.2</command>
      <command name="load">trilinos/11.10.2</command>
      <command name="load">esmf</command>
    </modules>
    <modules compiler="intel" mpilib="!mpi-serial" debug="true">
      <command name="load">esmf-6.3.0rp1-defio-mpi-g</command>
    </modules>
    <modules compiler="intel" mpilib="!mpi-serial" debug="false">
      <command name="load">esmf-6.3.0rp1-defio-mpi-O</command>
    </modules>
    <modules compiler="intel" mpilib="mpi-serial" debug="true">
      <command name="load">esmf-6.3.0rp1-ncdfio-uni-g</command>
    </modules>
    <modules compiler="intel" mpilib="mpi-serial" debug="false">
      <command name="load"> esmf-6.3.0rp1-ncdfio-uni-O</command>
    </modules>
    <modules compiler="pgi">
      <command name="load">pgi/15.10</command>
    </modules>
    <modules compiler="gnu">
      <command name="load">gnu/5.2.0</command>
    </modules>
    <modules mpilib="mpi-serial">
      <command name="load">netcdf/4.3.3.1</command>
    </modules>
    <modules mpilib="!mpi-serial">
      <command name="load">netcdf-mpi/4.3.3.1</command>
      <command name="load">pnetcdf/1.6.1</command>
    </modules>
    <modules>
      <command name="load">ncarcompilers/1.0</command>
      <command name="load">cmake/3.0.2</command>
    </modules>
  </module_system>
  <environment_variables>
    <env name="OMP_STACKSIZE">256M</env>
    <env name="MP_LABELIO">yes</env>
    <env name="MP_INFOLEVEL">2</env>
    <env name="MP_SHARED_MEMORY">yes</env>
    <env name="MP_EUILIB">us</env>
    <env name="MP_MPILIB">$MPILIB</env>
    <env name="MP_STDOUTMODE">unordered</env>
    <env name="MP_RC_USE_LMC">yes</env>
  </environment_variables>
  <environment_variables debug="true">
    <env name="MP_EAGER_LIMIT">0</env>
  </environment_variables>
</machine>


<default_run_suffix>
  <default_run_exe>$config{'EXEROOT'}/acme.exe </default_run_exe>
  <default_run_misc_suffix> >> acme.log.$LID 2>&amp;1 </default_run_misc_suffix>
</default_run_suffix>

</config_machines>
